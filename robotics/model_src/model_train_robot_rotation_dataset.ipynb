{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0314445227a3baf",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "30e071ec36273394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:32:57.965469Z",
     "start_time": "2025-06-23T19:32:57.119066Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "c87935b1-8a73-42d4-85c2-cbc07b163a07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:32:57.981298Z",
     "start_time": "2025-06-23T19:32:57.979500Z"
    }
   },
   "source": [
    "# import sys, importlib.util, pathlib\n",
    "#\n",
    "# PROJECT_ROOT = pathlib.Path(\"/home/may33/projects/ml_portfolio\").resolve()\n",
    "# if PROJECT_ROOT not in sys.path:\n",
    "#     sys.path.insert(0, str(PROJECT_ROOT))\n",
    "#\n",
    "#\n",
    "# print(\"Top-level path  :\", sys.path[0])\n",
    "# print(\"robotics found  :\", importlib.util.find_spec(\"robotics\") is not None)\n",
    "# print(\"BeautifulSoup   :\", importlib.util.find_spec(\"bs4\"))\n",
    "# print(\"Notebook dir    :\", pathlib.Path().resolve())"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "c62ccd11679045ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:32:58.049124Z",
     "start_time": "2025-06-23T19:32:58.024680Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:32:58.499873Z",
     "start_time": "2025-06-23T19:32:58.481192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import h5py\n",
    "\n",
    "data_path = \"../robomimic/datasets/tool_hang/ph/image_agent.hdf5\"\n",
    "\n",
    "f = h5py.File(data_path, \"r\")\n",
    "\n",
    "data = f[\"data\"]"
   ],
   "id": "3dba3aed726029ba",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:39:52.879144Z",
     "start_time": "2025-06-23T19:39:52.859993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "demo = data[\"demo_2\"]\n",
    "\n",
    "obs = demo[\"obs\"][\"agentview_image_normalized\"][0]"
   ],
   "id": "f5d3100c63edf9fa",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:32:59.436404Z",
     "start_time": "2025-06-23T19:32:59.425433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from robotics.model_src.dataset import normalize_data\n",
    "# from tqdm import tqdm\n",
    "#\n",
    "# with h5py.File(data_path, \"r+\") as f:\n",
    "#\n",
    "#     data = f[\"data\"]\n",
    "#\n",
    "#     for demo_name in tqdm(data.keys()):\n",
    "#         demo = data[demo_name]\n",
    "#         obs_data = demo[\"obs\"][\"agentview_image\"][:]\n",
    "#\n",
    "#         normalized = normalize_data(obs_data, 255)\n",
    "#\n",
    "#         # del demo[\"obs\"][\"agentview_image_normalized\"]\n",
    "#         demo[\"obs\"][\"agentview_image_normalized\"] = normalized"
   ],
   "id": "9bd0bdc773467049",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:32:59.976295Z",
     "start_time": "2025-06-23T19:32:59.969579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_keys = list(data.keys())\n",
    "\n",
    "ds_1_demos, ds_2_demos = data_keys[:99], data_keys[100:]"
   ],
   "id": "5d2ace37bf5c532e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:43:51.906664Z",
     "start_time": "2025-06-23T19:43:18.578199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from robotics.model_src.dataset import RobosuiteImageActionDataset, normalize_data\n",
    "\n",
    "camera_type = \"agentview\"\n",
    "\n",
    "pred_horizon = 8\n",
    "obs_horizon = 1\n",
    "\n",
    "ds_1 = RobosuiteImageActionDataset(data_path, camera_type, obs_horizon = obs_horizon, pred_horizon = pred_horizon, demos= ds_1_demos)\n",
    "ds_1.drop_data()\n",
    "ds_2 = RobosuiteImageActionDataset(data_path, camera_type, obs_horizon = obs_horizon, pred_horizon = pred_horizon, demos= ds_2_demos)"
   ],
   "id": "c9de2c082f28b87a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:00<00:00, 4524.60it/s]\n",
      "59it [00:32,  1.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[31]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      5\u001B[39m pred_horizon = \u001B[32m8\u001B[39m\n\u001B[32m      6\u001B[39m obs_horizon = \u001B[32m1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m ds_1 = \u001B[43mRobosuiteImageActionDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcamera_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobs_horizon\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mobs_horizon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpred_horizon\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mpred_horizon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdemos\u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mds_1_demos\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m ds_1.drop_data()\n\u001B[32m     10\u001B[39m ds_2 = RobosuiteImageActionDataset(data_path, camera_type, obs_horizon = obs_horizon, pred_horizon = pred_horizon, demos= ds_2_demos)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/ml_portfolio/robotics/model_src/dataset.py:170\u001B[39m, in \u001B[36mRobosuiteImageActionDataset.__init__\u001B[39m\u001B[34m(self, data_path, camera_type, obs_horizon, pred_horizon, image_size, demos)\u001B[39m\n\u001B[32m    167\u001B[39m \u001B[38;5;28mself\u001B[39m.episode_lens = episode_lens\n\u001B[32m    169\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m camera_type:\n\u001B[32m--> \u001B[39m\u001B[32m170\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mload_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    171\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    172\u001B[39m     states_np = np.concatenate(states, axis=\u001B[32m0\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/ml_portfolio/robotics/model_src/dataset.py:211\u001B[39m, in \u001B[36mRobosuiteImageActionDataset.load_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    210\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mload_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m211\u001B[39m     images_np = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mload_data_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    213\u001B[39m     \u001B[38;5;66;03m# self.obs_data_transformed = normalize_data(images_np, 255)\u001B[39;00m\n\u001B[32m    214\u001B[39m     \u001B[38;5;28mself\u001B[39m.obs_data_transformed = images_np\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<string>:10\u001B[39m, in \u001B[36mload_data_fn\u001B[39m\u001B[34m(self)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/diffusion/lib/python3.12/site-packages/h5py/_hl/dataset.py:1068\u001B[39m, in \u001B[36mDataset.read_direct\u001B[39m\u001B[34m(self, dest, source_sel, dest_sel)\u001B[39m\n\u001B[32m   1065\u001B[39m     dest_sel = sel.select(dest.shape, dest_sel)\n\u001B[32m   1067\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m mspace \u001B[38;5;129;01min\u001B[39;00m dest_sel.broadcast(source_sel.array_shape):\n\u001B[32m-> \u001B[39m\u001B[32m1068\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mid\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmspace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfspace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdxpl\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dxpl\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "b4be688e4f10d7c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:35:57.428549Z",
     "start_time": "2025-06-23T19:35:56.921734Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "val_ratio = 0.2\n",
    "n_total   = len(ds_1)\n",
    "n_val     = int(n_total * val_ratio)\n",
    "n_train   = n_total - n_val\n",
    "\n",
    "generator = torch.Generator().manual_seed(33)\n",
    "train_set_1, val_set = torch.utils.data.random_split(\n",
    "        ds_1, [n_train, n_val], generator=generator)\n",
    "\n",
    "\n",
    "\n",
    "train_loader_1 = torch.utils.data.DataLoader(\n",
    "    train_set_1, batch_size=224, shuffle=True,\n",
    "    num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "train_loader_2 = torch.utils.data.DataLoader(\n",
    "    ds_2, batch_size=224, shuffle=True,\n",
    "    num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size=224, shuffle=False,\n",
    "    num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "\n",
    "# visualize data in batch\n",
    "batch = next(iter(train_loader_2))\n",
    "print(\"batch['image'].shape:\", batch['img_obs'].shape)\n",
    "print(\"batch['act_obs'].shape:\", batch['act_obs'].shape)\n",
    "print(\"batch['act_pred'].shape\", batch['act_pred'].shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch['image'].shape: torch.Size([224, 2, 3, 224, 224])\n",
      "batch['act_obs'].shape: torch.Size([224, 2, 7])\n",
      "batch['act_pred'].shape torch.Size([224, 8, 7])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "6a0e7af3f0cc470f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:36:01.411029Z",
     "start_time": "2025-06-23T19:35:57.889313Z"
    }
   },
   "source": [
    "from robotics.model_src.diffusion_model import ConditionalUnet1D, ConditionalUnet1DTransformer\n",
    "from robotics.model_src.visual_encoder import CNNVisualEncoder\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# visual_encoder = CLIPVisualEncoder().to(device)\n",
    "\n",
    "visual_encoder = CNNVisualEncoder().to(device)\n",
    "\n",
    "vision_feature_dim = visual_encoder.get_output_shape()\n",
    "\n",
    "action_observation_dim = 7\n",
    "\n",
    "obs_dim = vision_feature_dim + action_observation_dim\n",
    "\n",
    "action_dim = 7\n",
    "\n",
    "noise_prediction_net = ConditionalUnet1DTransformer(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim * obs_horizon,\n",
    ").to(device)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/may33/miniconda3/envs/diffusion/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/may33/miniconda3/envs/diffusion/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 9.521678e+07\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "dfcf411fb96bd906",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:41:04.239150Z",
     "start_time": "2025-06-23T19:40:44.568375Z"
    }
   },
   "source": [
    "image = torch.Tensor(ds_2[22][\"img_obs\"][None, :obs_horizon, :, :, :]).to(device)\n",
    "act_obs = torch.Tensor(ds_2[0][\"act_obs\"][None, :obs_horizon, :]).to(device)"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[24]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m image = torch.Tensor(\u001B[43mds_2\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m22\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[33m\"\u001B[39m\u001B[33mimg_obs\u001B[39m\u001B[33m\"\u001B[39m][\u001B[38;5;28;01mNone\u001B[39;00m, :obs_horizon, :, :, :]).to(device)\n\u001B[32m      2\u001B[39m act_obs = torch.Tensor(ds_2[\u001B[32m0\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mact_obs\u001B[39m\u001B[33m\"\u001B[39m][\u001B[38;5;28;01mNone\u001B[39;00m, :obs_horizon, :]).to(device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/ml_portfolio/robotics/model_src/dataset.py:225\u001B[39m, in \u001B[36mRobosuiteImageActionDataset.__getitem__\u001B[39m\u001B[34m(self, idx)\u001B[39m\n\u001B[32m    222\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[32m    223\u001B[39m     trajectory_idx = \u001B[38;5;28mself\u001B[39m.indexes[idx]\n\u001B[32m--> \u001B[39m\u001B[32m225\u001B[39m     img_obs  = \u001B[38;5;28;43mself\u001B[39;49m.obs_data_transformed[trajectory_idx[:\u001B[38;5;28mself\u001B[39m.obs_horizon + \u001B[32m1\u001B[39m]]\n\u001B[32m    226\u001B[39m     act_obs  = \u001B[38;5;28mself\u001B[39m.actions_data_transformed[trajectory_idx[:\u001B[38;5;28mself\u001B[39m.obs_horizon + \u001B[32m1\u001B[39m]]\n\u001B[32m    227\u001B[39m     act_pred = \u001B[38;5;28mself\u001B[39m.actions_data_transformed[trajectory_idx[\u001B[38;5;28mself\u001B[39m.obs_horizon + \u001B[32m1\u001B[39m:]]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_frame.py:892\u001B[39m, in \u001B[36mPyDBFrame.trace_dispatch\u001B[39m\u001B[34m(self, frame, event, arg)\u001B[39m\n\u001B[32m    890\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_line:\n\u001B[32m    891\u001B[39m     \u001B[38;5;28mself\u001B[39m.set_suspend(thread, step_cmd)\n\u001B[32m--> \u001B[39m\u001B[32m892\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    893\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# return event\u001B[39;00m\n\u001B[32m    894\u001B[39m     back = frame.f_back\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_frame.py:412\u001B[39m, in \u001B[36mPyDBFrame.do_wait_suspend\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    411\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdo_wait_suspend\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m412\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_args\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python-ce/helpers/pydev/pydevd.py:1220\u001B[39m, in \u001B[36mPyDB.do_wait_suspend\u001B[39m\u001B[34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[39m\n\u001B[32m   1217\u001B[39m         from_this_thread.append(frame_id)\n\u001B[32m   1219\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, stop_reason):\n\u001B[32m-> \u001B[39m\u001B[32m1220\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python-ce/helpers/pydev/pydevd.py:1235\u001B[39m, in \u001B[36mPyDB._do_wait_suspend\u001B[39m\u001B[34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[39m\n\u001B[32m   1232\u001B[39m             \u001B[38;5;28mself\u001B[39m._call_mpl_hook()\n\u001B[32m   1234\u001B[39m         \u001B[38;5;28mself\u001B[39m.process_internal_commands()\n\u001B[32m-> \u001B[39m\u001B[32m1235\u001B[39m         \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1237\u001B[39m \u001B[38;5;28mself\u001B[39m.cancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[32m   1239\u001B[39m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "3a28b9062ae5369e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:36:27.221328Z",
     "start_time": "2025-06-23T19:36:27.159520Z"
    }
   },
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "im = image[0,0, :, :].cpu().numpy()\n",
    "\n",
    "plt.imshow(im.transpose((1, 2, 0)))\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "404c3b9fc97655fd",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    image_features = visual_encoder.encode(image.flatten(start_dim=0, end_dim=1))\n",
    "\n",
    "    image_features = image_features.reshape(*image.shape[:2], -1)\n",
    "\n",
    "    obs = torch.cat([image_features, act_obs], dim=-1)\n",
    "\n",
    "    noised_action = torch.randn((1, pred_horizon, action_dim)).to(device)\n",
    "\n",
    "    timestep_tensor = torch.randint(0, 101, (1,), device=device)\n",
    "\n",
    "    noise = noise_prediction_net(\n",
    "        sample=noised_action,\n",
    "        timestep=timestep_tensor,\n",
    "        global_cond=obs.flatten(start_dim=1)\n",
    "    )\n",
    "\n",
    "    denoised_action = noised_action - noise\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "accd1216068cb090",
   "metadata": {},
   "source": [
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "\n",
    "num_diffusion_iters = 100\n",
    "\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    clip_sample=True,\n",
    "    prediction_type='epsilon'\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f1cfb8707059725b",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def save_final_models(visual_encoder, noise_pred_net, out_dir):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"visual_encoder\": visual_encoder.state_dict(),\n",
    "            \"noise_pred_net\": noise_pred_net.state_dict(),\n",
    "        },\n",
    "        out_dir / \"model_final.pth\",\n",
    "    )\n",
    "    print(f\"Saved to {out_dir / 'models.pth'}\")\n",
    "\n",
    "def load_final_models(visual_encoder, noise_pred_net, ckpt_path, device=\"cuda\"):\n",
    "    ckpt_path = Path(ckpt_path)\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    visual_encoder.load_state_dict(state[\"visual_encoder\"], strict=True)\n",
    "    noise_pred_net.load_state_dict(state[\"noise_pred_net\"], strict=True)\n",
    "\n",
    "    visual_encoder.to(device).eval()\n",
    "    noise_pred_net.to(device).eval()\n",
    "    print(f\"Loaded weights from {ckpt_path}\")\n",
    "\n",
    "def save_checkpoint(\n",
    "    epoch,\n",
    "    loss,\n",
    "    visual_encoder,\n",
    "    noise_pred_net,\n",
    "    ema,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    out_dir=\"checkpoints\",\n",
    "):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_name = f\"checkpoint_epoch{epoch:03d}_loss{loss:.4f}.pth\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": loss,\n",
    "            \"visual_encoder\": visual_encoder.state_dict(),\n",
    "            \"noise_pred_net\": noise_pred_net.state_dict(),\n",
    "            \"ema\": ema.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "        },\n",
    "        out_dir / ckpt_name,\n",
    "    )\n",
    "    print(f\"Checkpoint saved to {out_dir / ckpt_name}\")\n",
    "\n",
    "def load_checkpoint(\n",
    "    ckpt_path,\n",
    "    visual_encoder,\n",
    "    noise_pred_net,\n",
    "    ema,\n",
    "    optimizer=None,\n",
    "    scheduler=None,\n",
    "    map_location=\"cpu\",\n",
    "):\n",
    "    ckpt = torch.load(ckpt_path, map_location=map_location)\n",
    "    visual_encoder.load_state_dict(ckpt[\"visual_encoder\"])\n",
    "    noise_pred_net.load_state_dict(ckpt[\"noise_pred_net\"])\n",
    "    ema.load_state_dict(ckpt[\"ema\"])\n",
    "    if optimizer is not None and \"optimizer\" in ckpt:\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "    if scheduler is not None and \"scheduler\" in ckpt:\n",
    "        scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
    "    print(f\"Checkpoint loaded from {ckpt_path}\")\n",
    "    return ckpt[\"epoch\"], ckpt.get(\"loss\", None)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "from diffusers import EMAModel, get_scheduler\n",
    "\n",
    "def forward_loss(nbatch):\n",
    "    nobs  = nbatch['img_obs'][:, :obs_horizon].to(device)\n",
    "    a_obs = nbatch['act_obs'][:, :obs_horizon].to(device)\n",
    "    a_gt  = nbatch['act_pred'].to(device)\n",
    "    B = a_obs.size(0)\n",
    "\n",
    "\n",
    "    image_features = visual_encoder.encode(nobs.flatten(start_dim=0, end_dim=1))\n",
    "\n",
    "    image_features = image_features.reshape(*nobs.shape[:2], -1)\n",
    "\n",
    "    obs = torch.cat([image_features, a_obs], dim=-1)\n",
    "\n",
    "    obs_cond = obs.flatten(start_dim=1)  # (B, H*obs_dim)\n",
    "\n",
    "    noise = torch.randn_like(a_gt)\n",
    "    timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps,\n",
    "                               (B,), device=device).long()\n",
    "    noisy_a = noise_scheduler.add_noise(a_gt, noise, timesteps)\n",
    "    noise_pred = noise_prediction_net(noisy_a, timesteps, global_cond=obs_cond)\n",
    "    return nn.functional.mse_loss(noise_pred, noise)#%%\n"
   ],
   "id": "4376394fcf62c4fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_epochs = 500\n",
    "\n",
    "# EMA params\n",
    "all_params = list(noise_prediction_net.parameters())\n",
    "ema = EMAModel(parameters=all_params, power=0.75)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=all_params,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-6\n",
    ")\n",
    "\n",
    "# LR scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(train_loader) * num_epochs\n",
    ")\n",
    "\n",
    "\n",
    "# train loop\n",
    "train_hist, val_hist = [], []\n",
    "for epoch_idx in range(num_epochs):\n",
    "    epoch_loss_sum = 0.0\n",
    "\n",
    "    for nbatch in train_loader:\n",
    "        # prepare data\n",
    "        nobs = nbatch['img_obs'][:, :obs_horizon].to(device)  # (B, H, state_len)\n",
    "        action_obs = nbatch['act_obs'][:, :obs_horizon].to(device)  # (B, H, 7)\n",
    "        action_pred = nbatch['act_pred'].to(device)  # (B, P, 7)\n",
    "        B = action_obs.size(0)\n",
    "\n",
    "        image_features = visual_encoder.encode(nobs.flatten(start_dim=0, end_dim=1))\n",
    "\n",
    "        image_features = image_features.reshape(*nobs.shape[:2], -1)\n",
    "\n",
    "        obs = torch.cat([image_features, action_obs], dim=-1)\n",
    "\n",
    "        obs_cond = obs.flatten(start_dim=1)  # (B, H*obs_dim)\n",
    "\n",
    "        noise = torch.randn_like(action_pred)\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.config.num_train_timesteps,\n",
    "            (B,), device=device).long()\n",
    "\n",
    "        noisy_actions = noise_scheduler.add_noise(action_pred, noise, timesteps)\n",
    "        noise_pred = noise_prediction_net(noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "        loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step()\n",
    "        ema.step(all_params)\n",
    "\n",
    "        epoch_loss_sum += loss.item()\n",
    "\n",
    "        avg_train = epoch_loss_sum / len(train_loader)\n",
    "        train_hist.append(avg_train)\n",
    "\n",
    "    # validation\n",
    "    noise_prediction_net.eval()\n",
    "    val_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            val_sum += forward_loss(batch).item()\n",
    "    avg_val = val_sum / len(val_loader)\n",
    "    val_hist.append(avg_val)\n",
    "\n",
    "    print(f\"Epoch {epoch_idx+1:03d}/{num_epochs} | \"\n",
    "          f\"train {avg_train:.6f} | val {avg_val:.6f}\")\n",
    "\n",
    "    if (epoch_idx + 1) % 10 == 0:\n",
    "        save_checkpoint(\n",
    "            epoch=epoch_idx + 1,\n",
    "            loss=avg_val,\n",
    "            visual_encoder=visual_encoder,\n",
    "            noise_pred_net=noise_prediction_net,\n",
    "            ema=ema,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=lr_scheduler,\n",
    "            out_dir=\"checkpoints\",\n",
    "        )\n",
    "\n",
    "# copy EMA weights for inference\n",
    "ema.copy_to(all_params)"
   ],
   "id": "6ef21157758ee4e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ckpt_path = \"./checkpoints/checkpoint_epoch190_loss0.0316.pth\"\n",
    "#\n",
    "# ckpt = torch.load(ckpt_path, map_location=\"cuda\")\n",
    "# visual_encoder.load_state_dict(ckpt[\"visual_encoder\"])\n",
    "# noise_prediction_net.load_state_dict(ckpt[\"noise_pred_net\"])"
   ],
   "id": "21e5a38fb73e0156",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18f7349f71d96bd5",
   "metadata": {},
   "source": [
    "save_final_models(visual_encoder, noise_prediction_net,\n",
    "                  \"../models/robot_v8_tool_hang_agent_224_epoch215\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "89c55255cce7005",
   "metadata": {},
   "source": "load_final_models(visual_encoder, noise_prediction_net, \"../models/robot_v7.1_tool_hang_agent_224_epoch200/model_final.pth\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val_sum = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            val_sum += forward_loss(batch).item()\n",
    "\n",
    "val_sum / len(val_loader)"
   ],
   "id": "d8b5982238c9a952",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "65c7ae7c4c252883",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:37:11.735582Z",
     "start_time": "2025-06-21T14:37:11.702483Z"
    }
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "id": "c7de96297d547660",
   "metadata": {},
   "source": [
    "import robomimic\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "import robomimic.utils.env_utils as EnvUtils\n",
    "import robomimic.utils.file_utils as FileUtils\n",
    "from robomimic.utils.vis_utils import depth_to_rgb\n",
    "from robomimic.envs.env_base import EnvBase, EnvType\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "env_meta = FileUtils.get_env_metadata_from_dataset(dataset_path=data_path)\n",
    "env_meta[\"env_kwargs\"][\"reward_shaping\"] = True\n",
    "env_meta[\"env_kwargs\"][\"reward_scale\"]   = 1.0\n",
    "\n",
    "dummy_spec = dict(\n",
    "    obs=dict(\n",
    "        low_dim=[\"robot0_eef_pos\"],\n",
    "        rgb=[\"agentview_image\"]\n",
    "        # rgb=[\"robot0_eye_in_hand_image\"]\n",
    "    ),\n",
    ")\n",
    "\n",
    "ObsUtils.initialize_obs_utils_with_obs_specs(obs_modality_specs=dummy_spec)\n",
    "\n",
    "env = EnvUtils.create_env_from_metadata(env_meta=env_meta, render=True, render_offscreen=True, use_image_obs=True)\n",
    "\n",
    "a = env.reset()\n",
    "\n",
    "from collections import deque\n",
    "obs_deque  = deque(maxlen=obs_horizon)\n",
    "act_deque  = deque(maxlen=obs_horizon)\n",
    "rewards    = []\n",
    "imgs       = []\n",
    "step_idx   = 0\n",
    "\n",
    "max_steps = 500\n",
    "action_horizon  = 4\n",
    "\n",
    "# ─── 6. Main rollout ──────────────────────────────────────────────────────────\n",
    "obs = env.reset()\n",
    "# wrap obs in same format as env.step\n",
    "obs = obs if isinstance(obs, dict) else obs[0]\n",
    "for i in range(obs_deque.maxlen):\n",
    "    obs_deque.append(obs)\n",
    "    act_deque.append(np.zeros(action_dim, dtype=np.float32))\n",
    "\n",
    "pbar = tqdm(total=max_steps)\n",
    "done = False\n",
    "\n",
    "while not done and step_idx < max_steps:\n",
    "    # 6.1 build the image & action history tensor\n",
    "    img_np = np.array([obs_deque[i][camera_type + \"_image\"] for i in range(obs_deque.maxlen)])\n",
    "\n",
    "    img_t   = torch.from_numpy(img_np).float().to(device)\n",
    "\n",
    "    actions_hist = torch.stack(\n",
    "        [torch.from_numpy(a) for a in list(act_deque)],\n",
    "        dim=0\n",
    "    ).to(device)                           # (1, H_a, 7)\n",
    "\n",
    "    # 6.2 compute visual features + conditioning\n",
    "    with torch.no_grad():\n",
    "        img_feat = visual_encoder(img_t)                # (1, C)\n",
    "        obs_cond = torch.cat([img_feat.flatten(start_dim=0).unsqueeze(0) , actions_hist.flatten(start_dim=0).unsqueeze(0)], dim=1)\n",
    "\n",
    "        # 6.3 sample a future action sequence via diffusion\n",
    "        B = 1\n",
    "        pred_actions = torch.randn((B, pred_horizon, action_dim), device=device)\n",
    "        noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "        for t in noise_scheduler.timesteps:\n",
    "            noise_pred    = noise_prediction_net(pred_actions, t, global_cond=obs_cond)\n",
    "            out           = noise_scheduler.step(noise_pred, t, pred_actions)\n",
    "            pred_actions  = out.prev_sample\n",
    "\n",
    "    pred_actions = pred_actions.cpu().numpy()[0]        # (pred_horizon, 7)\n",
    "\n",
    "    # 6.4 execute the next block of actions\n",
    "    start = obs_horizon\n",
    "    end   = start + action_horizon\n",
    "    action_block = pred_actions[start:end]          # (5, 7)\n",
    "\n",
    "    for act in action_block:\n",
    "        obs, rew, done, info = env.step(act)\n",
    "        obs = obs if isinstance(obs, dict) else obs[0]\n",
    "\n",
    "        frame = env.render(mode=\"rgb_array\", height=512, width=512)\n",
    "\n",
    "        obs_deque.append(obs)\n",
    "        act_deque.append(act.astype(np.float32))\n",
    "\n",
    "        rewards.append(rew)\n",
    "        imgs.append(frame)\n",
    "\n",
    "        step_idx += 1\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix(reward=float(rew))\n",
    "\n",
    "        if done or step_idx >= max_steps:\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# ─── 7. Wrap up ───────────────────────────────────────────────────────────────\n",
    "print(f\"Rollout finished: {step_idx} steps, total reward {sum(rewards):.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9fe8151cf6b7358b",
   "metadata": {},
   "source": [
    "import imageio\n",
    "\n",
    "video_path = \"test_larger_size_img.mp4\"\n",
    "fps = 24\n",
    "\n",
    "with imageio.get_writer(video_path, fps=fps, codec=\"libx264\") as writer:\n",
    "    for frame in imgs:\n",
    "        writer.append_data(frame)\n",
    "\n",
    "print(f\"Saved video to {video_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f62615db63e5ccd6",
   "metadata": {},
   "source": "img = img_np[0]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.imshow(img.transpose(1,2,0))",
   "id": "10812269deff5c5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5730143767b72223",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
