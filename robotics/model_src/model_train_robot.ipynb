{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load Data",
   "id": "a0314445227a3baf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "30e071ec36273394",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "c62ccd11679045ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from robotics.model_src.dataset import RobosuiteImageActionDataset\n",
    "\n",
    "# data_path = \"../robomimic/datasets/can/ph/image_griper.hdf5\"\n",
    "data_path = \"../robomimic/datasets/tool_hang/ph/image_griper.hdf5\"\n",
    "\n",
    "# camera_type = \"robot0_eye_in_hand\"\n",
    "# camera_type = \"agentview\"\n",
    "camera_type = None\n",
    "\n",
    "pred_horizon = 8\n",
    "obs_horizon = 4\n",
    "\n",
    "ds = RobosuiteImageActionDataset(data_path, camera_type, obs_horizon = obs_horizon, prediction_horizon = pred_horizon)"
   ],
   "id": "c2933d24731266ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    ds,\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    # accelerate cpu-gpu transfer\n",
    "    pin_memory=True,\n",
    "    # don't kill worker process afte each epoch\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# visualize data in batch\n",
    "batch = next(iter(dataloader))\n",
    "print(\"batch['image'].shape:\", batch['img_obs'].shape)\n",
    "print(\"batch['act_obs'].shape:\", batch['act_obs'].shape)\n",
    "print(\"batch['act_pred'].shape\", batch['act_pred'].shape)"
   ],
   "id": "b4be688e4f10d7c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from robotics.model_src.diffusion_model import ConditionalUnet1D\n",
    "from robotics.model_src.visual_encoder import CNNVisualEncoder\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# visual_encoder = CLIPVisualEncoder().to(device)\n",
    "\n",
    "visual_encoder = CNNVisualEncoder().to(device)\n",
    "\n",
    "vision_feature_dim = visual_encoder.get_output_shape()\n",
    "\n",
    "action_observation_dim = 7\n",
    "\n",
    "obs_dim = vision_feature_dim + action_observation_dim\n",
    "\n",
    "action_dim = 7\n",
    "\n",
    "noise_prediction_net = ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim * obs_horizon,\n",
    ").to(device)"
   ],
   "id": "6a0e7af3f0cc470f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image = torch.Tensor(ds[0][\"img_obs\"][None, :obs_horizon, :, :, :]).to(device)\n",
    "act_obs = torch.Tensor(ds[0][\"act_obs\"][None, :obs_horizon, :]).to(device)"
   ],
   "id": "dfcf411fb96bd906",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "im = image[0,0, :, :].cpu().numpy()\n",
    "\n",
    "plt.imshow(im.transpose((1, 2, 0)))"
   ],
   "id": "3a28b9062ae5369e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    image_features = visual_encoder.encode(image.flatten(start_dim=0, end_dim=1))\n",
    "\n",
    "    image_features = image_features.reshape(*image.shape[:2], -1)\n",
    "\n",
    "    obs = torch.cat([image_features, act_obs], dim=-1)\n",
    "\n",
    "    noised_action = torch.randn((1, pred_horizon, action_dim)).to(device)\n",
    "\n",
    "    timestep_tensor = torch.randint(0, 101, (1,), device=device)\n",
    "\n",
    "    noise = noise_prediction_net(\n",
    "        sample=noised_action,\n",
    "        timestep=timestep_tensor,\n",
    "        global_cond=obs.flatten(start_dim=1)\n",
    "    )\n",
    "\n",
    "    denoised_action = noised_action - noise\n"
   ],
   "id": "404c3b9fc97655fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "\n",
    "num_diffusion_iters = 100\n",
    "\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    clip_sample=True,\n",
    "    prediction_type='epsilon'\n",
    ")"
   ],
   "id": "accd1216068cb090",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "from diffusers import EMAModel, get_scheduler\n",
    "\n",
    "num_epochs = 400\n",
    "\n",
    "# EMA params\n",
    "all_params = list(visual_encoder.parameters()) + list(noise_prediction_net.parameters())\n",
    "ema = EMAModel(parameters=all_params, power=0.75)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=all_params,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-6\n",
    ")\n",
    "\n",
    "# LR scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(dataloader) * num_epochs\n",
    ")\n",
    "\n",
    "# train loop\n",
    "for epoch_idx in range(num_epochs):\n",
    "    epoch_loss_sum = 0.0\n",
    "\n",
    "    for nbatch in dataloader:\n",
    "        # prepare data\n",
    "        nimage = nbatch['img_obs'][:, :obs_horizon].to(device)  # (B, H, 3,96,96)\n",
    "        action_obs = nbatch['act_obs'][:, :obs_horizon].to(device)  # (B, H, 2)\n",
    "        action_pred = nbatch['act_pred'].to(device)  # (B, P, 2)\n",
    "        B = action_obs.size(0)\n",
    "\n",
    "        # forward pass\n",
    "        image_features = visual_encoder.forward(\n",
    "            nimage.flatten(end_dim=1)  # (B*H,3,96,96)\n",
    "        ).reshape(*nimage.shape[:2], -1)  # (B, H, D=512)\n",
    "\n",
    "        obs_features = torch.cat([image_features, action_obs], dim=-1)\n",
    "        obs_cond = obs_features.flatten(start_dim=1)  # (B, H*obs_dim)\n",
    "\n",
    "        noise = torch.randn_like(action_pred)\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.config.num_train_timesteps,\n",
    "            (B,), device=device).long()\n",
    "\n",
    "        noisy_actions = noise_scheduler.add_noise(action_pred, noise, timesteps)\n",
    "        noise_pred = noise_prediction_net(noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "        loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step()\n",
    "        ema.step(all_params)\n",
    "\n",
    "        epoch_loss_sum += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss_sum / len(dataloader)\n",
    "    print(f\"Epoch {epoch_idx + 1:3d}/{num_epochs} â”€ average loss: {avg_loss:.6f}\")\n",
    "\n",
    "# copy EMA weights for inference\n",
    "ema.copy_to(all_params)"
   ],
   "id": "7997cb109c1bc32b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def save_final_models(visual_encoder, noise_pred_net, out_dir):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"visual_encoder\": visual_encoder.state_dict(),\n",
    "            \"noise_pred_net\": noise_pred_net.state_dict(),\n",
    "        },\n",
    "        out_dir / \"model_final.pth\",\n",
    "    )\n",
    "    print(f\"Saved to {out_dir / 'models.pth'}\")"
   ],
   "id": "2f67f591e91d4e7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_final_models(visual_encoder, noise_prediction_net,\n",
    "                  \"../models/robot_v5_can_cnn_griper_124\")"
   ],
   "id": "18f7349f71d96bd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_final_models(visual_encoder, noise_pred_net, ckpt_path, device=\"cuda\"):\n",
    "    ckpt_path = Path(ckpt_path)\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    visual_encoder.load_state_dict(state[\"visual_encoder\"], strict=True)\n",
    "    noise_pred_net.load_state_dict(state[\"noise_pred_net\"], strict=True)\n",
    "\n",
    "    visual_encoder.to(device).eval()\n",
    "    noise_pred_net.to(device).eval()\n",
    "    print(f\"Loaded weights from {ckpt_path}\")\n"
   ],
   "id": "930f3e6741b692cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "load_final_models(visual_encoder, noise_prediction_net, \"../models/v6_success_both_cnn_actions_h1/model_final.pth\")",
   "id": "89c55255cce7005",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "65c7ae7c4c252883"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import robomimic\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "import robomimic.utils.env_utils as EnvUtils\n",
    "import robomimic.utils.file_utils as FileUtils\n",
    "from robomimic.utils.vis_utils import depth_to_rgb\n",
    "from robomimic.envs.env_base import EnvBase, EnvType\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "env_meta = FileUtils.get_env_metadata_from_dataset(dataset_path=data_path)\n",
    "\n",
    "dummy_spec = dict(\n",
    "    obs=dict(\n",
    "        low_dim=[\"robot0_eef_pos\"],\n",
    "        rgb=[\"agentview_image\"]\n",
    "        # rgb=[\"robot0_eye_in_hand_image\"]\n",
    "    ),\n",
    ")\n",
    "\n",
    "ObsUtils.initialize_obs_utils_with_obs_specs(obs_modality_specs=dummy_spec)\n",
    "\n",
    "env = EnvUtils.create_env_from_metadata(env_meta=env_meta, render=True, render_offscreen=True, use_image_obs=True)\n",
    "\n",
    "a = env.reset()\n",
    "\n",
    "from collections import deque\n",
    "obs_deque  = deque(maxlen=obs_horizon)\n",
    "act_deque  = deque(maxlen=obs_horizon)\n",
    "rewards    = []\n",
    "imgs       = []\n",
    "step_idx   = 0\n",
    "\n",
    "max_steps = 1000\n",
    "action_horizon  = 4\n",
    "\n",
    "# â”€â”€â”€ 6. Main rollout â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "obs = env.reset()\n",
    "# wrap obs in same format as env.step\n",
    "obs = obs if isinstance(obs, dict) else obs[0]\n",
    "for i in range(obs_deque.maxlen):\n",
    "    obs_deque.append(obs)\n",
    "    act_deque.append(np.zeros(action_dim, dtype=np.float32))\n",
    "\n",
    "pbar = tqdm(total=max_steps)\n",
    "done = False\n",
    "\n",
    "while not done and step_idx < max_steps:\n",
    "    # 6.1 build the image & action history tensor\n",
    "    img_np = np.array([obs_deque[i][camera_type + \"_image\"] for i in range(obs_deque.maxlen)])\n",
    "\n",
    "    # img_np = obs_deque[-1][\"robot0_eye_in_hand_image\"]\n",
    "\n",
    "    # img_t   = torch.from_numpy(img_np.transpose(0, 3, 1, 2)).unsqueeze(0).float().to(device) / 255.0\n",
    "\n",
    "    img_t   = torch.from_numpy(img_np.transpose(0, 3, 1, 2)).float().to(device) / 255.0\n",
    "\n",
    "    actions_hist = torch.stack(\n",
    "        [torch.from_numpy(a) for a in list(act_deque)],\n",
    "        dim=0\n",
    "    ).to(device)                           # (1, H_a, 7)\n",
    "\n",
    "    # 6.2 compute visual features + conditioning\n",
    "    with torch.no_grad():\n",
    "        img_feat = visual_encoder(img_t)                # (1, C)\n",
    "        obs_cond = torch.cat([img_feat.flatten(start_dim=0).unsqueeze(0) , actions_hist.flatten(start_dim=0).unsqueeze(0)], dim=1)\n",
    "\n",
    "        # 6.3 sample a future action sequence via diffusion\n",
    "        B = 1\n",
    "        pred_actions = torch.randn((B, pred_horizon, action_dim), device=device)\n",
    "        noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "        for t in noise_scheduler.timesteps:\n",
    "            noise_pred    = noise_prediction_net(pred_actions, t, global_cond=obs_cond)\n",
    "            out           = noise_scheduler.step(noise_pred, t, pred_actions)\n",
    "            pred_actions  = out.prev_sample\n",
    "\n",
    "    pred_actions = pred_actions.cpu().numpy()[0]        # (pred_horizon, 7)\n",
    "\n",
    "    # 6.4 execute the next block of actions\n",
    "    start = obs_horizon\n",
    "    end   = start + action_horizon\n",
    "    action_block = pred_actions[start:end]          # (5, 7)\n",
    "\n",
    "    for act in action_block:\n",
    "        obs, rew, done, info = env.step(act)\n",
    "        obs = obs if isinstance(obs, dict) else obs[0]\n",
    "\n",
    "        frame = env.render(mode=\"rgb_array\", height=512, width=512)\n",
    "\n",
    "        obs_deque.append(obs)\n",
    "        act_deque.append(act.astype(np.float32))\n",
    "\n",
    "        rewards.append(rew)\n",
    "        imgs.append(frame)\n",
    "\n",
    "        step_idx += 1\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix(reward=float(rew))\n",
    "\n",
    "        if done or step_idx >= max_steps:\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# â”€â”€â”€ 7. Wrap up â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"Rollout finished: {step_idx} steps, total reward {sum(rewards):.2f}\")"
   ],
   "id": "c7de96297d547660",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import imageio\n",
    "\n",
    "video_path = \"test_2.mp4\"\n",
    "fps = 24\n",
    "\n",
    "with imageio.get_writer(video_path, fps=fps, codec=\"libx264\") as writer:\n",
    "    for frame in imgs:\n",
    "        writer.append_data(frame)\n",
    "\n",
    "print(f\"Saved video to {video_path}\")"
   ],
   "id": "9fe8151cf6b7358b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f62615db63e5ccd6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
