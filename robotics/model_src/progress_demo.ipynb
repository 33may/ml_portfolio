{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Project introduction\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Robots that move in unstructured environments still face a gap between what they see and how they should act. Diffusion models offer a way to bridge that gap because they generate complex outputs through a sequence of small refinements, which makes them robust to distribution shift and naturally multimodal. In this project we apply a conditional diffusion model to robotic control. From a handful of recent camera frames the policy should output a smooth two-dimensional trajectory for the robot end-effector that reaches a goal in a simulated plane. The broader aim is to keep the architecture so general that retraining on another state space—joint angles, 3-D Cartesian poses, or even a completely different task—requires almost no code changes.\n",
    "\n",
    "## Problem statement\n",
    "\n",
    "During training we observe expert demonstrations. Each demonstration supplies a stack of $K$ RGB frames and a ground-truth end-effector path of length $T$ in the plane. At test time the policy sees new frames and must produce a feasible path that completes the task despite novel object layouts and lighting. The current focus is two-dimensional tabletop manipulation, but the same network can be re-used for other robots or higher-dimensional workspaces after retraining.\n",
    "\n",
    "## Approach in brief\n",
    "\n",
    "A one-dimensional U-Net operates in trajectory space. To train it we add Gaussian noise to the expert path according to the forward diffusion distribution $q(a_t\\mid a_0)$. The network receives that noisy path, a sinusoidal embedding that codes only the diffusion step index $t$ (not the images), and a visual context vector $g$ extracted from the image stack. It predicts the noise component $\\hat\\varepsilon$ and is optimised with the mean-squared error $\\|\\varepsilon-\\hat\\varepsilon\\|^2$. At inference we start from pure Gaussian noise and iterate the learned reverse process until we recover a clean end-effector path.\n",
    "\n",
    "## High-level pipeline\n",
    "\n",
    "Raw frames pass through a visual encoder to obtain the latent vector $g$. A positional module turns the diffusion step $t$ into a sine-cosine code, and the concatenation $[p_t;g]$ conditions every residual block in the U-Net through FiLM. Convolutions run along the temporal axis of the trajectory, with down- and up-sampling providing a wide receptive field at modest cost. The final convolution maps features to the two Cartesian action channels $(x, y)$. A scheduler then converts the predicted noise into the next cleaner trajectory sample.\n",
    "\n",
    "## Expected outcomes\n",
    "\n",
    "The policy should produce end-effector paths that are dynamically consistent, visually grounded, and more robust than one-shot regression baselines. Because the positional embedding is tied only to the diffusion step and the U-Net is agnostic to how many channels the action space holds, the same code base can tackle joint-space control, 3-D positioning, or alternate robots by retraining on new demonstrations.\n",
    "\n",
    "## Roadmap and current status\n",
    "\n",
    "### Done:\n",
    "- Diffusion Models theory\n",
    "- Dataset structure and Class\n",
    "- Expert demonstrations collection setup\n",
    "- Visual Encoder\n",
    "- Sinusoidal step embeddings\n",
    "- Convolutional and residual blocks\n",
    "\n",
    "\n",
    "The next milestones are FiLM integration, full U-Net assembly, and evaluation on both transformer- and CNN-based visual encoders. Once these parts are finished we will run large-scale training in a 2-D simulator, followed by transfer tests to more complex state spaces without modifying the core architecture."
   ],
   "id": "858a9cfe827cfe27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Diffusion Models",
   "id": "8ebfef6d3e9dc60e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-05-14T14:04:27.980906Z",
     "start_time": "2025-05-14T13:59:23.865204Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "device = torch.device(\"mps\")",
   "id": "1705d0e52900ec16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "\n",
    "In this example notebook we will use the Swiss Roll points distribution, and take only $R^2$ version of it.\n",
    "The loaded data is normilized for stability during training and puted into the `Dataset` class to later use the `DataLoader` that manage batching and shuffling."
   ],
   "id": "2a7e94e721eeffae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_swiss_roll_data(n_samples=400, noise=0.2):\n",
    "    data, _ = make_swiss_roll(n_samples=n_samples, noise=noise)\n",
    "    data_2d = data[:, [0, 2]]\n",
    "    data_tensor = torch.tensor(data_2d, dtype=torch.float32)\n",
    "    data_tensor = (data_tensor - data_tensor.mean(dim=0)) / data_tensor.std(dim=0)\n",
    "    return data_tensor\n",
    "\n",
    "class SwissRollDataset(Dataset):\n",
    "    def __init__(self, n_samples=400, noise=0.2):\n",
    "        self.data = load_swiss_roll_data(n_samples=n_samples, noise=noise)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "def plot_data_points(all_data_tensor, draw_samples=None, ax = None):\n",
    "    \"\"\"\n",
    "    Visualizes 2D data points.\n",
    "\n",
    "    Parameters:\n",
    "      - all_data_tensor: torch.Tensor of shape [N, 2] containing the main data points.\n",
    "      - draw_samples: torch.Tensor or None of shape [M, 2] containing additional points to highlight (plotted in green).\n",
    "      - ax: matplotlib axes object\n",
    "    \"\"\"\n",
    "    data_np = all_data_tensor.detach().cpu().numpy()\n",
    "\n",
    "    canvas = ax if ax is not None else plt.gca()\n",
    "\n",
    "    if draw_samples is not None:\n",
    "        draw_samples_np = draw_samples.detach().cpu().numpy()\n",
    "    else:\n",
    "        draw_samples_np = None\n",
    "\n",
    "    alpha_value = 0.2 if draw_samples_np is not None else 1.0\n",
    "    canvas.scatter(data_np[:, 0], data_np[:, 1], alpha=alpha_value)\n",
    "\n",
    "    if draw_samples_np is not None:\n",
    "        canvas.scatter(draw_samples_np[:, 0], draw_samples_np[:, 1], c=\"purple\")\n",
    "\n",
    "    canvas.set_xticks([])\n",
    "    canvas.set_yticks([])"
   ],
   "id": "365c3afeafe275bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = SwissRollDataset(n_samples=5000, noise=0.2)\n",
    "\n",
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(data, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "plot_data_points(data[:], data[0:20])"
   ],
   "id": "e511c06d78ac78f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Diffusion model details\n",
    "\n",
    "Originally the diffusion model is idea, that came from statistical physics, in the domain of Machine Learning this is class of generative models. Initially it was introduced to generate images and performed good beating some of the traditional approaches like GAN and VAEs, during this explanantion block I will consider reviewing the case of image generation, but during the implementation and in Future Work section also will be covered how these kind of models perform on other modalities. The idea of training approach consist of 2 main steps:\n",
    "\n",
    "## Forward Pass\n",
    "On the high level the forward pass is the process that takes item $x_{t-1}$ at time step $t-1$ and produced the more noisy version of it $x_t$:\n",
    "$$\n",
    "q(x_t \\mid x_{t-1}) = \\mathcal{N}\\Bigl(x_t; \\sqrt{1-\\beta_t}\\, x_{t-1},\\, \\beta_t I\\Bigr)\n",
    "$$\n",
    "\n",
    "So $q$ here is PDF, which is described by parametrized Gaussian Distribution, that is used to sample the value of each pixel at next time step.\n",
    "On the interpretation level, using the scheduler $\\beta$ the function is described:\n",
    "1. *Mean*: Gradually decrease the mean with factor $\\sqrt{\\beta_t - 1}$, this means that the significance of the main signal is reduced every timestep;\n",
    "2. *Variance*: Described with covariation matrix $\\beta_t I$, where $I$ is one matrix for $R^{dim}$, this means noisy is added independently to each value of the input.\n",
    "\n",
    "By applying this stochastic transformation for large number of times, we arrive to the point where initial signal is just Gaussian Noise.\n",
    "\n",
    "### Implemenation trick\n",
    "\n",
    "Mostly while working with diffusion models we want to compute noisy image $x_t$ at time step $t$ given the original input $x_0$, of course we could just chain the computations and iteratively arrive to the desired timestep:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1 &= \\sqrt{1-\\beta_1}\\, x_0 + \\sqrt{\\beta_1}\\,\\epsilon_1, \\quad \\epsilon_1 \\sim \\mathcal{N}(0, I) \\\\\n",
    "x_2 &= \\sqrt{1-\\beta_2}\\, x_1 + \\sqrt{\\beta_2}\\,\\epsilon_2, \\quad \\epsilon_2 \\sim \\mathcal{N}(0, I) \\\\\n",
    "&\\ \\, \\vdots \\\\\n",
    "x_t &= \\sqrt{1-\\beta_t}\\, x_{t-1} + \\sqrt{\\beta_t}\\,\\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0, I)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "However, there is the trick that allows to come up with the image $x_t$ directly given the $x_0$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q(x_t | x_0) = \\sqrt{\\bar{\\alpha}_t}\\cdot x_0 + \\sqrt{1 - \\bar{\\alpha}_t}, \\quad \\text{where } \\quad\n",
    "\\alpha_t = 1 - \\beta_t\\\\\n",
    "\\bar{\\alpha}_t = \\prod_{i=1}^{n} a_i\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In the following math part the formula is derived, also the explanation of the new concept I learned during that called reparameterization trick:\n",
    "\n",
    "<img alt=\"Описание изображения\" src=\"math_img/forward_chain_p1.jpeg\" width=\"600\"/>\n",
    "\n",
    "<img alt=\"Описание изображения\" src=\"math_img/forward_chain_p2.jpeg\" width=\"600\"/>\n",
    "\n",
    "<img alt=\"Описание изображения\" src=\"math_img/reparametrizstion_trick.jpeg\" width=\"600\"/>\n"
   ],
   "id": "b11f2f9704f5d481"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Diffusion Scheduler\n",
    "\n",
    "Scheduler is important part of the diffusion networks, it is used to control the speed and \"shape\" of the noising process, for example:\n",
    "\n",
    "1. **Linear Scheduler**\n",
    "   - **Description:** The noise variance (β) increases linearly over time.\n",
    "   - **Theory:** With a linear schedule, each timestep adds a fixed increment of noise, making the process simple to implement. It works well when the diffusion process needs a constant rate of degradation.\n",
    "\n",
    "2. **Cosine Scheduler**\n",
    "   - **Description:** Uses a cosine function to schedule noise variance.\n",
    "   - **Theory:** A cosine scheduler adjusts the noise levels more gradually at the beginning and end of the diffusion process. This can help in preserving fine details early on and allows for smoother transitions later.\n",
    "\n",
    "3. **Quadratic Scheduler**\n",
    "   - **Description:** The noise variance increases quadratically with time.\n",
    "   - **Theory:** In a quadratic schedule, noise starts growing slowly at first and then increases more rapidly. This can be useful when initial timesteps should retain more of the original signal while later timesteps rapidly add noise.\n",
    "\n",
    "4. **Exponential Scheduler**\n",
    "   - **Description:** The noise variance increases exponentially over time.\n",
    "   - **Theory:** This scheduler adds little noise in the early stages and then rapidly increases the noise later. It is useful if you want the early diffusion process to be almost deterministic and introduce randomness primarily in later steps.\n",
    "\n",
    "5. **Custom Scheduler**\n",
    "   - **Description:** Any user-defined schedule that fits specific requirements.\n",
    "   - **Theory:** Depending on the dataset or application, custom schedules can be designed to control the \"shape\" of the noise addition. For instance, a scheduler might use a piecewise function that is linear in one interval and cosine in another.\n",
    "\n",
    "In this notebook the Linear scheduler will be used, but it is easily replacable by defining  the abstract scheduler class.\n"
   ],
   "id": "9a3cf5d6e2b60bef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "\n",
    "class BaseDiffusionSchedule(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self, timesteps, beta_start=0.0001, beta_end=0.02):\n",
    "        \"\"\"\n",
    "        Abstract base class for diffusion schedules.\n",
    "        Should be defined by subclasses.:\n",
    "          - sqrt_alphas_cumprod,\n",
    "          - sqrt_one_minus_alphas_cumprod.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_index(self, t):\n",
    "        \"\"\"\n",
    "        Abstract method that returns (sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod) for the time step t.\n",
    "\n",
    "        Params:\n",
    "        - t: time step,\n",
    "\n",
    "        Returns:\n",
    "            (sqrt alphas cumprod at t, sqrt one minus alphas cumprod at t)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class LinearDiffusionSchedule(BaseDiffusionSchedule):\n",
    "    def __init__(self, timesteps, beta_start=0.0001, beta_end=0.02, device='mps'):\n",
    "        self.timesteps = timesteps\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps, device=device)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - self.alphas_cumprod)\n",
    "\n",
    "    def get_index(self, t):\n",
    "        return self.sqrt_alphas_cumprod[t], self.sqrt_one_minus_alphas_cumprod[t]\n"
   ],
   "id": "83a7be8db8a9eddb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "noise_scheduler = LinearDiffusionSchedule(100)\n",
    "\n",
    "first = noise_scheduler.get_index(4)\n",
    "\n",
    "first"
   ],
   "id": "5477876c3bee2711"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Forward Diffusion Sample\n",
    "This function just applies the formula derived in the math block that produces $x_t | x_0$:\n",
    "$$\n",
    "q(x_t \\mid x_0) = \\sqrt{\\bar{\\alpha}_t}\\, x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon, \\quad \\text{where } \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "$$\n"
   ],
   "id": "c0baa221c1c2ba4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def forward_diffusion_sample(x_0, t):\n",
    "    \"\"\"\n",
    "    Takes the origial state of the vector and add noise to it accordingly to the timestep t.\n",
    "\n",
    "    Params:\n",
    "    - x_0: torch tensor 2d\n",
    "    - t: time step,\n",
    "\n",
    "    Returns:\n",
    "        The noisy version of vector at timestep t,\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_0, device=device)\n",
    "    sqrt_alphas_cumprod_t = noise_scheduler.sqrt_alphas_cumprod[t]\n",
    "    sqrt_one_minus_alphas_cumprod_t = noise_scheduler.sqrt_one_minus_alphas_cumprod[t]\n",
    "    noisy = sqrt_alphas_cumprod_t * x_0.to(device) + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "    return noisy, noise"
   ],
   "id": "4941faadcd09a337"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_vecs = data[0:100]\n",
    "\n",
    "test_vecs_display = test_vecs.clone()\n",
    "\n",
    "timesteps = torch.arange(100)\n",
    "\n",
    "k = 20\n",
    "\n",
    "fig, axes = plt.subplots(1, len(timesteps) // k, figsize=(20, min(8, 2 * k)))\n",
    "\n",
    "for i, timestep in enumerate(timesteps):\n",
    "    if  i % k == 0:\n",
    "        ax = axes[i // k]\n",
    "        ax.set_title(f\"Step {timestep.item()}\")\n",
    "        plot_data_points(data[:], test_vecs_display, ax)\n",
    "\n",
    "    test_vecs_display = forward_diffusion_sample(test_vecs, timestep)[0]\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "9565e4d4f65a3dd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note even though here we iterate over every value of $t$, you could see that the function is called without need of previous timestep. This is just to show the graduall process how points are distorted.",
   "id": "f1e0d371e8dd4003"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Basic ANN for noise estimation",
   "id": "3fb043d905a80df3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import random\n",
    "\n",
    "class NoiseEstimator(nn.Module):\n",
    "    def __init__(self, input_dim, timestep_dim):\n",
    "        super(NoiseEstimator, self).__init__()\n",
    "\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            nn.Linear(1, timestep_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(timestep_dim, timestep_dim)\n",
    "        ).to(device)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + timestep_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, data, timestep):\n",
    "        time_step = torch.full((data.shape[0], 1), float(timestep), dtype=torch.float, device=data.device)\n",
    "        t_emb = self.time_embedding(time_step)\n",
    "        x_input = torch.cat([data, t_emb], dim=1)\n",
    "        noise_pred = self.model(x_input)\n",
    "        return noise_pred"
   ],
   "id": "c0b7288736497074"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train",
   "id": "359fa32b18a24d28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train(model, data_loader, optimizer, epochs=100):\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        acc_loss = 0\n",
    "\n",
    "        for batch_idx, data in enumerate(data_loader):\n",
    "            data = data.to(device)\n",
    "\n",
    "            timestep = random.randint(0, noise_scheduler.timesteps-1)\n",
    "\n",
    "            noised_vectors, actual_noise = forward_diffusion_sample(data, timestep)\n",
    "\n",
    "            predicted_noise = model(noised_vectors, timestep)\n",
    "\n",
    "            loss = F.mse_loss(predicted_noise, actual_noise)\n",
    "\n",
    "            acc_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}: Loss = {acc_loss / (len(data_loader) * data_loader.batch_size) :.4f}\")\n",
    "        epoch_loss.append(np.mean(acc_loss))\n",
    "\n",
    "        plt.plot(epoch_loss)"
   ],
   "id": "81041febe7f32236"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "noise_model = NoiseEstimator(input_dim=2, timestep_dim=16)\n",
    "optimizer_instance = torch.optim.Adam(noise_model.parameters(), lr=0.001)\n",
    "train_acc = train(noise_model, train_loader, epochs=400, optimizer=optimizer_instance)"
   ],
   "id": "89a80d7b8044720f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@torch.no_grad()\n",
    "def reverse_diffusion_sample(model, noise_scheduler, shape, period = 1, device='mps'):\n",
    "    \"\"\"\n",
    "    Generates data using the reverse diffusion process.\n",
    "\n",
    "    Parameters:\n",
    "      - model: The noise prediction model.\n",
    "      - noise_scheduler: Object containing betas, alphas, and alphas_cumprod.\n",
    "      - shape: Shape of the generated data (e.g., [batch_size, 2]).\n",
    "      - device: Device to run the computations on.\n",
    "\n",
    "    Returns:\n",
    "      - x: Generated data tensor.\n",
    "    \"\"\"\n",
    "    # Start from pure noise\n",
    "    x = torch.randn(shape, device=device)\n",
    "\n",
    "    for t in reversed(range(noise_scheduler.timesteps)):\n",
    "        # Predict the noise component using the model\n",
    "        pred_noise = model(x, t)\n",
    "\n",
    "        beta_t = noise_scheduler.betas[t]\n",
    "        alpha_t = noise_scheduler.alphas[t]\n",
    "        alpha_bar_t = noise_scheduler.alphas_cumprod[t]\n",
    "        sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "        sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - alpha_bar_t)\n",
    "\n",
    "        # Compute the mean of the reverse process distribution:\n",
    "        mean = (x - (beta_t / sqrt_one_minus_alpha_bar_t) * pred_noise) / sqrt_alpha_t\n",
    "\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            sigma_t = torch.sqrt(beta_t)\n",
    "            # Update x_t to x_{t-1}\n",
    "            x = mean + sigma_t * noise\n",
    "        else:\n",
    "            x = mean\n",
    "\n",
    "        if t % period == 0:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plot_data_points(data[:], x)\n",
    "            plt.title(f\"Step {t}\")\n",
    "            plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plot_data_points(data[:], x)\n",
    "    plt.title(\"Final result\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "eval_vecs = torch.randn((2000,2)).to(device) * 2\n",
    "\n",
    "reverse_diffusion_sample(noise_model, noise_scheduler, (200,2), period=10)"
   ],
   "id": "2cd00b6050862944"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "dc9aba048903d3c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Zarr Dataset structure",
   "id": "a7de05f8f4ca243f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dataset = zarr.open(\"../data/demonstrations_snapshot_1.zarr\", mode=\"r\")",
   "id": "367e3e2dd79cfbd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = dataset[\"data\"]\n",
    "imgs = data[\"img\"][:]\n",
    "actions = data[\"action\"][:]\n",
    "episode_ends = dataset[\"episode_ends\"][:]"
   ],
   "id": "abfd1b1130b9cd4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Length of images: {len(imgs)}\")\n",
    "print(f\"Action shape: {actions[0].shape}\")\n",
    "print(f\"Length of actions: {len(actions)}\")\n",
    "print(f\"Image shape: {imgs[0].shape}\")\n",
    "print(f\"Length of episode_ends: {len(episode_ends)}\")\n",
    "\n",
    "img = imgs[340]\n",
    "plt.imshow(img/ 255)"
   ],
   "id": "5d6d9e758a9dff6d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Understanding of dataset\n",
    "\n",
    "* **actions**\n",
    "  `[1,3,5,2,1,7,9,4,3,2]`\n",
    "* **images**\n",
    "  `[img1,img2,img3,img4,img5,img6,img7,img8,img9,img10]`\n",
    "\n",
    "#### Proper `episode_ends` array\n",
    "\n",
    "The recording code appends **cumulative** episode lengths.\n",
    "With three episodes of lengths\n",
    "\n",
    "* episode1=4 steps\n",
    "* episode2=4 steps\n",
    "* episode3=2 steps\n",
    "\n",
    "we get:\n",
    "\n",
    "```python\n",
    "episode_ends = [0, 4, 8, 10]\n",
    "```\n",
    "\n",
    "#### Split\n",
    "\n",
    "| Episode | Slice in the array | actions     | images                 |\n",
    "| ------- | ------------ | ----------- | ---------------------- |\n",
    "| 1       | `0:4`        | `[1,3,5,2]` | `[img1,img2,img3,img4]` |\n",
    "| 2       | `4:8`        | `[1,7,9,4]` | `[img5,img6,img7,img8]` |\n",
    "| 3       | `8:10`       | `[3,2]`     | `[img9,img10]`         |\n"
   ],
   "id": "1855069e2b998619"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from robotics.model_src.dataset import generate_sample_dataset\n",
    "\n",
    "imgs, actions, episode_ends = generate_sample_dataset(20)"
   ],
   "id": "dbce6ee71fc2bce2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "actions",
   "id": "1e985657fe295112"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "episode_ends",
   "id": "bc7b36331e54b87a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Class",
   "id": "95ea140ced67f78f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import zarr\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# ---------- helpers -----------------------------------------------------------\n",
    "def create_trajectory_indices(episode_ends: np.ndarray,\n",
    "                              horizon_left: int,\n",
    "                              horizon_right: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pre‑compute every possible window (one row = full window indices).\n",
    "\n",
    "    episode_ends  – cumulative end indices, e.g. [0, 4, 8, 10]\n",
    "    horizon_left  – how many frames *before* current step to feed as obs\n",
    "    horizon_right – how many future actions to predict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, shape = (N_windows, W)\n",
    "        W = horizon_left + horizon_right + 1\n",
    "        Each row already clipped to the episode boundaries.\n",
    "    \"\"\"\n",
    "    all_windows = []\n",
    "    start_idx = 0\n",
    "    window_template = np.arange(-horizon_left, horizon_right + 1) # [W,]\n",
    "    for i in tqdm(range(len(episode_ends) - 1), total=len(episode_ends) - 1):\n",
    "        end_idx = episode_ends[i + 1]\n",
    "        if i > 0:\n",
    "            start_idx = episode_ends[i] + 1 # first valid frame in ep\n",
    "\n",
    "        base = np.arange(start_idx, end_idx)[:, None] # [L, 1]\n",
    "\n",
    "        windows = base + window_template # [L, W]\n",
    "\n",
    "        np.clip(windows, start_idx, end_idx, out=windows) # padding\n",
    "\n",
    "        all_windows.append(windows)\n",
    "\n",
    "    return np.concatenate(all_windows, axis=0) # (N, W)\n",
    "\n",
    "\n",
    "def normalize_data(arr, scale):\n",
    "    # map raw values from [0, scale] into canonical [0, 1] range\n",
    "    return arr / scale\n",
    "\n",
    "\n",
    "def denormalize_data(arr, scale):\n",
    "    # recover original units by reversing the previous scaling\n",
    "    return arr * scale\n",
    "\n",
    "\n",
    "# ---------- dataset -----------------------------------------------------------\n",
    "class PushTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch dataset that returns:\n",
    "        img_obs  – images for the observation horizon (oh,H,W,C)\n",
    "        act_obs  – actions for the observation horizon (oh, 2)\n",
    "        act_pred – actions for the prediction horizon (ph, 2)\n",
    "    All indices are pre‑computed once in create_trajectory_indices().\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, obs_horizon, prediction_horizon, image_size = None):\n",
    "        self.obs_horizon = obs_horizon\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "\n",
    "        dataset = zarr.open(data_path, mode=\"r\") # action, img, keypoint, n_contacts, state\n",
    "\n",
    "        # --- images ---------------------------------------------------------\n",
    "        image_data = dataset[\"data\"][\"img\"][:] # ndarray [0-255], shape = (total, 224, 224, 3)\n",
    "        self.image_data_transformed = normalize_data(image_data, 255) # ndarray [0-1], shape = (total, 224, 224, 3)\n",
    "\n",
    "        # --- actions --------------------------------------------------------\n",
    "        actions_data = dataset[\"data\"][\"action\"][:] # ndarray [0-512], shape = (total, 2)\n",
    "        self.actions_data_transformed = normalize_data(actions_data, 512) # ndarray [0-1], shape = (total, 2)\n",
    "\n",
    "        # --- windows --------------------------------------------------------\n",
    "        self.episode_ends = dataset[\"episode_ends\"][:]\n",
    "        self.indexes = create_trajectory_indices(self.episode_ends, obs_horizon, prediction_horizon)\n",
    "\n",
    "    # total number of windows\n",
    "    def __len__(self):\n",
    "        return len(self.indexes)\n",
    "\n",
    "    # slice arrays by pre‑computed row of indices\n",
    "    def __getitem__(self, idx):\n",
    "        trajectory_idx = self.indexes[idx]\n",
    "\n",
    "        img_obs  = self.image_data_transformed[trajectory_idx[:self.obs_horizon + 1]]\n",
    "        act_obs  = self.actions_data_transformed[trajectory_idx[:self.obs_horizon + 1]]\n",
    "        act_pred = self.actions_data_transformed[trajectory_idx[self.obs_horizon + 1:]]\n",
    "\n",
    "        return {\n",
    "            \"img_obs\" : img_obs,\n",
    "            \"act_obs\" : act_obs,\n",
    "            \"act_pred\" : act_pred,\n",
    "        }"
   ],
   "id": "1abc0fa89e2d585b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from robotics.model_src.dataset import PushTDataset\n",
    "\n",
    "dataset = PushTDataset(\"../data/demonstrations_snapshot_1.zarr\", obs_horizon=4, prediction_horizon=8, image_size=None)"
   ],
   "id": "e3cb9a7877e91b8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "item = dataset[0]",
   "id": "8f5fe527e6f44208"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Observation images: {item[\"img_obs\"].shape}\")\n",
    "print(f\"Observation actions: {item[\"act_obs\"].shape}\")\n",
    "print(f\"Prediction actions: {item[\"act_pred\"].shape}\")"
   ],
   "id": "53b2a79047885d0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Expert demonstration collection\n",
    "\n",
    "For the initial experiment I am using the PushT env. The env is not vanilla, I have forked it from official repo and modified to better suit my use-case, I will provide the changes in the final report of the project."
   ],
   "id": "bbc6f037e0b5cc49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# init arrays for data\n",
    "\n",
    "actions = []\n",
    "images = []"
   ],
   "id": "e0403d00146d5d44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from robotics.gym_pusht.envs.pusht import PushTEnv\n",
    "import time\n",
    "\n",
    "env = PushTEnv(obs_type=\"pixels\", render_mode=\"human\", goal_pose=\"random\")\n",
    "teleop = env.teleop_agent()\n",
    "obs, info = env.reset()"
   ],
   "id": "675af3a8b6e1e4b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "step_in_episode = 0\n",
    "episode_ends = [0]\n",
    "\n",
    "while True:\n",
    "    time.sleep(0.1)\n",
    "    action = teleop.act(obs)\n",
    "\n",
    "    if action is None:\n",
    "        env.render()\n",
    "        continue\n",
    "\n",
    "    images.append(obs)\n",
    "    actions.append(action)\n",
    "\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    obs = next_obs\n",
    "    step_in_episode += 1\n",
    "\n",
    "    if terminated or truncated:\n",
    "        episode_ends.append(episode_ends[-1] + step_in_episode)\n",
    "        step_in_episode = 0\n",
    "        obs, _ = env.reset()"
   ],
   "id": "2fcbd619990eb6b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save Dataset snapshot",
   "id": "d9806f86338aea57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Prepare file\n",
    "dataset_version = 1\n",
    "\n",
    "file_name = f\"./data/demonstrations_snapshot_{dataset_version}.zarr\"\n",
    "\n",
    "## Prepare data\n",
    "actions_np = np.array(actions)\n",
    "imgs_np = np.array(images)\n",
    "episode_ends_np = np.array(episode_ends)"
   ],
   "id": "ee097a11698a615d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file = zarr.open(file_name, mode=\"w\")\n",
    "data_group = file.create_group(\"data\")\n",
    "\n",
    "# 1) action\n",
    "arr_a = data_group.create_array(\n",
    "    name=\"action\",\n",
    "    shape=actions_np.shape,\n",
    "    dtype=actions_np.dtype,\n",
    "    chunks=actions_np.shape,\n",
    ")\n",
    "arr_a[:] = actions_np\n",
    "\n",
    "# 2) img\n",
    "arr_i = data_group.create_array(\n",
    "    name=\"img\",\n",
    "    shape=imgs_np.shape,\n",
    "    dtype=imgs_np.dtype,\n",
    "    chunks=imgs_np.shape,\n",
    ")\n",
    "arr_i[:] = imgs_np\n",
    "\n",
    "# 3) episode_ends\n",
    "arr_e = file.create_array(\n",
    "    name=\"episode_ends\",\n",
    "    shape=episode_ends_np.shape,\n",
    "    dtype=episode_ends_np.dtype,\n",
    "    chunks=episode_ends_np.shape,\n",
    ")\n",
    "arr_e[:] = episode_ends_np"
   ],
   "id": "fb65fb3df0998017"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Models\n",
   "id": "9a90dc7c28160afe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visual encoder\n",
    "\n",
    "### Abstraction layer\n",
    "\n",
    "The code defines a `VisualEncoderBase` interface with four compulsory methods—`load_model`, `preprocess`, `encode`, and `get_output_shape`—plus a simple `to` helper that moves everything to the required device. This thin wrapper isolates the policy from any specific vision backbone, so swapping architectures never touches downstream diffusion or FiLM code. The interface also enforces consistent batching and precision handling across all visual variants.\n",
    "\n",
    "### Transformer baseline\n",
    "\n",
    "`CLIPVisualEncoder` instantiates the interface with a ViT-B/32 backbone from CLIP. Pre-processing follows the official CLIP recipe (resize, center-crop, normalize to ±1) and the forward pass returns a 512-dimensional, L2-normalized embedding via `get_image_features`. Running the model in `eval()` freezes all weights by default, but the design still lets you un-freeze selective layers for fine-tuning experiments. The transformer path is expected to capture high-level semantics that may improve generalisation to cluttered real-world scenes.\n",
    "\n",
    "### CNN alternative\n",
    "\n",
    "A second implementation, tentatively named `CNNVisualEncoder`, will plug a lightweight convolutional backbone such as ResNet-34 or EfficientNet-B0 into the same interface. After the global-average-pool, a small linear head projects to the same 512-D space used by CLIP, keeping the rest of the pipeline unchanged. A CNN offers lower latency, stronger inductive bias for limited data, and better compatibility with edge devices or real-time control loops.\n",
    "\n",
    "### Evaluation rationale\n",
    "\n",
    "Both encoders will be dropped into the diffusion policy unchanged, letting us measure three aspects: sample efficiency during fine-tuning, robustness to sim-to-real shifts, and pure throughput in frames per second. The plan is to keep only the best-performing backbone as the default, while retaining the common interface so future backbones remain one import away.\n"
   ],
   "id": "f82c826069800b67"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-14T14:12:46.222652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "class VisualEncoderBase(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for visual encoders.\n",
    "    All custom encoders must inherit from this class and implement the required methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.device = device\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Loads the pretrained model.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def preprocess(self, image):\n",
    "        \"\"\"\n",
    "        Preprocesses a single image or a batch of images for the encoder.\n",
    "        Returns a tensor ready to be passed to the model.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def encode(self, image_tensor):\n",
    "        \"\"\"\n",
    "        Runs the model on the input tensor and returns the embeddings.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_output_shape(self):\n",
    "        \"\"\"\n",
    "        Runs the model on the input tensor and returns the embeddings.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def to(self, device):\n",
    "        \"\"\"\n",
    "        Moves the model to the specified device.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "\n",
    "class CLIPVisualEncoder(VisualEncoderBase):\n",
    "    def __init__(self, model_name=\"openai/clip-vit-base-patch32\", device=\"cuda\"):\n",
    "        super().__init__(device)\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.model_name = model_name\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        self.processor = CLIPProcessor.from_pretrained(self.model_name)\n",
    "        self.model = CLIPModel.from_pretrained(self.model_name)\n",
    "        self.model.eval().to(self.device)\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        return inputs[\"pixel_values\"].to(self.device)\n",
    "\n",
    "    def encode(self, image_tensor):\n",
    "        inputs = self.preprocess(image_tensor)\n",
    "        with torch.no_grad():\n",
    "            return self.model.get_image_features(pixel_values=inputs)\n",
    "\n",
    "    def get_output_shape(self):\n",
    "        return self.model.config.projection_dim"
   ],
   "id": "c8336e8b7ad90704",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Observation aggregator\n",
    "\n",
    "Not implemented yet\n",
    "\n",
    "\n",
    "*Purpose:* Collapse the last $K=\\texttt{obs\\_horizon}$ embeddings $\\{e_0,\\dots,e_{K-1}\\}$ into a fixed-length context $g$.\n",
    "- *Concat* → $g\\in\\mathbb{R}^{K d_{\\text{img}}}$\n",
    "- *Average* → $g\\in\\mathbb{R}^{d_{\\text{img}}}$\n"
   ],
   "id": "fa68acb6d4833f9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Sinusoidal positional embedding\n",
    "*Purpose:* Encode the diffusion step index $t$ into a vector $p_t\\in\\mathbb{R}^{d_t}$ with\n",
    "\n",
    "why we even need this. The idea of positional encodings adds the information about the order of the input embeddings. In our problem this is essential part of the noise prediction model design, since the main model should know which step it is now (how much noise will be added). The positional encoding helps to fix this problem by providing an additional component (in our case, addition will be the choice, since concatenating the vectors sounds more expressive but adds extra computational load). Such positional encoding should have 4 main properties:\n",
    "\n",
    "1. **uniqueness** (position ≡ value)\n",
    "2. **determinism** (constructed from a known function — this lets the model learn to exploit structure)\n",
    "3. **distance awareness** (positional similarity encodes proximity)\n",
    "4. **generalization** (it should extrapolate to longer sequences than seen during training)\n",
    "\n",
    "there are multiple approaches — learned embeddings, relative encodings, rotary — but sinusoidal encoding is often enough when the exact structure is not language-sensitive but rather needs stable, general-purpose spatial/temporal indices. in this project we rely on sinusoidal encoding because of its simplicity, efficiency, and ability to generalize without extra parameters.\n",
    "\n",
    "### Math and intuition\n",
    "\n",
    "for a given position `pos` and embedding dimension index `i`, the positional encoding is defined as:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "* `d` is the total embedding dimension.\n",
    "* every pair of sine and cosine encodes one frequency.\n",
    "* **low i ⇒ high frequency** (short patterns), **high i ⇒ low frequency** (longer patterns)\n",
    "\n",
    "### Why 10000?\n",
    "\n",
    "the `10000^(2i/d)` term ensures that frequencies vary exponentially across dimensions. small `i` results in fast oscillations (fine-grained encoding), while large `i` results in slower changes (coarse-grained encoding). this gives the model a sort of **Fourier basis**, where attention mechanisms can match patterns at different temporal scales.\n",
    "\n",
    "### Derivatives and learning\n",
    "\n",
    "a nice side-effect: the derivatives of `sin` and `cos` are bounded, periodic functions. this means the gradients during backprop stay stable over long sequences, avoiding vanishing or exploding gradients — which is useful for deep models and long time horizons.\n",
    "\n",
    "\n"
   ],
   "id": "82ecaa7bc77979e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def sinusoidal_pos_emb(\n",
    "    pos: int | torch.Tensor,\n",
    "    dim: int,\n",
    "    base: float = 10000.0,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute a sinusoidal positional embedding for a single position.\n",
    "\n",
    "    Args:\n",
    "    pos   : int or 0-D tensor – position index.\n",
    "    dim   : int – embedding dimension (must be even).\n",
    "    base  : float – base for the exponential frequency scaling.\n",
    "\n",
    "    Returns:\n",
    "    Tensor of shape (dim,) containing the positional embedding.\n",
    "    \"\"\"\n",
    "    pos = torch.as_tensor(pos, dtype=torch.float)        # if pos was int\n",
    "    if pos.dim() == 0:                                   # scalar → (1,)\n",
    "        pos = pos.unsqueeze(0)\n",
    "\n",
    "    # Half of the dimensions will be sine, half cosine\n",
    "    half_dim = dim // 2\n",
    "\n",
    "    # Exponent term:  base^{2i/dim}  for i = 0 .. half_dim-1\n",
    "    exponent = torch.arange(half_dim, dtype=torch.float)\n",
    "    div_term = base ** (2 * exponent / dim)  # (half_dim,)\n",
    "\n",
    "    # Compute the value: pos / base^{2i/dim}\n",
    "    value = pos.unsqueeze(-1) / div_term                    # (num_pos, half_dim)\n",
    "\n",
    "    emb = torch.empty(pos.size(0), dim, dtype=torch.float)\n",
    "    emb[:, 0::2] = torch.sin(value)              # even indices  -> sin\n",
    "    emb[:, 1::2] = torch.cos(value)              # odd  indices -> cos\n",
    "    return emb\n",
    "\n",
    "\n",
    "def visualize_encoding(vectors):\n",
    "    plt.imshow(vectors.permute(1, 0), aspect='auto', cmap='viridis')\n",
    "    plt.colorbar(label='value')\n",
    "    plt.ylabel('embedding dimension')\n",
    "    plt.xlabel('position')\n",
    "    plt.title('Sinusoidal Embeddings heatmap view')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "2768239701b4774d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualisation",
   "id": "d6ec57dd77809f27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "positions = torch.arange(256)\n",
    "\n",
    "embeddings = sinusoidal_pos_emb(positions, 64)\n",
    "\n",
    "visualize_encoding(embeddings)"
   ],
   "id": "fbe9afafa3884bab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "**4. Diffusion-step MLP**\n",
    "*Purpose:* Push $p_t$ through $\\text{Linear}\\,\\rightarrow\\,\\text{Mish}\\,\\rightarrow\\,\\text{Linear}$ (dimension preserved) to give the model nonlinear capacity over the time index.\n",
    "\n",
    "**5. Condition builder**\n",
    "*Purpose:* Concatenate the time code and visual context\n",
    "\n",
    "$$\n",
    "\\text{cond} = [\\,p_t\\,;\\,g\\,] \\in \\mathbb{R}^{d_t + C_{\\text{cond}}},\n",
    "$$\n",
    "\n",
    "which will modulate every FiLM layer.\n",
    "\n",
    "**6. Conv1d block $(\\text{in\\_c},\\text{out\\_c},k)$**\n",
    "*Conv1d → GroupNorm → activation*; keeps sequence length $T=\\texttt{pred\\_horizon}$.\n",
    "\n",
    "**7. ResidualBlock1D**\n",
    "Two Conv1d blocks plus an identity skip; eases gradient flow.\n",
    "\n",
    "**8. ConditionalResidualBlock1D**\n",
    "Same as (7) but FiLM-modulated: from **cond** a linear layer predicts $[\\text{scale};\\text{bias}]\\in\\mathbb{R}^{2C_{\\text{out}}}$ and applies\n",
    "\n",
    "$$\n",
    "h' = \\text{scale}\\,\\cdot h + \\text{bias}.\n",
    "$$\n",
    "\n",
    "**9. Downsample1d**\n",
    "$\\text{Conv1d}(\\text{stride}=2)$ halves the temporal length $T$.\n",
    "\n",
    "**10. Upsample1d**\n",
    "$\\text{ConvTranspose1d}(\\text{stride}=2)$ restores $T$.\n",
    "\n",
    "**11. ConditionalUnet1D**\n",
    "\n",
    "* Down path (repeated $L$ times): **CondRes → CondRes → Down**\n",
    "* Bottleneck: two **CondRes** blocks\n",
    "* Up path (repeated $L$ times): **Upsample → concat skip → CondRes → CondRes**\n",
    "* Final: **Conv1d block → Conv1d(1 × 1)** to get $C_{\\text{action}}=2$.\n",
    "\n",
    "**12. Reverse-process scheduler**\n",
    "Uses $(a_t,\\hat\\varepsilon,t)$ to compute $a_{t-1}$ under a chosen DDPM or DDIM rule.\n",
    "\n",
    "**13. Loss**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}=\\mathbb{E}\\bigl[\\;\\|\\varepsilon-\\hat\\varepsilon_\\theta(a_t,t,\\phi(o))\\|^2\\bigr],\n",
    "$$\n",
    "\n",
    "optimising image encoder, FiLM projections and all convolutional weights.\n",
    "\n",
    "---\n",
    "\n",
    "## One training / inference step\n",
    "\n",
    "1. Collect the last $K$ frames, encode each with the visual encoder, aggregate to get $g$.\n",
    "2. Encode the current diffusion step: $t \\rightarrow p_t$.\n",
    "3. Build the global condition $\\text{cond}=[p_t;g]$.\n",
    "4. Feed the noisy action sequence $a_t$ (reshaped to $(B,2,T)$) through the Conditional U-Net; FiLM layers inject **cond** at every scale.\n",
    "5. The U-Net outputs $\\hat\\varepsilon$:\n",
    "\n",
    "   * **Training:** compute the loss above and update parameters.\n",
    "   * **Inference:** the scheduler moves $a_t \\mapsto a_{t-1}$; repeat until $a_0$ (the clean action trajectory) is produced.\n",
    "\n",
    "Visual information thus modulates every layer via FiLM, the time embedding tells the network how much noise to remove, and the 1-D convolutional U-Net learns a coherent action sequence of length $\\texttt{pred\\_horizon}$ conditioned on the observations.\n"
   ],
   "id": "dd5767ac28876d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "68306364eb1668a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
