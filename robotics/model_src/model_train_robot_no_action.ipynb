{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0314445227a3baf",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "30e071ec36273394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T22:51:53.487190Z",
     "start_time": "2025-06-28T22:51:52.449504Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "c62ccd11679045ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T22:51:54.050106Z",
     "start_time": "2025-06-28T22:51:54.002534Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T22:51:54.472563Z",
     "start_time": "2025-06-28T22:51:54.465688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import h5py\n",
    "\n",
    "generator = torch.Generator().manual_seed(33)\n",
    "\n",
    "persistent_workers = False\n",
    "batch_size = 128\n",
    "\n",
    "camera_type = [\"agentview\", \"robot0_eye_in_hand\"]\n",
    "\n",
    "pred_horizon = 8\n",
    "obs_horizon = 1\n",
    "\n",
    "data_path = \"../robomimic/datasets/tool_hang/ph/image_agent_hand.hdf5\""
   ],
   "id": "3dba3aed726029ba",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from robotics.model_src.dataset import preprocess_images_in_place\n",
    "#\n",
    "# preprocess_images_in_place(\n",
    "#     h5_path=data_path,\n",
    "#     cameras=camera_type,\n",
    "#     target_dtype=np.float32,\n",
    "# )"
   ],
   "id": "6eb090adf92dd9c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T22:51:56.299859Z",
     "start_time": "2025-06-28T22:51:56.217492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import torch\n",
    "import gc\n",
    "import ctypes\n",
    "from torch.utils.data import DataLoader\n",
    "from robotics.model_src.dataset import RobosuiteImageActionDataset\n",
    "\n",
    "\n",
    "class EnsembleLoader:\n",
    "    \"\"\"\n",
    "    A helper that keeps several (Dataset, DataLoader) pairs and exposes a single\n",
    "    “active” pair. Call rotate() to switch the active pair and release memory\n",
    "    used by the previous one.\n",
    "\n",
    "        mgr = DemoDataManager(...)\n",
    "        ds  = mgr.get_ds()           # active dataset\n",
    "        ld  = mgr.get_loader()       # active dataloader\n",
    "        mgr.rotate()                 # switch to next pair\n",
    "        idx = mgr.get_active()       # index of the active pair\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_ds(data_path, camera, obs_h, pred_h, demo_subset):\n",
    "        return RobosuiteImageActionDataset(\n",
    "            data_path,\n",
    "            camera,\n",
    "            obs_horizon=obs_h,\n",
    "            pred_horizon=pred_h,\n",
    "            demos=demo_subset,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_loader(ds, batch_size, num_workers, shuffle, gen, persistent):\n",
    "        return DataLoader(\n",
    "            ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            generator=gen if shuffle else None,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=persistent,\n",
    "        )\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        chunk_size: int = 30,\n",
    "        validation_size: int = 20,\n",
    "        batch_size: int = 256,\n",
    "        obs_horizon: int = 1,\n",
    "        pred_horizon: int = 8,\n",
    "        camera: list[str] = [\"agentview\"],\n",
    "        num_workers: int = 4,\n",
    "        persistent_workers: bool = False,\n",
    "        seed: int = 33,\n",
    "    ):\n",
    "        f = h5py.File(data_path, \"r\")\n",
    "\n",
    "        data = f[\"data\"]\n",
    "\n",
    "        demos = list(data.keys())\n",
    "\n",
    "        g = torch.Generator().manual_seed(seed)\n",
    "\n",
    "        # validation set\n",
    "        self.val_ds = self._build_ds(\n",
    "            data_path, camera, obs_horizon, pred_horizon, demos[-validation_size:]\n",
    "        )\n",
    "        self.val_loader = self._build_loader(\n",
    "            self.val_ds, batch_size, num_workers, False, g, True\n",
    "        )\n",
    "\n",
    "        # training sets divided into chunks\n",
    "        train_demos = demos[:-validation_size]\n",
    "        self.train_pairs = []\n",
    "        for i in range(0, len(train_demos), chunk_size):\n",
    "            subset = train_demos[i : i + chunk_size]\n",
    "            ds = self._build_ds(data_path, camera, obs_horizon, pred_horizon, subset)\n",
    "            ld = self._build_loader(ds, batch_size, num_workers, True, g, persistent_workers)\n",
    "            self.train_pairs.append((ds, ld))\n",
    "            ds.drop_data()  # keep RAM usage low\n",
    "\n",
    "        self.active = 0\n",
    "        self.train_pairs[0][0].load_data()\n",
    "\n",
    "    def rotate(self, use_random = False):\n",
    "        prev = self.active\n",
    "\n",
    "        # stop workers and drop current data\n",
    "        self._stop_workers(self.train_pairs[prev][1])\n",
    "        self.train_pairs[prev][0].drop_data()\n",
    "\n",
    "        # move to next index\n",
    "        if use_random:\n",
    "            self.active = random.randint(0, len(self.train_pairs) - 1)\n",
    "        else:\n",
    "            self.active = (self.active + 1) % len(self.train_pairs)\n",
    "        print(f\"rotating from {prev} to {self.active} dataset\")\n",
    "\n",
    "        # load data for the new active dataset\n",
    "        self.train_pairs[self.active][0].load_data()\n",
    "\n",
    "        self._trim()\n",
    "\n",
    "    def get_ds(self, idx: int | None = None):\n",
    "        if idx is None:\n",
    "            idx = self.active\n",
    "        return self.train_pairs[idx][0]\n",
    "\n",
    "    def get_loader(self, idx: int | None = None):\n",
    "        if idx is None:\n",
    "            idx = self.active\n",
    "        return self.train_pairs[idx][1]\n",
    "\n",
    "    def get_val_loader(self, idx: int | None = None):\n",
    "        return self.val_loader\n",
    "\n",
    "    def get_active_loader(self) -> int:\n",
    "        return self.get_loader(self.active)\n",
    "\n",
    "    @staticmethod\n",
    "    def _stop_workers(loader):\n",
    "        it = getattr(loader, \"_iterator\", None)\n",
    "        if it is not None:\n",
    "            it._shutdown_workers()\n",
    "            loader._iterator = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _trim():\n",
    "        gc.collect()\n",
    "        try:\n",
    "            ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "        except OSError:\n",
    "            pass\n"
   ],
   "id": "1cec8f8dae8cde6e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T22:55:02.454258Z",
     "start_time": "2025-06-28T22:51:57.968334Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = EnsembleLoader(data_path, batch_size=128, camera = camera_type, validation_size= 10, chunk_size=6, persistent_workers=False)",
   "id": "479ca24ebbfbf799",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:07,  1.26it/s]\n",
      "6it [00:07,  1.21s/it]\n",
      "6it [00:04,  1.32it/s]\n",
      "6it [00:04,  1.22it/s]\n",
      "6it [00:04,  1.27it/s]\n",
      "6it [00:04,  1.37it/s]\n",
      "6it [00:05,  1.20it/s]\n",
      "6it [00:04,  1.34it/s]\n",
      "6it [00:05,  1.03it/s]\n",
      "6it [00:04,  1.41it/s]\n",
      "6it [00:04,  1.24it/s]\n",
      "6it [00:06,  1.00s/it]\n",
      "6it [00:05,  1.13it/s]\n",
      "6it [00:04,  1.22it/s]\n",
      "6it [00:05,  1.09it/s]\n",
      "6it [00:04,  1.29it/s]\n",
      "6it [00:05,  1.12it/s]\n",
      "6it [00:05,  1.08it/s]\n",
      "6it [00:04,  1.38it/s]\n",
      "6it [00:05,  1.03it/s]\n",
      "6it [00:06,  1.09s/it]\n",
      "6it [00:05,  1.12it/s]\n",
      "6it [00:05,  1.07it/s]\n",
      "6it [00:06,  1.01s/it]\n",
      "6it [00:05,  1.02it/s]\n",
      "6it [00:06,  1.02s/it]\n",
      "6it [00:05,  1.16it/s]\n",
      "6it [00:06,  1.03s/it]\n",
      "6it [00:05,  1.11it/s]\n",
      "6it [00:04,  1.24it/s]\n",
      "6it [00:04,  1.23it/s]\n",
      "6it [00:04,  1.31it/s]\n",
      "4it [00:03,  1.18it/s]\n",
      "6it [00:06,  1.12s/it]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "6a0e7af3f0cc470f",
   "metadata": {},
   "source": [
    "from robotics.model_src.diffusion_model import ConditionalUnet1D, ConditionalUnet1DTransformer\n",
    "from robotics.model_src.visual_encoder import CNNVisualEncoder\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# visual_encoder = CLIPVisualEncoder().to(device)\n",
    "\n",
    "visual_agentview_encoder = CNNVisualEncoder().to(device)\n",
    "\n",
    "visual_eye_in_hand_encoder = CNNVisualEncoder().to(device)\n",
    "\n",
    "vision_feature_dim = visual_agentview_encoder.get_output_shape() + visual_eye_in_hand_encoder.get_output_shape()\n",
    "\n",
    "action_observation_dim = 7\n",
    "\n",
    "obs_dim = vision_feature_dim + action_observation_dim\n",
    "\n",
    "action_dim = 7\n",
    "\n",
    "noise_prediction_net = ConditionalUnet1DTransformer(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim * obs_horizon,\n",
    ").to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dfcf411fb96bd906",
   "metadata": {},
   "source": [
    "# image = torch.Tensor(dataset.val_ds[0][\"img_obs\"][None, :obs_horizon, :, :, :]).to(device)\n",
    "#\n",
    "# image_agent = image[0, 0, 0]\n",
    "#\n",
    "# image_hand = image[0, 0, 1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "#\n",
    "# im = image_agent.cpu().numpy()\n",
    "#\n",
    "# plt.imshow(im.transpose((1, 2, 0)))\n",
    "#\n",
    "# plt.show()\n",
    "#\n",
    "#\n",
    "# im = image_hand.cpu().numpy()\n",
    "#\n",
    "# plt.imshow(im.transpose((1, 2, 0)))\n",
    "#\n",
    "# plt.show()"
   ],
   "id": "3d9458fbe0f64c81",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "404c3b9fc97655fd",
   "metadata": {},
   "source": [
    "# with torch.no_grad():\n",
    "#\n",
    "#     image_agent = image[:, : , 0]\n",
    "#     image_hand = image[:, : , 1]\n",
    "#\n",
    "#     image_features_agent = visual_agentview_encoder.encode(image_agent.flatten(start_dim=0, end_dim=1))\n",
    "#\n",
    "#     image_features_hand = visual_eye_in_hand_encoder.encode(image_hand.flatten(start_dim=0, end_dim=1))\n",
    "#\n",
    "#     image_features = torch.cat([image_features_agent, image_features_hand], dim=-1)\n",
    "#\n",
    "#     obs = image_features\n",
    "#\n",
    "#     noised_action = torch.randn((1, pred_horizon, action_dim)).to(device)\n",
    "#\n",
    "#     timestep_tensor = torch.randint(0, 101, (1,), device=device)\n",
    "#\n",
    "#     noise = noise_prediction_net(\n",
    "#         sample=noised_action,\n",
    "#         timestep=timestep_tensor,\n",
    "#         global_cond=obs.flatten(start_dim=1)\n",
    "#     )\n",
    "#\n",
    "#     denoised_action = noised_action - noise"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "accd1216068cb090",
   "metadata": {},
   "source": [
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "\n",
    "num_diffusion_iters = 100\n",
    "\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    clip_sample=True,\n",
    "    prediction_type='epsilon'\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f1cfb8707059725b",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def save_final_models(visual_encoder_agent, visual_encoder_hand, noise_pred_net, out_dir):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"visual_encoder_agent\": visual_encoder_agent.state_dict(),\n",
    "            \"visual_encoder_hand\": visual_encoder_hand.state_dict(),\n",
    "            \"noise_pred_net\": noise_pred_net.state_dict(),\n",
    "        },\n",
    "        out_dir / \"model_final.pth\",\n",
    "    )\n",
    "    print(f\"Saved to {out_dir / 'models.pth'}\")\n",
    "\n",
    "def load_final_models(visual_encoder, noise_pred_net, ckpt_path, device=\"cuda\"):\n",
    "    ckpt_path = Path(ckpt_path)\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    visual_encoder.load_state_dict(state[\"visual_encoder\"], strict=True)\n",
    "    noise_pred_net.load_state_dict(state[\"noise_pred_net\"], strict=True)\n",
    "\n",
    "    visual_encoder.to(device).eval()\n",
    "    noise_pred_net.to(device).eval()\n",
    "    print(f\"Loaded weights from {ckpt_path}\")\n",
    "\n",
    "def save_checkpoint(\n",
    "    epoch,\n",
    "    loss,\n",
    "    visual_encoder,\n",
    "    noise_pred_net,\n",
    "    ema,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    out_dir=\"checkpoints\",\n",
    "):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_name = f\"checkpoint_epoch{epoch:03d}_loss{loss:.4f}.pth\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": loss,\n",
    "            \"visual_encoder\": visual_encoder.state_dict(),\n",
    "            \"noise_pred_net\": noise_pred_net.state_dict(),\n",
    "            \"ema\": ema.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "        },\n",
    "        out_dir / ckpt_name,\n",
    "    )\n",
    "    print(f\"Checkpoint saved to {out_dir / ckpt_name}\")\n",
    "\n",
    "def load_checkpoint(\n",
    "    ckpt_path,\n",
    "    visual_encoder,\n",
    "    noise_pred_net,\n",
    "    ema,\n",
    "    optimizer=None,\n",
    "    scheduler=None,\n",
    "    map_location=\"cpu\",\n",
    "):\n",
    "    ckpt = torch.load(ckpt_path, map_location=map_location)\n",
    "    visual_encoder.load_state_dict(ckpt[\"visual_encoder\"])\n",
    "    noise_pred_net.load_state_dict(ckpt[\"noise_pred_net\"])\n",
    "    ema.load_state_dict(ckpt[\"ema\"])\n",
    "    if optimizer is not None and \"optimizer\" in ckpt:\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "    if scheduler is not None and \"scheduler\" in ckpt:\n",
    "        scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
    "    print(f\"Checkpoint loaded from {ckpt_path}\")\n",
    "    return ckpt[\"epoch\"], ckpt.get(\"loss\", None)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "from diffusers import EMAModel, get_scheduler\n",
    "\n",
    "def forward_loss(nbatch):\n",
    "    nobs  = nbatch['img_obs'][:, :obs_horizon].to(device)\n",
    "    a_obs = nbatch['act_obs'][:, :obs_horizon].to(device)\n",
    "    a_gt  = nbatch['act_pred'].to(device)\n",
    "    B = a_obs.size(0)\n",
    "\n",
    "    image_agent = nobs[:, : , 0]\n",
    "    image_hand = nobs[:, : , 1]\n",
    "\n",
    "    image_features_agent = visual_agentview_encoder.encode(image_agent.flatten(start_dim=0, end_dim=1)).reshape(*nobs.shape[:2], -1)\n",
    "\n",
    "    image_features_hand = visual_eye_in_hand_encoder.encode(image_hand.flatten(start_dim=0, end_dim=1)).reshape(*nobs.shape[:2], -1)\n",
    "\n",
    "    image_features = torch.cat([image_features_agent, image_features_hand, a_obs], dim=2).to(torch.float32)\n",
    "\n",
    "    obs = image_features\n",
    "\n",
    "    obs_cond = obs.flatten(start_dim=1)  # (B, H*obs_dim)\n",
    "\n",
    "    noise = torch.randn_like(a_gt)\n",
    "    timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps,\n",
    "                               (B,), device=device).long()\n",
    "    noisy_a = noise_scheduler.add_noise(a_gt, noise, timesteps)\n",
    "    noise_pred = noise_prediction_net(noisy_a, timesteps, global_cond=obs_cond)\n",
    "    return nn.functional.mse_loss(noise_pred, noise)"
   ],
   "id": "4376394fcf62c4fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_epochs = 3000\n",
    "\n",
    "# EMA params\n",
    "all_params = list(noise_prediction_net.parameters()) + list(visual_agentview_encoder.parameters()) + list(visual_eye_in_hand_encoder.parameters())\n",
    "ema = EMAModel(parameters=all_params, power=0.75)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=all_params,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-6\n",
    ")\n",
    "\n",
    "# LR scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=(6000) * num_epochs\n",
    ")\n"
   ],
   "id": "b96abee87c6d98ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# load_checkpoint(\"../model_src/checkpoints/checkpoint_epoch050_loss0.0398.pth\", visual_encoder, noise_prediction_net, ema, optimizer, lr_scheduler, map_location=\"cuda\")",
   "id": "c7c4a296a598a892",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rotate_every = 5\n",
    "\n",
    "from torch import nn\n",
    "from diffusers import EMAModel, get_scheduler\n",
    "\n",
    "\n",
    "train_hist, val_hist = [], []\n",
    "for epoch_idx in range(num_epochs):\n",
    "\n",
    "    loader = dataset.get_active_loader()     # use current loader\n",
    "\n",
    "    epoch_loss_sum = 0.0\n",
    "    noise_prediction_net.train()\n",
    "    for nbatch in loader:\n",
    "        nobs         = nbatch['img_obs'][:, :obs_horizon].to(device)\n",
    "        action_obs = nbatch['act_obs'][:, :obs_horizon].to(device)  # (B, H, 2)\n",
    "        action_pred  = nbatch['act_pred'].to(device).to(torch.float32)\n",
    "        B            = nobs.size(0)\n",
    "\n",
    "        image_agent = nobs[:, : , 0]\n",
    "        image_hand = nobs[:, : , 1]\n",
    "\n",
    "        image_features_agent = visual_agentview_encoder.encode(image_agent.flatten(start_dim=0, end_dim=1)).reshape(*nobs.shape[:2], -1)\n",
    "\n",
    "        image_features_hand = visual_eye_in_hand_encoder.encode(image_hand.flatten(start_dim=0, end_dim=1)).reshape(*nobs.shape[:2], -1)\n",
    "\n",
    "        image_features = torch.cat([image_features_agent, image_features_hand, action_obs], dim=2).to(torch.float32)\n",
    "\n",
    "        del image_features_agent, image_features_hand\n",
    "\n",
    "        obs_cond = image_features.flatten(1)\n",
    "\n",
    "        noise     = torch.randn_like(action_pred, dtype=torch.float32).to(device)\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.config.num_train_timesteps,\n",
    "            (B,), device=device).long()\n",
    "        noisy_act = noise_scheduler.add_noise(action_pred, noise, timesteps).to(torch.float32)\n",
    "        noise_pred = noise_prediction_net(noisy_act, timesteps, global_cond=obs_cond)\n",
    "\n",
    "        loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        ema.step(all_params)\n",
    "\n",
    "        epoch_loss_sum += loss.item()\n",
    "\n",
    "    avg_train = epoch_loss_sum / len(loader)\n",
    "    train_hist.append(avg_train)\n",
    "\n",
    "    noise_prediction_net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_sum = sum(forward_loss(b).item() for b in dataset.get_val_loader())\n",
    "    avg_val = val_sum / len(dataset.get_val_loader())\n",
    "    val_hist.append(avg_val)\n",
    "\n",
    "    print(f\"Epoch {epoch_idx+1:03d}/{num_epochs} | \"\n",
    "          f\"train {avg_train:.6f} | val {avg_val:.6f}\")\n",
    "\n",
    "    if (epoch_idx + 1) % 100 == 0:\n",
    "        save_checkpoint(\n",
    "            epoch         = epoch_idx + 1,\n",
    "            loss          = avg_val,\n",
    "            visual_encoder= visual_agentview_encoder,\n",
    "            noise_pred_net= noise_prediction_net,\n",
    "            ema           = ema,\n",
    "            optimizer     = optimizer,\n",
    "            scheduler     = lr_scheduler,\n",
    "            out_dir       = \"checkpoints\",\n",
    "        )\n",
    "\n",
    "    # rotate only after the epoch, when loader is finished\n",
    "    if (epoch_idx + 1) % rotate_every == 0:\n",
    "        del loader                          # drop reference\n",
    "        dataset.rotate(use_random=True)                    # switch to next dataset\n",
    "\n",
    "ema.copy_to(all_params)\n",
    "\n",
    "# ckpt_path = \"./checkpoints/checkpoint_epoch190_loss0.0316.pth\"\n",
    "#\n",
    "# ckpt = torch.load(ckpt_path, map_location=\"cuda\")\n",
    "# visual_encoder.load_state_dict(ckpt[\"visual_encoder\"])\n",
    "# noise_prediction_net.load_state_dict(ckpt[\"noise_pred_net\"])"
   ],
   "id": "21e5a38fb73e0156",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18f7349f71d96bd5",
   "metadata": {},
   "source": [
    "save_final_models(visual_agentview_encoder, visual_eye_in_hand_encoder, noise_prediction_net,\n",
    "                  \"../models/robot_v8_tool_hang_agent_img_only_two_cameras\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "89c55255cce7005",
   "metadata": {},
   "source": "load_final_models(visual_encoder, noise_prediction_net, \"../models/robot_v8_tool_hang_agent_224_e40/model_final.pth\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val_sum = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "        for batch in dataset.get_val_loader():\n",
    "            val_sum += forward_loss(batch).item()\n",
    "\n",
    "val_sum / len(dataset.get_val_loader())"
   ],
   "id": "d8b5982238c9a952",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "65c7ae7c4c252883",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:37:11.735582Z",
     "start_time": "2025-06-21T14:37:11.702483Z"
    }
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "id": "c7de96297d547660",
   "metadata": {},
   "source": [
    "import robomimic\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "import robomimic.utils.env_utils as EnvUtils\n",
    "import robomimic.utils.file_utils as FileUtils\n",
    "from robomimic.utils.vis_utils import depth_to_rgb\n",
    "from robomimic.envs.env_base import EnvBase, EnvType\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "env_meta = FileUtils.get_env_metadata_from_dataset(dataset_path=data_path)\n",
    "env_meta[\"env_kwargs\"][\"reward_shaping\"] = True\n",
    "env_meta[\"env_kwargs\"][\"reward_scale\"]   = 1.0\n",
    "\n",
    "dummy_spec = dict(\n",
    "    obs=dict(\n",
    "        low_dim=[\"robot0_eef_pos\"],\n",
    "        rgb=[\"agentview_image\"]\n",
    "        # rgb=[\"robot0_eye_in_hand_image\"]\n",
    "    ),\n",
    ")\n",
    "\n",
    "ObsUtils.initialize_obs_utils_with_obs_specs(obs_modality_specs=dummy_spec)\n",
    "\n",
    "env = EnvUtils.create_env_from_metadata(env_meta=env_meta, render=True, render_offscreen=True, use_image_obs=True)\n",
    "\n",
    "a = env.reset()\n",
    "\n",
    "from collections import deque\n",
    "obs_deque  = deque(maxlen=obs_horizon)\n",
    "act_deque = deque(maxlen=obs_horizon)\n",
    "rewards    = []\n",
    "imgs       = []\n",
    "step_idx   = 0\n",
    "\n",
    "max_steps = 500\n",
    "action_horizon  = 8\n",
    "\n",
    "# ─── 6. Main rollout ──────────────────────────────────────────────────────────\n",
    "obs = env.reset()\n",
    "# wrap obs in same format as env.step\n",
    "obs = obs if isinstance(obs, dict) else obs[0]\n",
    "for i in range(obs_deque.maxlen):\n",
    "    obs_deque.append(obs)\n",
    "    act_deque.append(torch.zeros((1,7)))\n",
    "\n",
    "pbar = tqdm(total=max_steps)\n",
    "done = False\n",
    "\n",
    "while not done and step_idx < max_steps:\n",
    "    # 6.1 build the image & action history tensor\n",
    "    img_np_agent = np.array([obs_deque[i][\"agentview_image\"] for i in range(obs_deque.maxlen)])\n",
    "\n",
    "    img_t_agent   = torch.from_numpy(img_np_agent).float().to(device)\n",
    "\n",
    "    img_np_hand = np.array([obs_deque[i][\"robot0_eye_in_hand_image\"] for i in range(obs_deque.maxlen)])\n",
    "\n",
    "    img_t_hand   = torch.from_numpy(img_np_hand).float().permute(0, 3, 1, 2).to(device) / 256\n",
    "\n",
    "    action_hist = act_deque.popleft().to(device)\n",
    "\n",
    "    # 6.2 compute visual features + conditioning\n",
    "    with torch.no_grad():\n",
    "        image_features_agent = visual_agentview_encoder.encode(img_t_agent)\n",
    "\n",
    "        image_features_hand = visual_eye_in_hand_encoder.encode(img_t_hand)\n",
    "\n",
    "        image_features = torch.cat([image_features_agent, image_features_hand, action_hist], dim=-1)\n",
    "\n",
    "        del image_features_agent, image_features_hand\n",
    "\n",
    "        obs_cond = image_features.flatten(1)\n",
    "\n",
    "        # 6.3 sample a future action sequence via diffusion\n",
    "        B = 1\n",
    "        pred_actions = torch.randn((B, pred_horizon, action_dim), device=device)\n",
    "        noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "        for t in noise_scheduler.timesteps:\n",
    "            noise_pred    = noise_prediction_net(pred_actions, t, global_cond=obs_cond)\n",
    "            out           = noise_scheduler.step(noise_pred, t, pred_actions)\n",
    "            pred_actions  = out.prev_sample\n",
    "\n",
    "    pred_actions = pred_actions.cpu().numpy()[0]        # (pred_horizon, 7)\n",
    "\n",
    "    # 6.4 execute the next block of actions\n",
    "    start = obs_horizon\n",
    "    end   = start + action_horizon\n",
    "    action_block = pred_actions[start:end]          # (5, 7)\n",
    "\n",
    "    for act in action_block:\n",
    "        obs, rew, done, info = env.step(act)\n",
    "        obs = obs if isinstance(obs, dict) else obs[0]\n",
    "\n",
    "        act_deque.append(torch.Tensor(act).unsqueeze(0))\n",
    "\n",
    "        frame = env.render(mode=\"rgb_array\", height=512, width=512)\n",
    "\n",
    "        obs_deque.append(obs)\n",
    "\n",
    "        rewards.append(rew)\n",
    "        imgs.append(frame)\n",
    "\n",
    "        step_idx += 1\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix(reward=float(rew))\n",
    "\n",
    "        if done or step_idx >= max_steps:\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# ─── 7. Wrap up ───────────────────────────────────────────────────────────────\n",
    "print(f\"Rollout finished: {step_idx} steps, total reward {sum(rewards):.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9fe8151cf6b7358b",
   "metadata": {},
   "source": [
    "import imageio\n",
    "\n",
    "video_path = \"test_two_cams.mp4\"\n",
    "fps = 24\n",
    "\n",
    "with imageio.get_writer(video_path, fps=fps, codec=\"libx264\") as writer:\n",
    "    for frame in imgs:\n",
    "        writer.append_data(frame)\n",
    "\n",
    "print(f\"Saved video to {video_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f62615db63e5ccd6",
   "metadata": {},
   "source": "img = img_np[0]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.imshow(img.transpose(1,2,0))",
   "id": "10812269deff5c5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5730143767b72223",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
