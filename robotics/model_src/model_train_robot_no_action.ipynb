{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0314445227a3baf",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "30e071ec36273394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T23:28:03.094301Z",
     "start_time": "2025-06-24T23:28:02.124758Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "c62ccd11679045ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T23:28:03.142357Z",
     "start_time": "2025-06-24T23:28:03.097701Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T23:28:03.215311Z",
     "start_time": "2025-06-24T23:28:03.188726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import h5py\n",
    "\n",
    "data_path = \"../robomimic/datasets/tool_hang/ph/image_agent.hdf5\""
   ],
   "id": "3dba3aed726029ba",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T23:28:03.309219Z",
     "start_time": "2025-06-24T23:28:03.230329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import gc\n",
    "import ctypes\n",
    "from torch.utils.data import DataLoader\n",
    "from robotics.model_src.dataset import RobosuiteImageActionDataset\n",
    "\n",
    "\n",
    "class EnsembleLoader:\n",
    "    \"\"\"\n",
    "    A helper that keeps several (Dataset, DataLoader) pairs and exposes a single\n",
    "    “active” pair. Call rotate() to switch the active pair and release memory\n",
    "    used by the previous one.\n",
    "\n",
    "        mgr = DemoDataManager(...)\n",
    "        ds  = mgr.get_ds()           # active dataset\n",
    "        ld  = mgr.get_loader()       # active dataloader\n",
    "        mgr.rotate()                 # switch to next pair\n",
    "        idx = mgr.get_active()       # index of the active pair\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_ds(data_path, camera, obs_h, pred_h, demo_subset):\n",
    "        return RobosuiteImageActionDataset(\n",
    "            data_path,\n",
    "            camera,\n",
    "            obs_horizon=obs_h,\n",
    "            pred_horizon=pred_h,\n",
    "            demos=demo_subset,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_loader(ds, batch_size, num_workers, shuffle, gen, persistent):\n",
    "        return DataLoader(\n",
    "            ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            generator=gen if shuffle else None,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=persistent,\n",
    "        )\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        chunk_size: int = 30,\n",
    "        validation_size: int = 20,\n",
    "        batch_size: int = 256,\n",
    "        obs_horizon: int = 1,\n",
    "        pred_horizon: int = 8,\n",
    "        camera: str = \"agentview\",\n",
    "        num_workers: int = 4,\n",
    "        persistent_workers: bool = False,\n",
    "        seed: int = 33,\n",
    "    ):\n",
    "        f = h5py.File(data_path, \"r\")\n",
    "\n",
    "        data = f[\"data\"]\n",
    "\n",
    "        demos = list(data.keys())\n",
    "\n",
    "        g = torch.Generator().manual_seed(seed)\n",
    "\n",
    "        # validation set\n",
    "        self.val_ds = self._build_ds(\n",
    "            data_path, camera, obs_horizon, pred_horizon, demos[-validation_size:]\n",
    "        )\n",
    "        self.val_loader = self._build_loader(\n",
    "            self.val_ds, batch_size, num_workers, False, g, True\n",
    "        )\n",
    "\n",
    "        # training sets divided into chunks\n",
    "        train_demos = demos[:-validation_size]\n",
    "        self.train_pairs = []\n",
    "        for i in range(0, len(train_demos), chunk_size):\n",
    "            subset = train_demos[i : i + chunk_size]\n",
    "            ds = self._build_ds(data_path, camera, obs_horizon, pred_horizon, subset)\n",
    "            ld = self._build_loader(ds, batch_size, num_workers, True, g, persistent_workers)\n",
    "            self.train_pairs.append((ds, ld))\n",
    "            ds.drop_data()  # keep RAM usage low\n",
    "\n",
    "        self.active = 0\n",
    "        self.train_pairs[0][0].load_data()\n",
    "\n",
    "    def rotate(self):\n",
    "        prev = self.active\n",
    "\n",
    "        # stop workers and drop current data\n",
    "        self._stop_workers(self.train_pairs[prev][1])\n",
    "        self.train_pairs[prev][0].drop_data()\n",
    "\n",
    "        # move to next index\n",
    "        self.active = (self.active + 1) % len(self.train_pairs)\n",
    "        print(f\"rotating from {prev} to {self.active} dataset\")\n",
    "\n",
    "        # load data for the new active dataset\n",
    "        self.train_pairs[self.active][0].load_data()\n",
    "\n",
    "        self._trim()\n",
    "\n",
    "    def get_ds(self, idx: int | None = None):\n",
    "        if idx is None:\n",
    "            idx = self.active\n",
    "        return self.train_pairs[idx][0]\n",
    "\n",
    "    def get_loader(self, idx: int | None = None):\n",
    "        if idx is None:\n",
    "            idx = self.active\n",
    "        return self.train_pairs[idx][1]\n",
    "\n",
    "    def get_val_loader(self, idx: int | None = None):\n",
    "        return self.val_loader\n",
    "\n",
    "    def get_active_loader(self) -> int:\n",
    "        return self.get_loader(self.active)\n",
    "\n",
    "    @staticmethod\n",
    "    def _stop_workers(loader):\n",
    "        it = getattr(loader, \"_iterator\", None)\n",
    "        if it is not None:\n",
    "            it._shutdown_workers()\n",
    "            loader._iterator = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _trim():\n",
    "        gc.collect()\n",
    "        try:\n",
    "            ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "        except OSError:\n",
    "            pass\n"
   ],
   "id": "1cec8f8dae8cde6e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-24T23:28:03.312505Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = EnsembleLoader(data_path, chunk_size=20, persistent_workers=False)",
   "id": "479ca24ebbfbf799",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:01, 10.14it/s]\n",
      "20it [00:03,  6.09it/s]\n",
      "20it [00:02,  7.17it/s]\n",
      "20it [00:02,  7.27it/s]\n",
      "16it [00:02,  4.43it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset.rotate()",
   "id": "8bd9f090a2bc0087",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from robotics.model_src.dataset import RobosuiteImageActionDataset, RobosuiteImageActionDatasetMem, normalize_data\n",
    "\n",
    "generator = torch.Generator().manual_seed(33)\n",
    "\n",
    "persistent_workers = False\n",
    "batch_size = 256\n",
    "\n",
    "camera_type = \"agentview\"\n",
    "\n",
    "pred_horizon = 8\n",
    "obs_horizon = 1\n",
    "#\n",
    "# ds_1 = RobosuiteImageActionDataset(data_path, camera_type, obs_horizon = obs_horizon, pred_horizon = pred_horizon, demos = demos_1)\n",
    "#\n",
    "# train_loader_1 = torch.utils.data.DataLoader(\n",
    "#     ds_1, batch_size=batch_size, shuffle=True,\n",
    "#     num_workers=4, pin_memory=True, persistent_workers=persistent_workers)\n",
    "#\n",
    "# ds_1.drop_data()\n",
    "# ds_2 = RobosuiteImageActionDataset(data_path, camera_type, obs_horizon = obs_horizon, pred_horizon = pred_horizon, demos = demos_2)\n",
    "#\n",
    "# train_loader_2 = torch.utils.data.DataLoader(\n",
    "#     ds_2, batch_size=batch_size, shuffle=True,\n",
    "#     num_workers=4, pin_memory=True, persistent_workers=persistent_workers)\n",
    "#\n",
    "#\n",
    "# ds_2.drop_data()\n",
    "# ds_3 = RobosuiteImageActionDataset(data_path, camera_type, obs_horizon = obs_horizon, pred_horizon = pred_horizon, demos = demos_3)\n",
    "#\n",
    "# train_loader_3 = torch.utils.data.DataLoader(\n",
    "#     ds_3, batch_size=batch_size, shuffle=True,\n",
    "#     num_workers=4, pin_memory=True, persistent_workers=persistent_workers)\n",
    "#\n",
    "# ds_3.drop_data()\n",
    "# ds_4 = RobosuiteImageActionDataset(data_path, camera_type, obs_horizon = obs_horizon, pred_horizon = pred_horizon, demos = demos_4)\n",
    "#\n",
    "# train_loader_4 = torch.utils.data.DataLoader(\n",
    "#     ds_4, batch_size=batch_size, shuffle=True,\n",
    "#     num_workers=4, pin_memory=True, persistent_workers=persistent_workers)\n",
    "#\n",
    "# ds_4.drop_data()\n",
    "# ds_5 = RobosuiteImageActionDataset(data_path, camera_type, obs_horizon = obs_horizon, pred_horizon = pred_horizon, demos = demos_5)\n",
    "#\n",
    "# train_loader_5 = torch.utils.data.DataLoader(\n",
    "#     ds_5, batch_size=batch_size, shuffle=True,\n",
    "#     num_workers=4, pin_memory=True, persistent_workers=persistent_workers)\n",
    "#\n",
    "# ds_5.drop_data()\n",
    "# ds_6 = RobosuiteImageActionDataset(data_path, camera_type, obs_horizon = obs_horizon, pred_horizon = pred_horizon, demos = demos_6)\n",
    "#\n",
    "# train_loader_6 = torch.utils.data.DataLoader(\n",
    "#     ds_6, batch_size=batch_size, shuffle=True,\n",
    "#     num_workers=4, pin_memory=True, persistent_workers=persistent_workers)\n",
    "#\n",
    "# ds_6.drop_data()\n",
    "#\n",
    "# train_ds = RobosuiteImageActionDataset(data_path, camera_type, obs_horizon = obs_horizon, pred_horizon = pred_horizon, demos = train_demos)\n",
    "#\n",
    "# val_loader = torch.utils.data.DataLoader(\n",
    "#     train_ds, batch_size=batch_size, shuffle=False,\n",
    "#     num_workers=4, pin_memory=True, persistent_workers=True)"
   ],
   "id": "c9de2c082f28b87a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # visualize data in batch\n",
    "# batch = next(iter(dataset.get_val_loader()))\n",
    "# print(\"batch['image'].shape:\", batch['img_obs'].shape)\n",
    "# print(\"batch['act_obs'].shape:\", batch['act_obs'].shape)\n",
    "# print(\"batch['act_pred'].shape\", batch['act_pred'].shape)"
   ],
   "id": "b4be688e4f10d7c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a0e7af3f0cc470f",
   "metadata": {},
   "source": [
    "from robotics.model_src.diffusion_model import ConditionalUnet1D, ConditionalUnet1DTransformer\n",
    "from robotics.model_src.visual_encoder import CNNVisualEncoder\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# visual_encoder = CLIPVisualEncoder().to(device)\n",
    "\n",
    "visual_encoder = CNNVisualEncoder().to(device)\n",
    "\n",
    "vision_feature_dim = visual_encoder.get_output_shape()\n",
    "\n",
    "action_observation_dim = 0\n",
    "\n",
    "obs_dim = vision_feature_dim + action_observation_dim\n",
    "\n",
    "action_dim = 7\n",
    "\n",
    "noise_prediction_net = ConditionalUnet1DTransformer(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim * obs_horizon,\n",
    ").to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dfcf411fb96bd906",
   "metadata": {},
   "source": [
    "# image = torch.Tensor(train_ds[0][\"img_obs\"][None, :obs_horizon, :, :, :]).to(device)\n",
    "# act_obs = torch.Tensor(train_ds[0][\"act_obs\"][None, :obs_horizon, :]).to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a28b9062ae5369e",
   "metadata": {},
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "#\n",
    "# im = image[0,0, :, :].cpu().numpy()\n",
    "#\n",
    "# plt.imshow(im.transpose((1, 2, 0)))\n",
    "#\n",
    "# plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "404c3b9fc97655fd",
   "metadata": {},
   "source": [
    "# with torch.no_grad():\n",
    "#     image_features = visual_encoder.encode(image.flatten(start_dim=0, end_dim=1))\n",
    "#\n",
    "#     image_features = image_features.reshape(*image.shape[:2], -1)\n",
    "#\n",
    "#     obs = image_features\n",
    "#\n",
    "#     noised_action = torch.randn((1, pred_horizon, action_dim)).to(device)\n",
    "#\n",
    "#     timestep_tensor = torch.randint(0, 101, (1,), device=device)\n",
    "#\n",
    "#     noise = noise_prediction_net(\n",
    "#         sample=noised_action,\n",
    "#         timestep=timestep_tensor,\n",
    "#         global_cond=obs.flatten(start_dim=1)\n",
    "#     )\n",
    "#\n",
    "#     denoised_action = noised_action - noise"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import ctypes\n",
    "# import gc\n",
    "#\n",
    "# datasets = [ds_1, ds_2, ds_3, ds_4, train_ds]\n",
    "# loaders  = [train_loader_1, train_loader_2, train_loader_3, train_loader_4, val_loader]\n",
    "#\n",
    "# def clear_dataset(idx: int) -> None:\n",
    "#     \"\"\"\n",
    "#     Полностью выгружает из RAM датасет и связанных с ним DataLoader-воркеров.\n",
    "#       idx = 0  → ds_1  / train_loader_1\n",
    "#       idx = 1  → ds_2  / train_loader_2\n",
    "#       ...\n",
    "#     \"\"\"\n",
    "#     ds = datasets[idx]\n",
    "#     ld = loaders[idx]\n",
    "#\n",
    "#     # ── 1. Останавливаем persistent-воркеров ───────────────────────────\n",
    "#     it = getattr(ld, \"_iterator\", None)\n",
    "#     if it is not None:\n",
    "#         it._shutdown_workers()\n",
    "#         ld._iterator = None           # позволяем PyTorch создать новый позже\n",
    "#\n",
    "#     # ── 2. Удаляем массив изображений у датасета ───────────────────────\n",
    "#     if getattr(ds, \"obs_data_transformed\", None) is not None:\n",
    "#         del ds.obs_data_transformed\n",
    "#         ds.obs_data_transformed = None\n",
    "#     ds.drop_data()                    # сброс дополнительных ссылок\n",
    "#\n",
    "#     # ── 3. Принудительный сборщик мусора + trim arenas glibc ───────────\n",
    "#     gc.collect()\n",
    "#\n",
    "#     ctypes.CDLL(\"libc.so.6\").malloc_trim(0)   # Linux: вернуть память ядру\n",
    "#\n",
    "# clear_dataset(4)"
   ],
   "id": "7002a74baad30d6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "accd1216068cb090",
   "metadata": {},
   "source": [
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "\n",
    "num_diffusion_iters = 100\n",
    "\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    clip_sample=True,\n",
    "    prediction_type='epsilon'\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f1cfb8707059725b",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def save_final_models(visual_encoder, noise_pred_net, out_dir):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"visual_encoder\": visual_encoder.state_dict(),\n",
    "            \"noise_pred_net\": noise_pred_net.state_dict(),\n",
    "        },\n",
    "        out_dir / \"model_final.pth\",\n",
    "    )\n",
    "    print(f\"Saved to {out_dir / 'models.pth'}\")\n",
    "\n",
    "def load_final_models(visual_encoder, noise_pred_net, ckpt_path, device=\"cuda\"):\n",
    "    ckpt_path = Path(ckpt_path)\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    visual_encoder.load_state_dict(state[\"visual_encoder\"], strict=True)\n",
    "    noise_pred_net.load_state_dict(state[\"noise_pred_net\"], strict=True)\n",
    "\n",
    "    visual_encoder.to(device).eval()\n",
    "    noise_pred_net.to(device).eval()\n",
    "    print(f\"Loaded weights from {ckpt_path}\")\n",
    "\n",
    "def save_checkpoint(\n",
    "    epoch,\n",
    "    loss,\n",
    "    visual_encoder,\n",
    "    noise_pred_net,\n",
    "    ema,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    out_dir=\"checkpoints\",\n",
    "):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_name = f\"checkpoint_epoch{epoch:03d}_loss{loss:.4f}.pth\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": loss,\n",
    "            \"visual_encoder\": visual_encoder.state_dict(),\n",
    "            \"noise_pred_net\": noise_pred_net.state_dict(),\n",
    "            \"ema\": ema.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "        },\n",
    "        out_dir / ckpt_name,\n",
    "    )\n",
    "    print(f\"Checkpoint saved to {out_dir / ckpt_name}\")\n",
    "\n",
    "def load_checkpoint(\n",
    "    ckpt_path,\n",
    "    visual_encoder,\n",
    "    noise_pred_net,\n",
    "    ema,\n",
    "    optimizer=None,\n",
    "    scheduler=None,\n",
    "    map_location=\"cpu\",\n",
    "):\n",
    "    ckpt = torch.load(ckpt_path, map_location=map_location)\n",
    "    visual_encoder.load_state_dict(ckpt[\"visual_encoder\"])\n",
    "    noise_pred_net.load_state_dict(ckpt[\"noise_pred_net\"])\n",
    "    ema.load_state_dict(ckpt[\"ema\"])\n",
    "    if optimizer is not None and \"optimizer\" in ckpt:\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "    if scheduler is not None and \"scheduler\" in ckpt:\n",
    "        scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
    "    print(f\"Checkpoint loaded from {ckpt_path}\")\n",
    "    return ckpt[\"epoch\"], ckpt.get(\"loss\", None)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "from diffusers import EMAModel, get_scheduler\n",
    "\n",
    "def forward_loss(nbatch):\n",
    "    nobs  = nbatch['img_obs'][:, :obs_horizon].to(device)\n",
    "    a_obs = nbatch['act_obs'][:, :obs_horizon].to(device)\n",
    "    a_gt  = nbatch['act_pred'].to(device)\n",
    "    B = a_obs.size(0)\n",
    "\n",
    "\n",
    "    image_features = visual_encoder.encode(nobs.flatten(start_dim=0, end_dim=1))\n",
    "\n",
    "    image_features = image_features.reshape(*nobs.shape[:2], -1)\n",
    "\n",
    "    obs = image_features\n",
    "\n",
    "    obs_cond = obs.flatten(start_dim=1)  # (B, H*obs_dim)\n",
    "\n",
    "    noise = torch.randn_like(a_gt)\n",
    "    timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps,\n",
    "                               (B,), device=device).long()\n",
    "    noisy_a = noise_scheduler.add_noise(a_gt, noise, timesteps)\n",
    "    noise_pred = noise_prediction_net(noisy_a, timesteps, global_cond=obs_cond)\n",
    "    return nn.functional.mse_loss(noise_pred, noise)#%%\n"
   ],
   "id": "4376394fcf62c4fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_epochs = 1200\n",
    "\n",
    "# EMA params\n",
    "all_params = list(noise_prediction_net.parameters())\n",
    "ema = EMAModel(parameters=all_params, power=0.75)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=all_params,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-6\n",
    ")\n",
    "\n",
    "# LR scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=(98000) * num_epochs\n",
    ")\n"
   ],
   "id": "b96abee87c6d98ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# load_checkpoint(\"../model_src/checkpoints/checkpoint_epoch050_loss0.0398.pth\", visual_encoder, noise_prediction_net, ema, optimizer, lr_scheduler, map_location=\"cuda\")",
   "id": "c7c4a296a598a892",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rotate_every = 4\n",
    "\n",
    "train_hist, val_hist = [], []\n",
    "for epoch_idx in range(num_epochs):\n",
    "\n",
    "    loader = dataset.get_active_loader()     # use current loader\n",
    "\n",
    "    epoch_loss_sum = 0.0\n",
    "    noise_prediction_net.train()\n",
    "    for nbatch in loader:\n",
    "        nobs         = nbatch['img_obs'][:, :obs_horizon].to(device)\n",
    "        action_obs   = nbatch['act_obs'][:, :obs_horizon].to(device)\n",
    "        action_pred  = nbatch['act_pred'].to(device)\n",
    "        B            = action_obs.size(0)\n",
    "\n",
    "        image_features = visual_encoder.encode(\n",
    "            nobs.flatten(0, 1)\n",
    "        ).reshape(*nobs.shape[:2], -1)\n",
    "        obs_cond = image_features.flatten(1)\n",
    "\n",
    "        noise     = torch.randn_like(action_pred)\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.config.num_train_timesteps,\n",
    "            (B,), device=device).long()\n",
    "        noisy_act = noise_scheduler.add_noise(action_pred, noise, timesteps)\n",
    "        noise_pred = noise_prediction_net(noisy_act, timesteps, global_cond=obs_cond)\n",
    "\n",
    "        loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        ema.step(all_params)\n",
    "\n",
    "        epoch_loss_sum += loss.item()\n",
    "\n",
    "    avg_train = epoch_loss_sum / len(loader)\n",
    "    train_hist.append(avg_train)\n",
    "\n",
    "    noise_prediction_net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_sum = sum(forward_loss(b).item() for b in dataset.get_val_loader())\n",
    "    avg_val = val_sum / len(dataset.get_val_loader())\n",
    "    val_hist.append(avg_val)\n",
    "\n",
    "    print(f\"Epoch {epoch_idx+1:03d}/{num_epochs} | \"\n",
    "          f\"train {avg_train:.6f} | val {avg_val:.6f}\")\n",
    "\n",
    "    if (epoch_idx + 1) % 50 == 0:\n",
    "        save_checkpoint(\n",
    "            epoch         = epoch_idx + 1,\n",
    "            loss          = avg_val,\n",
    "            visual_encoder= visual_encoder,\n",
    "            noise_pred_net= noise_prediction_net,\n",
    "            ema           = ema,\n",
    "            optimizer     = optimizer,\n",
    "            scheduler     = lr_scheduler,\n",
    "            out_dir       = \"checkpoints\",\n",
    "        )\n",
    "\n",
    "    # rotate only after the epoch, when loader is finished\n",
    "    if (epoch_idx + 1) % rotate_every == 0:\n",
    "        del loader                          # drop reference\n",
    "        dataset.rotate()                    # switch to next dataset\n",
    "\n",
    "ema.copy_to(all_params)\n"
   ],
   "id": "6ef21157758ee4e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ckpt_path = \"./checkpoints/checkpoint_epoch190_loss0.0316.pth\"\n",
    "#\n",
    "# ckpt = torch.load(ckpt_path, map_location=\"cuda\")\n",
    "# visual_encoder.load_state_dict(ckpt[\"visual_encoder\"])\n",
    "# noise_prediction_net.load_state_dict(ckpt[\"noise_pred_net\"])"
   ],
   "id": "21e5a38fb73e0156",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18f7349f71d96bd5",
   "metadata": {},
   "source": [
    "save_final_models(visual_encoder, noise_prediction_net,\n",
    "                  \"../models/robot_v9_tool_hang_agent_224_e131\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "89c55255cce7005",
   "metadata": {},
   "source": "load_final_models(visual_encoder, noise_prediction_net, \"../models/robot_v8_tool_hang_agent_224_e40/model_final.pth\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val_sum = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            val_sum += forward_loss(batch).item()\n",
    "\n",
    "val_sum / len(val_loader)"
   ],
   "id": "d8b5982238c9a952",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "65c7ae7c4c252883",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:37:11.735582Z",
     "start_time": "2025-06-21T14:37:11.702483Z"
    }
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "id": "c7de96297d547660",
   "metadata": {},
   "source": [
    "import robomimic\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "import robomimic.utils.env_utils as EnvUtils\n",
    "import robomimic.utils.file_utils as FileUtils\n",
    "from robomimic.utils.vis_utils import depth_to_rgb\n",
    "from robomimic.envs.env_base import EnvBase, EnvType\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "env_meta = FileUtils.get_env_metadata_from_dataset(dataset_path=data_path)\n",
    "env_meta[\"env_kwargs\"][\"reward_shaping\"] = True\n",
    "env_meta[\"env_kwargs\"][\"reward_scale\"]   = 1.0\n",
    "\n",
    "dummy_spec = dict(\n",
    "    obs=dict(\n",
    "        low_dim=[\"robot0_eef_pos\"],\n",
    "        rgb=[\"agentview_image\"]\n",
    "        # rgb=[\"robot0_eye_in_hand_image\"]\n",
    "    ),\n",
    ")\n",
    "\n",
    "ObsUtils.initialize_obs_utils_with_obs_specs(obs_modality_specs=dummy_spec)\n",
    "\n",
    "env = EnvUtils.create_env_from_metadata(env_meta=env_meta, render=True, render_offscreen=True, use_image_obs=True)\n",
    "\n",
    "a = env.reset()\n",
    "\n",
    "from collections import deque\n",
    "obs_deque  = deque(maxlen=obs_horizon)\n",
    "act_deque  = deque(maxlen=obs_horizon)\n",
    "rewards    = []\n",
    "imgs       = []\n",
    "step_idx   = 0\n",
    "\n",
    "max_steps = 500\n",
    "action_horizon  = 4\n",
    "\n",
    "# ─── 6. Main rollout ──────────────────────────────────────────────────────────\n",
    "obs = env.reset()\n",
    "# wrap obs in same format as env.step\n",
    "obs = obs if isinstance(obs, dict) else obs[0]\n",
    "for i in range(obs_deque.maxlen):\n",
    "    obs_deque.append(obs)\n",
    "    act_deque.append(np.zeros(action_dim, dtype=np.float32))\n",
    "\n",
    "pbar = tqdm(total=max_steps)\n",
    "done = False\n",
    "\n",
    "while not done and step_idx < max_steps:\n",
    "    # 6.1 build the image & action history tensor\n",
    "    img_np = np.array([obs_deque[i][camera_type + \"_image\"] for i in range(obs_deque.maxlen)])\n",
    "\n",
    "    img_t   = torch.from_numpy(img_np).float().to(device)\n",
    "\n",
    "    actions_hist = torch.stack(\n",
    "        [torch.from_numpy(a) for a in list(act_deque)],\n",
    "        dim=0\n",
    "    ).to(device)                           # (1, H_a, 7)\n",
    "\n",
    "    # 6.2 compute visual features + conditioning\n",
    "    with torch.no_grad():\n",
    "        img_feat = visual_encoder(img_t)                # (1, C)\n",
    "        obs_cond = torch.cat([img_feat.flatten(start_dim=0).unsqueeze(0) , actions_hist.flatten(start_dim=0).unsqueeze(0)], dim=1)\n",
    "\n",
    "        # 6.3 sample a future action sequence via diffusion\n",
    "        B = 1\n",
    "        pred_actions = torch.randn((B, pred_horizon, action_dim), device=device)\n",
    "        noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "        for t in noise_scheduler.timesteps:\n",
    "            noise_pred    = noise_prediction_net(pred_actions, t, global_cond=obs_cond)\n",
    "            out           = noise_scheduler.step(noise_pred, t, pred_actions)\n",
    "            pred_actions  = out.prev_sample\n",
    "\n",
    "    pred_actions = pred_actions.cpu().numpy()[0]        # (pred_horizon, 7)\n",
    "\n",
    "    # 6.4 execute the next block of actions\n",
    "    start = obs_horizon\n",
    "    end   = start + action_horizon\n",
    "    action_block = pred_actions[start:end]          # (5, 7)\n",
    "\n",
    "    for act in action_block:\n",
    "        obs, rew, done, info = env.step(act)\n",
    "        obs = obs if isinstance(obs, dict) else obs[0]\n",
    "\n",
    "        frame = env.render(mode=\"rgb_array\", height=512, width=512)\n",
    "\n",
    "        obs_deque.append(obs)\n",
    "        act_deque.append(act.astype(np.float32))\n",
    "\n",
    "        rewards.append(rew)\n",
    "        imgs.append(frame)\n",
    "\n",
    "        step_idx += 1\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix(reward=float(rew))\n",
    "\n",
    "        if done or step_idx >= max_steps:\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# ─── 7. Wrap up ───────────────────────────────────────────────────────────────\n",
    "print(f\"Rollout finished: {step_idx} steps, total reward {sum(rewards):.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9fe8151cf6b7358b",
   "metadata": {},
   "source": [
    "import imageio\n",
    "\n",
    "video_path = \"test_larger_size_img.mp4\"\n",
    "fps = 24\n",
    "\n",
    "with imageio.get_writer(video_path, fps=fps, codec=\"libx264\") as writer:\n",
    "    for frame in imgs:\n",
    "        writer.append_data(frame)\n",
    "\n",
    "print(f\"Saved video to {video_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f62615db63e5ccd6",
   "metadata": {},
   "source": "img = img_np[0]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.imshow(img.transpose(1,2,0))",
   "id": "10812269deff5c5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5730143767b72223",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
