{
 "cells": [
  {
   "cell_type": "code",
   "id": "d97722ef4fcf9857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:05:14.821902Z",
     "start_time": "2025-05-20T21:05:11.629785Z"
    }
   },
   "source": [
    "import torch\n",
    "from robotics.model_src.dataset import PushTDataset\n",
    "from robotics.model_src.diffusion_model import ConditionalUnet1D\n",
    "from robotics.model_src.visual_encoder import CLIPVisualEncoder"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/may33/miniconda3/envs/diffusion/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/may33/miniconda3/envs/diffusion/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:05:14.882751Z",
     "start_time": "2025-05-20T21:05:14.824377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "247be12746aaf133",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-20T21:05:17.742365Z",
     "start_time": "2025-05-20T21:05:14.935724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "pred_horizon = 16\n",
    "obs_horizon = 2\n",
    "action_horizon = 8\n",
    "\n",
    "# create dataset from file\n",
    "dataset = PushTDataset(\n",
    "    data_path=\"../data/demonstrations_snapshot_1.zarr\",\n",
    "    obs_horizon=obs_horizon,\n",
    "    prediction_horizon=pred_horizon\n",
    ")\n",
    "\n",
    "# create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    # accelerate cpu-gpu transfer\n",
    "    pin_memory=True,\n",
    "    # don't kill worker process after each epoch\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# visualize data in batch\n",
    "batch = next(iter(dataloader))\n",
    "print(\"batch['img_obs'].shape:\", batch['img_obs'].shape)\n",
    "print(\"batch['act_obs'].shape:\", batch['act_obs'].shape)\n",
    "print(\"batch['act_pred'].shape\", batch['act_pred'].shape)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 82119.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch['img_obs'].shape: torch.Size([64, 3, 224, 224, 3])\n",
      "batch['act_obs'].shape: torch.Size([64, 3, 2])\n",
      "batch['act_pred'].shape torch.Size([64, 16, 2])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:05:17.830037Z",
     "start_time": "2025-05-20T21:05:17.796006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image = torch.Tensor(dataset[0][\"img_obs\"][None, :, :, :, :]).to(device)\n",
    "act_obs = torch.Tensor(dataset[0][\"act_obs\"][None, :, :]).to(device)"
   ],
   "id": "4f7e8070c15d9926",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:05:22.244656Z",
     "start_time": "2025-05-20T21:05:19.941974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "visual_encoder = CLIPVisualEncoder().to(device)\n",
    "\n",
    "vision_feature_dim = visual_encoder.get_output_shape()\n",
    "\n",
    "action_observation_dim = 2\n",
    "\n",
    "obs_dim = vision_feature_dim + action_observation_dim\n",
    "\n",
    "action_dim = 2\n",
    "\n",
    "noise_prediction_net = ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim * (obs_horizon + 1),\n",
    ").to(device)"
   ],
   "id": "f7d7b4313d91eca7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 8.698829e+07\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T21:05:24.095281Z",
     "start_time": "2025-05-20T21:05:23.527406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    image_features = visual_encoder.encode(image.flatten(start_dim=0, end_dim=1))\n",
    "\n",
    "    image_features = image_features.reshape(*image.shape[:2], -1)\n",
    "\n",
    "    obs = torch.cat([image_features, act_obs], dim=-1)\n",
    "\n",
    "    noised_action = torch.randn((1, pred_horizon, action_dim)).to(device)\n",
    "\n",
    "    timestep_tensor = torch.randint(0, 101, (1,), device=device)\n",
    "\n",
    "    noise = noise_prediction_net(\n",
    "        sample=noised_action,\n",
    "        timestep=timestep_tensor,\n",
    "        global_cond=obs.flatten(start_dim=1)\n",
    "    )"
   ],
   "id": "d65b3c308b220a5b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 7 but got size 8 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m      8\u001B[39m noised_action = torch.randn((\u001B[32m1\u001B[39m, pred_horizon, action_dim)).to(device)\n\u001B[32m     10\u001B[39m timestep_tensor = torch.randint(\u001B[32m0\u001B[39m, \u001B[32m101\u001B[39m, (\u001B[32m1\u001B[39m,), device=device)\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m noise = \u001B[43mnoise_prediction_net\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[43msample\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnoised_action\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtimestep\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimestep_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43mglobal_cond\u001B[49m\u001B[43m=\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mflatten\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstart_dim\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/diffusion/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/diffusion/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/ml_portfolio/robotics/model_src/diffusion_model.py:248\u001B[39m, in \u001B[36mConditionalUnet1D.forward\u001B[39m\u001B[34m(self, sample, timestep, global_cond)\u001B[39m\n\u001B[32m    245\u001B[39m     x = mid_module(x, global_feature)\n\u001B[32m    247\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m idx, (resnet, resnet2, upsample) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m.up_modules):\n\u001B[32m--> \u001B[39m\u001B[32m248\u001B[39m     x = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    249\u001B[39m     x = resnet(x, global_feature)\n\u001B[32m    250\u001B[39m     x = resnet2(x, global_feature)\n",
      "\u001B[31mRuntimeError\u001B[39m: Sizes of tensors must match except in dimension 1. Expected size 7 but got size 8 for tensor number 1 in the list."
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "17fba7cfa53aacd3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
