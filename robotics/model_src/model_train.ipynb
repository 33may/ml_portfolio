{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "1d20424215217f30"
  },
  {
   "cell_type": "code",
   "id": "d97722ef4fcf9857",
   "metadata": {},
   "source": [
    "from robotics.gym_pusht.envs.pusht import PushTImageEnv\n",
    "from robotics.model_src.dataset import PushTDataset\n",
    "from robotics.model_src.diffusion_model import ConditionalUnet1D\n",
    "from robotics.model_src.visual_encoder import CLIPVisualEncoder, CNNVisualEncoder\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from tqdm.auto import tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "247be12746aaf133",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Define Env",
   "id": "225d10a557eac2a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@markdown ### **Env Demo**\n",
    "#@markdown Standard Gym Env (0.21.0 API)\n",
    "\n",
    "# 0. create env object\n",
    "env = PushTImageEnv()\n",
    "\n",
    "# 1. seed env for initial state.\n",
    "# Seed 0-200 are used for the demonstration dataset.\n",
    "env.seed(1000)\n",
    "\n",
    "# 2. must reset before use\n",
    "obs, info = env.reset()\n",
    "\n",
    "# 3. 2D positional action space [0,512]\n",
    "action = env.action_space.sample()\n",
    "\n",
    "# 4. Standard gym step method\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# prints and explains each dimension of the observation and action vectors\n",
    "with np.printoptions(precision=4, suppress=True, threshold=5):\n",
    "    print(\"obs['image'].shape:\", obs['image'].shape, \"float32, [0,1]\")\n",
    "    print(\"obs['agent_pos'].shape:\", obs['agent_pos'].shape, \"float32, [0,512]\")\n",
    "    print(\"action.shape: \", action.shape, \"float32, [0,512]\")"
   ],
   "id": "f939aa83f8917c32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load Data",
   "id": "e89c2fcdd11c0d23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import zarr\n",
    "\n",
    "dataset_version = 1\n",
    "\n",
    "ds = zarr.open(f\"../data/demonstrations_snapshot_3.zarr\", mode=\"r\")\n",
    "\n",
    "prev_actions = ds[\"data\"][\"action\"][:]  # shape (N, 2)\n",
    "prev_images = ds[\"data\"][\"img\"][:]  # shape (N, 96, 96, 3)\n",
    "prev_episode_ends = ds[\"meta\"][\"episode_ends\"][:]  # shape (M,)\n",
    "\n",
    "print(\"actions:\", prev_actions.shape, prev_actions.dtype)\n",
    "print(\"images:\", prev_images.shape, prev_images.dtype)\n",
    "print(\"episode_ends:\", prev_episode_ends.shape, prev_episode_ends.dtype)"
   ],
   "id": "f9011054bf81aa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pred_horizon = 16\n",
    "obs_horizon = 1\n",
    "action_horizon = 8\n",
    "\n",
    "dataset_path = \"/home/may33/Downloads/pusht/pusht_cchi_v7_replay.zarr\"\n",
    "\n",
    "# create dataset from file\n",
    "dataset = PushTDataset(\n",
    "    data_path=dataset_path,\n",
    "    prediction_horizon=pred_horizon,\n",
    "    obs_horizon=obs_horizon,\n",
    ")\n",
    "\n",
    "# create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    # accelerate cpu-gpu transfer\n",
    "    pin_memory=True,\n",
    "    # don't kill worker process afte each epoch\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# visualize data in batch\n",
    "batch = next(iter(dataloader))\n",
    "print(\"batch['image'].shape:\", batch['img_obs'].shape)\n",
    "print(\"batch['act_obs'].shape:\", batch['act_obs'].shape)\n",
    "print(\"batch['act_pred'].shape\", batch['act_pred'].shape)"
   ],
   "id": "5ac457eb4dcd57d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset space coverage",
   "id": "90b4da41f7100d1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "actions = dataset.actions_data_transformed\n",
    "\n",
    "x = actions[:, 0] * 512\n",
    "y = actions[:, 1] * 512\n",
    "\n",
    "plt.scatter(x, y, s=0.2, c=\"purple\")\n",
    "\n",
    "plt.savefig(\"pushT_state_coverage.png\", dpi=300, bbox_inches=\"tight\")"
   ],
   "id": "c2282cbf1eace615",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image = torch.Tensor(dataset[0][\"img_obs\"][None, :obs_horizon, :, :, :]).to(device)\n",
    "act_obs = torch.Tensor(dataset[0][\"act_obs\"][None, :obs_horizon, :]).to(device)"
   ],
   "id": "4f7e8070c15d9926",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Define Models",
   "id": "d201297ac92de3ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# visual_encoder = CLIPVisualEncoder().to(device)\n",
    "\n",
    "visual_encoder = CNNVisualEncoder().to(device)\n",
    "\n",
    "vision_feature_dim = visual_encoder.get_output_shape()\n",
    "\n",
    "action_observation_dim = 2\n",
    "\n",
    "obs_dim = vision_feature_dim + action_observation_dim\n",
    "\n",
    "action_dim = 2\n",
    "\n",
    "noise_prediction_net = ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim * obs_horizon,\n",
    ").to(device)"
   ],
   "id": "f7d7b4313d91eca7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    image_features = visual_encoder.encode(image.flatten(start_dim=0, end_dim=1))\n",
    "\n",
    "    image_features = image_features.reshape(*image.shape[:2], -1)\n",
    "\n",
    "    obs = torch.cat([image_features, act_obs], dim=-1)\n",
    "\n",
    "    noised_action = torch.randn((1, pred_horizon, action_dim)).to(device)\n",
    "\n",
    "    timestep_tensor = torch.randint(0, 101, (1,), device=device)\n",
    "\n",
    "    noise = noise_prediction_net(\n",
    "        sample=noised_action,\n",
    "        timestep=timestep_tensor,\n",
    "        global_cond=obs.flatten(start_dim=1)\n",
    "    )\n",
    "\n",
    "    denoised_action = noised_action - noise\n"
   ],
   "id": "d65b3c308b220a5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "\n",
    "num_diffusion_iters = 100\n",
    "\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    clip_sample=True,\n",
    "    prediction_type='epsilon'\n",
    ")"
   ],
   "id": "dfe020289f9319e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train",
   "id": "63651d0fa71901bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_epochs = 100\n",
    "\n",
    "# EMA params\n",
    "all_params = list(visual_encoder.parameters()) + list(noise_prediction_net.parameters())\n",
    "ema = EMAModel(parameters=all_params, power=0.75)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=all_params,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-6\n",
    ")\n",
    "\n",
    "# LR scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(dataloader) * num_epochs\n",
    ")\n",
    "\n",
    "# train loop\n",
    "for epoch_idx in range(num_epochs):\n",
    "    epoch_loss_sum = 0.0\n",
    "\n",
    "    for nbatch in dataloader:\n",
    "        # prepare data\n",
    "        nimage = nbatch['img_obs'][:, :obs_horizon].to(device)  # (B, H, 3,96,96)\n",
    "        action_obs = nbatch['act_obs'][:, :obs_horizon].to(device)  # (B, H, 2)\n",
    "        action_pred = nbatch['act_pred'].to(device)  # (B, P, 2)\n",
    "        B = action_obs.size(0)\n",
    "\n",
    "        # forward pass\n",
    "        image_features = visual_encoder.forward(\n",
    "            nimage.flatten(end_dim=1)  # (B*H,3,96,96)\n",
    "        ).reshape(*nimage.shape[:2], -1)  # (B, H, D=512)\n",
    "\n",
    "        obs_features = torch.cat([image_features, action_obs], dim=-1)\n",
    "        obs_cond = obs_features.flatten(start_dim=1)  # (B, H*obs_dim)\n",
    "\n",
    "        noise = torch.randn_like(action_pred)\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.config.num_train_timesteps,\n",
    "            (B,), device=device).long()\n",
    "\n",
    "        noisy_actions = noise_scheduler.add_noise(action_pred, noise, timesteps)\n",
    "        noise_pred = noise_prediction_net(noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "        loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step()\n",
    "        ema.step(all_params)\n",
    "\n",
    "        epoch_loss_sum += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss_sum / len(dataloader)\n",
    "    print(f\"Epoch {epoch_idx + 1:3d}/{num_epochs} ─ average loss: {avg_loss:.6f}\")\n",
    "\n",
    "# copy EMA weights for inference\n",
    "ema.copy_to(all_params)"
   ],
   "id": "1d226ec8de977aaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Save / Load model.021677\n",
    "Epoch  19/100 ─ average loss: 0.021065\n",
    "Epoch  20/100 ─ average loss: 0.019704\n",
    "Epoch  21/100 ─ average loss: 0.020990\n",
    "Epoch  22/100 ─ average loss: 0.019516\n",
    "Epoch  23/100 ─ average loss: 0.019944\n",
    "Epoch  24/100 ─ average loss: 0.019400"
   ],
   "id": "ba97d1307de5a5e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def save_final_models(visual_encoder, noise_pred_net, out_dir):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"visual_encoder\": visual_encoder.state_dict(),\n",
    "            \"noise_pred_net\": noise_pred_net.state_dict(),\n",
    "        },\n",
    "        out_dir / \"model_final.pth\",\n",
    "    )\n",
    "    print(f\"Saved to {out_dir / 'models.pth'}\")"
   ],
   "id": "6a8deddb4840356",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_final_models(visual_encoder, noise_prediction_net,\n",
    "                  \"../models/v6_success_both_cnn_actions_h1\")"
   ],
   "id": "c5afb4eefc4e65dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_final_models(visual_encoder, noise_pred_net, ckpt_path, device=\"cuda\"):\n",
    "    ckpt_path = Path(ckpt_path)\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    visual_encoder.load_state_dict(state[\"visual_encoder\"], strict=True)\n",
    "    noise_pred_net.load_state_dict(state[\"noise_pred_net\"], strict=True)\n",
    "\n",
    "    visual_encoder.to(device).eval()\n",
    "    noise_pred_net.to(device).eval()\n",
    "    print(f\"Loaded weights from {ckpt_path}\")\n"
   ],
   "id": "f4bd716c01548aff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "load_final_models(visual_encoder, noise_prediction_net, \"../models/v6_success_both_cnn_actions_h1/model_final.pth\")",
   "id": "ac8bfe9d9e38317a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "dfcee4560809157a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from robotics.model_src.dataset import normalize_data\n",
    "\n",
    "# ─────────────── Config ───────────────\n",
    "n_val = 20\n",
    "max_steps = 200\n",
    "\n",
    "pred_horizon = 16\n",
    "action_dim = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# List of action_horizons to evaluate\n",
    "action_horizons = [1, 2, 4, 8, 16]\n",
    "# ─────────────── Evaluation ───────────────\n",
    "results = []\n",
    "\n",
    "for action_horizon in action_horizons:\n",
    "    print(f\"\\nEvaluating for action_horizon = {action_horizon}...\")\n",
    "    episodes = []\n",
    "    episode_times = []\n",
    "\n",
    "    for episode_i in range(n_val):\n",
    "        env = PushTImageEnv(render_size_vis=512)\n",
    "        env.seed(100000 + episode_i)\n",
    "        obs, info = env.reset()\n",
    "\n",
    "        obs_deque = collections.deque([obs] * obs_horizon, maxlen=obs_horizon)\n",
    "        init_action = obs['agent_pos'].copy()\n",
    "        act_deque = collections.deque([init_action] * obs_horizon, maxlen=obs_horizon)\n",
    "\n",
    "        done = False\n",
    "        rewards = []\n",
    "        step_idx = 0\n",
    "\n",
    "        episode_start_time = time.perf_counter()\n",
    "\n",
    "        with tqdm(total=max_steps, desc=f\"Episode {episode_i + 1}/{n_val} | Horizon {action_horizon}\") as pbar:\n",
    "            while not done:\n",
    "                # Stack input\n",
    "                images_hist = np.stack([x['image'] for x in obs_deque])\n",
    "                actions_hist = np.stack(act_deque)\n",
    "                actions_hist = normalize_data(actions_hist, scale=512)\n",
    "\n",
    "                images_hist = torch.from_numpy(images_hist).to(device, dtype=torch.float32)\n",
    "                actions_hist = torch.from_numpy(actions_hist).to(device, dtype=torch.float32)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    image_features = visual_encoder(images_hist)\n",
    "                    obs_features = torch.cat([image_features, actions_hist], dim=-1)\n",
    "                    obs_cond = obs_features.unsqueeze(0).flatten(start_dim=1)\n",
    "\n",
    "                    pred_actions = torch.randn((1, pred_horizon, action_dim), device=device)\n",
    "                    noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "                    for k in noise_scheduler.timesteps:\n",
    "                        noise_pred = noise_prediction_net(\n",
    "                            sample=pred_actions,\n",
    "                            timestep=k,\n",
    "                            global_cond=obs_cond\n",
    "                        )\n",
    "                        pred_actions = noise_scheduler.step(\n",
    "                            model_output=noise_pred,\n",
    "                            timestep=k,\n",
    "                            sample=pred_actions\n",
    "                        ).prev_sample\n",
    "\n",
    "                    pred_actions = pred_actions.cpu().numpy()[0]\n",
    "                    action_exec = normalize_data(pred_actions, scale=1 / 512)\n",
    "\n",
    "                # Execute action block\n",
    "                start = obs_horizon - 1\n",
    "                end = start + action_horizon\n",
    "                action_block = action_exec[start:end]\n",
    "\n",
    "                for act in action_block:\n",
    "                    obs, reward, done, _, info = env.step(act)\n",
    "                    obs_deque.append(obs)\n",
    "                    act_deque.append(act)\n",
    "\n",
    "                    rewards.append(reward)\n",
    "                    step_idx += 1\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_postfix(reward=float(reward))\n",
    "\n",
    "                    if step_idx >= max_steps:\n",
    "                        done = True\n",
    "                        break\n",
    "\n",
    "        episode_duration = time.perf_counter() - episode_start_time\n",
    "        print(f\"Score: {max(rewards)} | Episode Time: {episode_duration:.2f} s\")\n",
    "\n",
    "        episodes.append(max(rewards))\n",
    "        episode_times.append(episode_duration)\n",
    "\n",
    "    results.append({\n",
    "        \"horizon\": action_horizon,\n",
    "        \"avg_score\": np.mean(episodes),\n",
    "        \"std_score\": np.std(episodes),\n",
    "        \"avg_episode_time\": np.mean(episode_times),\n",
    "        \"std_episode_time\": np.std(episode_times),\n",
    "    })\n",
    "\n",
    "# ─────────────── Summary ───────────────\n",
    "print(\"\\n=== Evaluation Summary ===\")\n",
    "print(f\"{'Horizon':>8} | {'Score μ':>8} | {'Score σ':>8} | {'Time μ (s)':>10} | {'Time σ':>10}\")\n",
    "print(\"-\" * 58)\n",
    "for res in results:\n",
    "    print(\n",
    "        f\"{res['horizon']:>8} | {res['avg_score']:>8.2f} | {res['std_score']:>8.2f} | {res['avg_episode_time']:>10.4f} | {res['std_episode_time']:>10.4f}\")\n"
   ],
   "id": "4a7e58b979e68d36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Horizon categories\n",
    "horizons = [1, 2, 4, 8, 16]\n",
    "x_pos = list(range(len(horizons)))\n",
    "\n",
    "# Score data\n",
    "score_mu = [0.78, 0.82, 0.77, 0.79, 0.81]\n",
    "score_sigma = [0.29, 0.29, 0.28, 0.27, 0.25]\n",
    "\n",
    "# Time data\n",
    "time_mu = [34.8187, 21.4516, 11.3340, 5.5704, 3.3637]\n",
    "\n",
    "box_width = 0.35\n",
    "ymin_clip, ymax_clip = 0.4, 1.0\n",
    "top_line_y = 0.995  # new top line position\n",
    "\n",
    "fig, ax_score = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "# Draw rectangles, thicker mean and top lines\n",
    "for idx, (mu, sigma) in enumerate(zip(score_mu, score_sigma)):\n",
    "    rect = Rectangle((idx - box_width / 2, mu - sigma),\n",
    "                     box_width, 2 * sigma,\n",
    "                     fill=False, linewidth=1, edgecolor='black')\n",
    "    ax_score.add_patch(rect)\n",
    "\n",
    "    # Thicker mean line\n",
    "    ax_score.hlines(mu, idx - box_width / 2, idx + box_width / 2,\n",
    "                    linewidth=1, color='black')\n",
    "\n",
    "    # Thicker top line at y = 0.99\n",
    "    ax_score.hlines(top_line_y, idx - box_width / 2, idx + box_width / 2,\n",
    "                    linewidth=1, color='black')\n",
    "\n",
    "# Configure axes\n",
    "ax_score.set_ylabel(\"Score\", fontsize=16)\n",
    "ax_score.set_ylim(ymin_clip, ymax_clip)\n",
    "ax_score.set_xticks(x_pos)\n",
    "ax_score.set_xticklabels([str(h) for h in horizons])\n",
    "ax_score.set_xlabel(\"Horizon (categorical)\", fontsize=16)\n",
    "\n",
    "# Time axis\n",
    "ax_time = ax_score.twinx()\n",
    "ax_time.plot(x_pos, time_mu, linestyle='--', marker='o')\n",
    "ax_time.set_ylabel(\"Time (s)\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"pusht_eval.png\", dpi=300)\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "a4b76217f045c835",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from robotics.model_src.dataset import normalize_data\n",
    "\n",
    "# ─────────────── Config ───────────────\n",
    "max_steps = 500\n",
    "\n",
    "pred_horizon = 16\n",
    "action_dim = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ─────────────── Evaluation ───────────────\n",
    "results = []\n",
    "\n",
    "episodes = []\n",
    "episode_times = []\n",
    "imgs = []\n",
    "\n",
    "env = PushTImageEnv(render_size_vis=512)\n",
    "# env.seed(100000 + episode_i)\n",
    "obs, info = env.reset()\n",
    "\n",
    "obs_deque = collections.deque([obs] * obs_horizon, maxlen=obs_horizon)\n",
    "init_action = obs['agent_pos'].copy()\n",
    "act_deque = collections.deque([init_action] * obs_horizon, maxlen=obs_horizon)\n",
    "\n",
    "done = False\n",
    "rewards = []\n",
    "step_idx = 0\n",
    "\n",
    "episode_start_time = time.perf_counter()\n",
    "\n",
    "with tqdm(total=max_steps) as pbar:\n",
    "    while not done:\n",
    "        # Stack input\n",
    "        images_hist = np.stack([x['image'] for x in obs_deque])\n",
    "        actions_hist = np.stack(act_deque)\n",
    "        actions_hist = normalize_data(actions_hist, scale=512)\n",
    "\n",
    "        images_hist = torch.from_numpy(images_hist).to(device, dtype=torch.float32)\n",
    "        actions_hist = torch.from_numpy(actions_hist).to(device, dtype=torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = visual_encoder(images_hist)\n",
    "            obs_features = torch.cat([image_features, actions_hist], dim=-1)\n",
    "            obs_cond = obs_features.unsqueeze(0).flatten(start_dim=1)\n",
    "\n",
    "            pred_actions = torch.randn((1, pred_horizon, action_dim), device=device)\n",
    "            noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "            for k in noise_scheduler.timesteps:\n",
    "                noise_pred = noise_prediction_net(\n",
    "                    sample=pred_actions,\n",
    "                    timestep=k,\n",
    "                    global_cond=obs_cond\n",
    "                )\n",
    "                pred_actions = noise_scheduler.step(\n",
    "                    model_output=noise_pred,\n",
    "                    timestep=k,\n",
    "                    sample=pred_actions\n",
    "                ).prev_sample\n",
    "\n",
    "            pred_actions = pred_actions.cpu().numpy()[0]\n",
    "            action_exec = normalize_data(pred_actions, scale=1 / 512)\n",
    "\n",
    "        # Execute action block\n",
    "        start = obs_horizon - 1\n",
    "        end = start + action_horizon\n",
    "        action_block = action_exec[start:end]\n",
    "\n",
    "        for act in action_block:\n",
    "            obs, reward, done, _, info = env.step(act)\n",
    "            obs_deque.append(obs)\n",
    "            act_deque.append(act)\n",
    "\n",
    "            imgs.append(env.render(mode='rgb_array'))\n",
    "\n",
    "            rewards.append(reward)\n",
    "            step_idx += 1\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(reward=float(reward))\n",
    "\n",
    "            if step_idx >= max_steps:\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "episode_duration = time.perf_counter() - episode_start_time\n",
    "print(f\"Score: {max(rewards)} | Episode Time: {episode_duration:.2f} s\")\n",
    "\n",
    "episodes.append(max(rewards))\n",
    "episode_times.append(episode_duration)\n"
   ],
   "id": "38cb4d428b18f442",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import Video\n",
    "from skvideo.io import vwrite\n",
    "\n",
    "vwrite(\"../results/vis_5.mp4\", imgs)\n"
   ],
   "id": "eaea3a7c46df447",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e9bfcae714ae7d55",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
