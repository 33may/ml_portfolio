{
 "cells": [
  {
   "cell_type": "code",
   "id": "d97722ef4fcf9857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T10:58:30.645182Z",
     "start_time": "2025-05-21T10:58:28.584585Z"
    }
   },
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from robotics.model_src.dataset import PushTDataset\n",
    "from robotics.model_src.diffusion_model import ConditionalUnet1D\n",
    "from robotics.model_src.visual_encoder import CLIPVisualEncoder"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/may33/miniconda3/envs/diffusion/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/may33/miniconda3/envs/diffusion/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T10:58:30.676484Z",
     "start_time": "2025-05-21T10:58:30.647218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "247be12746aaf133",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T10:58:37.099059Z",
     "start_time": "2025-05-21T10:58:30.716736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "pred_horizon = 16\n",
    "obs_horizon = 2\n",
    "action_horizon = 8\n",
    "\n",
    "# create dataset from file\n",
    "dataset = PushTDataset(\n",
    "    data_path=\"../data/demonstrations_snapshot_1.zarr\",\n",
    "    obs_horizon=obs_horizon,\n",
    "    prediction_horizon=pred_horizon\n",
    ")"
   ],
   "id": "5ac457eb4dcd57d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 87415.69it/s]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-21T10:58:37.277797Z",
     "start_time": "2025-05-21T10:58:37.105447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    # num_workers=4,\n",
    "    shuffle=True,\n",
    "    # accelerate cpu-gpu transfer\n",
    "    pin_memory=True,\n",
    "    # don't kill worker process after each epoch\n",
    "    # persistent_workers=True\n",
    ")\n",
    "\n",
    "# visualize data in batch\n",
    "batch = next(iter(dataloader))\n",
    "print(\"batch['img_obs'].shape:\", batch['img_obs'].shape)\n",
    "print(\"batch['act_obs'].shape:\", batch['act_obs'].shape)\n",
    "print(\"batch['act_pred'].shape\", batch['act_pred'].shape)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch['img_obs'].shape: torch.Size([64, 3, 224, 224, 3])\n",
      "batch['act_obs'].shape: torch.Size([64, 3, 2])\n",
      "batch['act_pred'].shape torch.Size([64, 16, 2])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T10:58:37.318815Z",
     "start_time": "2025-05-21T10:58:37.291304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image = torch.Tensor(dataset[0][\"img_obs\"][None, :, :, :, :]).to(device)\n",
    "act_obs = torch.Tensor(dataset[0][\"act_obs\"][None, :, :]).to(device)"
   ],
   "id": "4f7e8070c15d9926",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T10:58:39.617781Z",
     "start_time": "2025-05-21T10:58:37.392510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "visual_encoder = CLIPVisualEncoder().to(device)\n",
    "\n",
    "vision_feature_dim = visual_encoder.get_output_shape()\n",
    "\n",
    "action_observation_dim = 2\n",
    "\n",
    "obs_dim = vision_feature_dim + action_observation_dim\n",
    "\n",
    "action_dim = 2\n",
    "\n",
    "noise_prediction_net = ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim * (obs_horizon + 1),\n",
    ").to(device)"
   ],
   "id": "f7d7b4313d91eca7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 8.731597e+07\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T11:04:52.995579Z",
     "start_time": "2025-05-21T11:04:43.046109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    image_features = visual_encoder.encode(image.flatten(start_dim=0, end_dim=1))\n",
    "\n",
    "    image_features = image_features.reshape(*image.shape[:2], -1)\n",
    "\n",
    "    obs = torch.cat([image_features, act_obs], dim=-1)\n",
    "\n",
    "    noised_action = torch.randn((1, pred_horizon, action_dim)).to(device)\n",
    "\n",
    "    timestep_tensor = torch.randint(0, 101, (1,), device=device)\n",
    "\n",
    "    noise = noise_prediction_net(\n",
    "        sample=noised_action,\n",
    "        timestep=timestep_tensor,\n",
    "        global_cond=obs.flatten(start_dim=1)\n",
    "    )\n",
    "\n",
    "    denoised_action = noised_action - noise\n"
   ],
   "id": "d65b3c308b220a5b",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m      8\u001B[39m noised_action = torch.randn((\u001B[32m1\u001B[39m, pred_horizon, action_dim)).to(device)\n\u001B[32m     10\u001B[39m timestep_tensor = torch.randint(\u001B[32m0\u001B[39m, \u001B[32m101\u001B[39m, (\u001B[32m1\u001B[39m,), device=device)\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m noise = \u001B[43mnoise_prediction_net\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[43msample\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnoised_action\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtimestep\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimestep_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43mglobal_cond\u001B[49m\u001B[43m=\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mflatten\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstart_dim\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     18\u001B[39m denoised_action = noised_action - noise\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/diffusion/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/diffusion/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/ml_portfolio/robotics/model_src/diffusion_model.py:236\u001B[39m, in \u001B[36mConditionalUnet1D.forward\u001B[39m\u001B[34m(self, sample, timestep, global_cond)\u001B[39m\n\u001B[32m    232\u001B[39m global_feature = \u001B[38;5;28mself\u001B[39m.diffusion_step_encoder(timesteps)\n\u001B[32m    234\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m global_cond \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    235\u001B[39m     global_feature = torch.cat([\n\u001B[32m--> \u001B[39m\u001B[32m236\u001B[39m         \u001B[43mglobal_feature\u001B[49m, global_cond\n\u001B[32m    237\u001B[39m     ], axis=-\u001B[32m1\u001B[39m)\n\u001B[32m    239\u001B[39m x = sample\n\u001B[32m    240\u001B[39m h = []\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_frame.py:892\u001B[39m, in \u001B[36mPyDBFrame.trace_dispatch\u001B[39m\u001B[34m(self, frame, event, arg)\u001B[39m\n\u001B[32m    890\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_line:\n\u001B[32m    891\u001B[39m     \u001B[38;5;28mself\u001B[39m.set_suspend(thread, step_cmd)\n\u001B[32m--> \u001B[39m\u001B[32m892\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    893\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# return event\u001B[39;00m\n\u001B[32m    894\u001B[39m     back = frame.f_back\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_frame.py:412\u001B[39m, in \u001B[36mPyDBFrame.do_wait_suspend\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    411\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdo_wait_suspend\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m412\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_args\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python-ce/helpers/pydev/pydevd.py:1220\u001B[39m, in \u001B[36mPyDB.do_wait_suspend\u001B[39m\u001B[34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[39m\n\u001B[32m   1217\u001B[39m         from_this_thread.append(frame_id)\n\u001B[32m   1219\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, stop_reason):\n\u001B[32m-> \u001B[39m\u001B[32m1220\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python-ce/helpers/pydev/pydevd.py:1235\u001B[39m, in \u001B[36mPyDB._do_wait_suspend\u001B[39m\u001B[34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[39m\n\u001B[32m   1232\u001B[39m             \u001B[38;5;28mself\u001B[39m._call_mpl_hook()\n\u001B[32m   1234\u001B[39m         \u001B[38;5;28mself\u001B[39m.process_internal_commands()\n\u001B[32m-> \u001B[39m\u001B[32m1235\u001B[39m         \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1237\u001B[39m \u001B[38;5;28mself\u001B[39m.cancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[32m   1239\u001B[39m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T10:58:44.514834Z",
     "start_time": "2025-05-21T10:58:44.480618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from robotics.model_src.diffusion_model import Config, CosineDDPMScheduler\n",
    "# from diffusers import DDPMScheduler\n",
    "#\n",
    "# noise_scheduler = DDPMScheduler(\n",
    "#     num_train_timesteps = 100,              # T\n",
    "#     beta_schedule      = \"cosine\",\n",
    "#     beta_start         = 1e-4,\n",
    "#     beta_end           = 2e-2,\n",
    "#     prediction_type    = \"epsilon\"\n",
    "# )\n",
    "\n",
    "# ddpm_cosine_scheduler.py\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "\n",
    "cfg = Config(T=100, device=\"cuda\")\n",
    "noise_scheduler = CosineDDPMScheduler(cfg)"
   ],
   "id": "17fba7cfa53aacd3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T10:58:46.119130Z",
     "start_time": "2025-05-21T10:58:46.103872Z"
    }
   },
   "cell_type": "code",
   "source": "noise_prediction_net.parameters()",
   "id": "e3fc5682fa5ee6ad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7fa6ca8d8900>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-21T11:17:45.263566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from time import perf_counter\n",
    "\n",
    "# Standard ADAM optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=noise_prediction_net.parameters(),\n",
    "    lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "# Cosine LR schedule with linear warmup\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     name='cosine',\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=500,\n",
    "#     num_training_steps=len(dataloader) * num_epochs\n",
    "# )\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "log_every = 10  # сколько батчей пропускать между выводом\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    epoch_loss = []\n",
    "    tic = perf_counter()\n",
    "\n",
    "    for batch_idx, nbatch in enumerate(dataloader):\n",
    "        # data normalized in dataset\n",
    "        # device transfer\n",
    "        obs_img = nbatch['img_obs'].to(device)\n",
    "        obs_action = nbatch['act_obs'].to(device)\n",
    "        target_action = nbatch['act_pred'].to(device)\n",
    "        B = obs_img.shape[0]\n",
    "\n",
    "        # encoder vision features\n",
    "        image_features = visual_encoder.encode(\n",
    "            obs_img.flatten(end_dim=1))\n",
    "        image_features = image_features.reshape(\n",
    "            *obs_img.shape[:2], -1)\n",
    "        # (B,obs_horizon,D)\n",
    "\n",
    "        # concatenate vision feature and low-dim obs\n",
    "        obs_features = torch.cat([image_features, obs_action], dim=-1)\n",
    "        obs_cond = obs_features.flatten(start_dim=1)\n",
    "        # (B, obs_horizon * obs_dim)\n",
    "\n",
    "        # sample noise to add to actions\n",
    "        noise = torch.randn(target_action.shape, device=device)\n",
    "\n",
    "        # sample a diffusion iteration for each data point\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.config.T,\n",
    "            (B,), device=device\n",
    "        ).long()\n",
    "\n",
    "        # add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_actions = noise_scheduler.add_noise(\n",
    "            target_action, noise, timesteps)\n",
    "\n",
    "        # predict the noise residual\n",
    "        noise_pred = noise_prediction_net(\n",
    "            noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "        # L2 loss\n",
    "        loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # step lr scheduler every batch\n",
    "        # this is different from standard pytorch behavior\n",
    "        # lr_scheduler.step()\n",
    "\n",
    "        # logging\n",
    "        loss_cpu = loss.item()\n",
    "        epoch_loss.append(loss_cpu)\n",
    "        if (batch_idx + 1) % log_every == 0 or (batch_idx + 1) == len(dataloader):\n",
    "            print(f\"[E{epoch_idx:03d}] batch {batch_idx + 1:04d}/{len(dataloader)} \"\n",
    "                  f\"loss={loss_cpu:.4f}\", flush=True)\n",
    "\n",
    "    toc = perf_counter()\n",
    "    print(f\"[E{epoch_idx:03d}] mean_loss={np.mean(epoch_loss):.4f} \"\n",
    "          f\"({toc - tic:.1f}s)\\n\", flush=True)"
   ],
   "id": "1d226ec8de977aaa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E000] batch 0010/142 loss=0.2310\n",
      "[E000] batch 0020/142 loss=0.1297\n",
      "[E000] batch 0030/142 loss=0.1065\n",
      "[E000] batch 0040/142 loss=0.1175\n",
      "[E000] batch 0050/142 loss=0.0681\n",
      "[E000] batch 0060/142 loss=0.0801\n",
      "[E000] batch 0070/142 loss=0.0734\n",
      "[E000] batch 0080/142 loss=0.0800\n",
      "[E000] batch 0090/142 loss=0.0620\n",
      "[E000] batch 0100/142 loss=0.0538\n",
      "[E000] batch 0110/142 loss=0.0665\n",
      "[E000] batch 0120/142 loss=0.0481\n",
      "[E000] batch 0130/142 loss=0.0580\n",
      "[E000] batch 0140/142 loss=0.0645\n",
      "[E000] batch 0142/142 loss=0.0367\n",
      "[E000] mean_loss=0.1034 (64.4s)\n",
      "\n",
      "[E001] batch 0010/142 loss=0.0460\n",
      "[E001] batch 0020/142 loss=0.0576\n",
      "[E001] batch 0030/142 loss=0.0578\n",
      "[E001] batch 0040/142 loss=0.0796\n",
      "[E001] batch 0050/142 loss=0.0546\n",
      "[E001] batch 0060/142 loss=0.0595\n",
      "[E001] batch 0070/142 loss=0.0276\n",
      "[E001] batch 0080/142 loss=0.0657\n",
      "[E001] batch 0090/142 loss=0.0475\n",
      "[E001] batch 0100/142 loss=0.0359\n",
      "[E001] batch 0110/142 loss=0.0501\n",
      "[E001] batch 0120/142 loss=0.0464\n",
      "[E001] batch 0130/142 loss=0.0413\n",
      "[E001] batch 0140/142 loss=0.0458\n",
      "[E001] batch 0142/142 loss=0.0914\n",
      "[E001] mean_loss=0.0514 (64.8s)\n",
      "\n",
      "[E002] batch 0010/142 loss=0.0431\n",
      "[E002] batch 0020/142 loss=0.0392\n",
      "[E002] batch 0030/142 loss=0.0440\n",
      "[E002] batch 0040/142 loss=0.0400\n",
      "[E002] batch 0050/142 loss=0.0370\n",
      "[E002] batch 0060/142 loss=0.0513\n",
      "[E002] batch 0070/142 loss=0.0552\n",
      "[E002] batch 0080/142 loss=0.0421\n",
      "[E002] batch 0090/142 loss=0.0638\n",
      "[E002] batch 0100/142 loss=0.0363\n",
      "[E002] batch 0110/142 loss=0.0491\n",
      "[E002] batch 0120/142 loss=0.0425\n",
      "[E002] batch 0130/142 loss=0.0475\n",
      "[E002] batch 0140/142 loss=0.0287\n",
      "[E002] batch 0142/142 loss=0.0455\n",
      "[E002] mean_loss=0.0458 (65.0s)\n",
      "\n",
      "[E003] batch 0010/142 loss=0.0342\n",
      "[E003] batch 0020/142 loss=0.0311\n",
      "[E003] batch 0030/142 loss=0.0442\n",
      "[E003] batch 0040/142 loss=0.0353\n",
      "[E003] batch 0050/142 loss=0.0342\n",
      "[E003] batch 0060/142 loss=0.0355\n",
      "[E003] batch 0070/142 loss=0.0354\n",
      "[E003] batch 0080/142 loss=0.0515\n",
      "[E003] batch 0090/142 loss=0.0532\n",
      "[E003] batch 0100/142 loss=0.0405\n",
      "[E003] batch 0110/142 loss=0.0521\n",
      "[E003] batch 0120/142 loss=0.0500\n",
      "[E003] batch 0130/142 loss=0.0301\n",
      "[E003] batch 0140/142 loss=0.0351\n",
      "[E003] batch 0142/142 loss=0.0258\n",
      "[E003] mean_loss=0.0415 (65.8s)\n",
      "\n",
      "[E004] batch 0010/142 loss=0.0351\n",
      "[E004] batch 0020/142 loss=0.0302\n",
      "[E004] batch 0030/142 loss=0.0455\n",
      "[E004] batch 0040/142 loss=0.0405\n",
      "[E004] batch 0050/142 loss=0.0399\n",
      "[E004] batch 0060/142 loss=0.0287\n",
      "[E004] batch 0070/142 loss=0.0367\n",
      "[E004] batch 0080/142 loss=0.0279\n",
      "[E004] batch 0090/142 loss=0.0326\n",
      "[E004] batch 0100/142 loss=0.0300\n",
      "[E004] batch 0110/142 loss=0.0547\n",
      "[E004] batch 0120/142 loss=0.0554\n",
      "[E004] batch 0130/142 loss=0.0567\n",
      "[E004] batch 0140/142 loss=0.0454\n",
      "[E004] batch 0142/142 loss=0.0365\n",
      "[E004] mean_loss=0.0395 (66.1s)\n",
      "\n",
      "[E005] batch 0010/142 loss=0.0370\n",
      "[E005] batch 0020/142 loss=0.0429\n",
      "[E005] batch 0030/142 loss=0.0474\n",
      "[E005] batch 0040/142 loss=0.0235\n",
      "[E005] batch 0050/142 loss=0.0259\n",
      "[E005] batch 0060/142 loss=0.0230\n",
      "[E005] batch 0070/142 loss=0.0350\n",
      "[E005] batch 0080/142 loss=0.0330\n",
      "[E005] batch 0090/142 loss=0.0292\n",
      "[E005] batch 0100/142 loss=0.0413\n",
      "[E005] batch 0110/142 loss=0.0610\n",
      "[E005] batch 0120/142 loss=0.0349\n",
      "[E005] batch 0130/142 loss=0.0390\n",
      "[E005] batch 0140/142 loss=0.0389\n",
      "[E005] batch 0142/142 loss=0.1363\n",
      "[E005] mean_loss=0.0377 (65.6s)\n",
      "\n",
      "[E006] batch 0010/142 loss=0.0539\n",
      "[E006] batch 0020/142 loss=0.0212\n",
      "[E006] batch 0030/142 loss=0.0344\n",
      "[E006] batch 0040/142 loss=0.0446\n",
      "[E006] batch 0050/142 loss=0.0313\n",
      "[E006] batch 0060/142 loss=0.0429\n",
      "[E006] batch 0070/142 loss=0.0336\n",
      "[E006] batch 0080/142 loss=0.0455\n",
      "[E006] batch 0090/142 loss=0.0326\n",
      "[E006] batch 0100/142 loss=0.0448\n",
      "[E006] batch 0110/142 loss=0.0424\n",
      "[E006] batch 0120/142 loss=0.0288\n",
      "[E006] batch 0130/142 loss=0.0253\n",
      "[E006] batch 0140/142 loss=0.0339\n",
      "[E006] batch 0142/142 loss=0.0608\n",
      "[E006] mean_loss=0.0383 (65.7s)\n",
      "\n",
      "[E007] batch 0010/142 loss=0.0604\n",
      "[E007] batch 0020/142 loss=0.0300\n",
      "[E007] batch 0030/142 loss=0.0413\n",
      "[E007] batch 0040/142 loss=0.0327\n",
      "[E007] batch 0050/142 loss=0.0436\n",
      "[E007] batch 0060/142 loss=0.0379\n",
      "[E007] batch 0070/142 loss=0.0270\n",
      "[E007] batch 0080/142 loss=0.0301\n",
      "[E007] batch 0090/142 loss=0.0295\n",
      "[E007] batch 0100/142 loss=0.0253\n",
      "[E007] batch 0110/142 loss=0.0242\n",
      "[E007] batch 0120/142 loss=0.0394\n",
      "[E007] batch 0130/142 loss=0.0259\n",
      "[E007] batch 0140/142 loss=0.0484\n",
      "[E007] batch 0142/142 loss=0.0330\n",
      "[E007] mean_loss=0.0364 (65.8s)\n",
      "\n",
      "[E008] batch 0010/142 loss=0.0392\n",
      "[E008] batch 0020/142 loss=0.0201\n",
      "[E008] batch 0030/142 loss=0.0339\n",
      "[E008] batch 0040/142 loss=0.0317\n",
      "[E008] batch 0050/142 loss=0.0453\n",
      "[E008] batch 0060/142 loss=0.0355\n",
      "[E008] batch 0070/142 loss=0.0360\n",
      "[E008] batch 0080/142 loss=0.0331\n",
      "[E008] batch 0090/142 loss=0.0467\n",
      "[E008] batch 0100/142 loss=0.0448\n",
      "[E008] batch 0110/142 loss=0.0264\n",
      "[E008] batch 0120/142 loss=0.0311\n",
      "[E008] batch 0130/142 loss=0.0403\n",
      "[E008] batch 0140/142 loss=0.0367\n",
      "[E008] batch 0142/142 loss=0.0108\n",
      "[E008] mean_loss=0.0350 (66.2s)\n",
      "\n",
      "[E009] batch 0010/142 loss=0.0340\n",
      "[E009] batch 0020/142 loss=0.0519\n",
      "[E009] batch 0030/142 loss=0.0171\n",
      "[E009] batch 0040/142 loss=0.0271\n",
      "[E009] batch 0050/142 loss=0.0423\n",
      "[E009] batch 0060/142 loss=0.0458\n",
      "[E009] batch 0070/142 loss=0.0262\n",
      "[E009] batch 0080/142 loss=0.0240\n",
      "[E009] batch 0090/142 loss=0.0288\n",
      "[E009] batch 0100/142 loss=0.0290\n",
      "[E009] batch 0110/142 loss=0.0492\n",
      "[E009] batch 0120/142 loss=0.0523\n",
      "[E009] batch 0130/142 loss=0.0581\n",
      "[E009] batch 0140/142 loss=0.0293\n",
      "[E009] batch 0142/142 loss=0.0416\n",
      "[E009] mean_loss=0.0337 (66.0s)\n",
      "\n",
      "[E010] batch 0010/142 loss=0.0324\n",
      "[E010] batch 0020/142 loss=0.0275\n",
      "[E010] batch 0030/142 loss=0.0342\n",
      "[E010] batch 0040/142 loss=0.0216\n",
      "[E010] batch 0050/142 loss=0.0254\n",
      "[E010] batch 0060/142 loss=0.0221\n",
      "[E010] batch 0070/142 loss=0.0537\n",
      "[E010] batch 0080/142 loss=0.0417\n",
      "[E010] batch 0090/142 loss=0.0394\n",
      "[E010] batch 0100/142 loss=0.0350\n",
      "[E010] batch 0110/142 loss=0.0367\n",
      "[E010] batch 0120/142 loss=0.0252\n",
      "[E010] batch 0130/142 loss=0.0298\n",
      "[E010] batch 0140/142 loss=0.0430\n",
      "[E010] batch 0142/142 loss=0.0303\n",
      "[E010] mean_loss=0.0329 (65.9s)\n",
      "\n",
      "[E011] batch 0010/142 loss=0.0407\n",
      "[E011] batch 0020/142 loss=0.0359\n",
      "[E011] batch 0030/142 loss=0.0367\n",
      "[E011] batch 0040/142 loss=0.0294\n",
      "[E011] batch 0050/142 loss=0.0367\n",
      "[E011] batch 0060/142 loss=0.0356\n",
      "[E011] batch 0070/142 loss=0.0323\n",
      "[E011] batch 0080/142 loss=0.0266\n",
      "[E011] batch 0090/142 loss=0.0288\n",
      "[E011] batch 0100/142 loss=0.0408\n",
      "[E011] batch 0110/142 loss=0.0403\n",
      "[E011] batch 0120/142 loss=0.0377\n",
      "[E011] batch 0130/142 loss=0.0213\n",
      "[E011] batch 0140/142 loss=0.0317\n",
      "[E011] batch 0142/142 loss=0.0467\n",
      "[E011] mean_loss=0.0329 (66.4s)\n",
      "\n",
      "[E012] batch 0010/142 loss=0.0283\n",
      "[E012] batch 0020/142 loss=0.0194\n",
      "[E012] batch 0030/142 loss=0.0266\n",
      "[E012] batch 0040/142 loss=0.0387\n",
      "[E012] batch 0050/142 loss=0.0406\n",
      "[E012] batch 0060/142 loss=0.0225\n",
      "[E012] batch 0070/142 loss=0.0236\n",
      "[E012] batch 0080/142 loss=0.0310\n",
      "[E012] batch 0090/142 loss=0.0328\n",
      "[E012] batch 0100/142 loss=0.0206\n",
      "[E012] batch 0110/142 loss=0.0317\n",
      "[E012] batch 0120/142 loss=0.0367\n",
      "[E012] batch 0130/142 loss=0.0474\n",
      "[E012] batch 0140/142 loss=0.0348\n",
      "[E012] batch 0142/142 loss=0.0273\n",
      "[E012] mean_loss=0.0317 (66.3s)\n",
      "\n",
      "[E013] batch 0010/142 loss=0.0345\n",
      "[E013] batch 0020/142 loss=0.0316\n",
      "[E013] batch 0030/142 loss=0.0408\n",
      "[E013] batch 0040/142 loss=0.0346\n",
      "[E013] batch 0050/142 loss=0.0290\n",
      "[E013] batch 0060/142 loss=0.0307\n",
      "[E013] batch 0070/142 loss=0.0269\n",
      "[E013] batch 0080/142 loss=0.0200\n",
      "[E013] batch 0090/142 loss=0.0309\n",
      "[E013] batch 0100/142 loss=0.0313\n",
      "[E013] batch 0110/142 loss=0.0435\n",
      "[E013] batch 0120/142 loss=0.0234\n",
      "[E013] batch 0130/142 loss=0.0386\n",
      "[E013] batch 0140/142 loss=0.0328\n",
      "[E013] batch 0142/142 loss=0.0511\n",
      "[E013] mean_loss=0.0320 (66.8s)\n",
      "\n",
      "[E014] batch 0010/142 loss=0.0439\n",
      "[E014] batch 0020/142 loss=0.0379\n",
      "[E014] batch 0030/142 loss=0.0395\n",
      "[E014] batch 0040/142 loss=0.0363\n",
      "[E014] batch 0050/142 loss=0.0406\n",
      "[E014] batch 0060/142 loss=0.0377\n",
      "[E014] batch 0070/142 loss=0.0454\n",
      "[E014] batch 0080/142 loss=0.0494\n",
      "[E014] batch 0090/142 loss=0.0358\n",
      "[E014] batch 0100/142 loss=0.0442\n",
      "[E014] batch 0110/142 loss=0.0307\n",
      "[E014] batch 0120/142 loss=0.0328\n",
      "[E014] batch 0130/142 loss=0.0294\n",
      "[E014] batch 0140/142 loss=0.0211\n",
      "[E014] batch 0142/142 loss=0.0193\n",
      "[E014] mean_loss=0.0314 (66.8s)\n",
      "\n",
      "[E015] batch 0010/142 loss=0.0254\n",
      "[E015] batch 0020/142 loss=0.0422\n",
      "[E015] batch 0030/142 loss=0.0372\n",
      "[E015] batch 0040/142 loss=0.0254\n",
      "[E015] batch 0050/142 loss=0.0339\n",
      "[E015] batch 0060/142 loss=0.0339\n",
      "[E015] batch 0070/142 loss=0.0293\n",
      "[E015] batch 0080/142 loss=0.0381\n",
      "[E015] batch 0090/142 loss=0.0183\n",
      "[E015] batch 0100/142 loss=0.0286\n",
      "[E015] batch 0110/142 loss=0.0219\n",
      "[E015] batch 0120/142 loss=0.0253\n",
      "[E015] batch 0130/142 loss=0.0279\n",
      "[E015] batch 0140/142 loss=0.0340\n",
      "[E015] batch 0142/142 loss=0.0385\n",
      "[E015] mean_loss=0.0312 (66.8s)\n",
      "\n",
      "[E016] batch 0010/142 loss=0.0164\n",
      "[E016] batch 0020/142 loss=0.0241\n",
      "[E016] batch 0030/142 loss=0.0162\n",
      "[E016] batch 0040/142 loss=0.0404\n",
      "[E016] batch 0050/142 loss=0.0228\n",
      "[E016] batch 0060/142 loss=0.0407\n",
      "[E016] batch 0070/142 loss=0.0370\n",
      "[E016] batch 0080/142 loss=0.0296\n",
      "[E016] batch 0090/142 loss=0.0264\n",
      "[E016] batch 0100/142 loss=0.0165\n",
      "[E016] batch 0110/142 loss=0.0183\n",
      "[E016] batch 0120/142 loss=0.0225\n",
      "[E016] batch 0130/142 loss=0.0229\n",
      "[E016] batch 0140/142 loss=0.0302\n",
      "[E016] batch 0142/142 loss=0.0488\n",
      "[E016] mean_loss=0.0306 (66.8s)\n",
      "\n",
      "[E017] batch 0010/142 loss=0.0263\n",
      "[E017] batch 0020/142 loss=0.0362\n",
      "[E017] batch 0030/142 loss=0.0244\n",
      "[E017] batch 0040/142 loss=0.0214\n",
      "[E017] batch 0050/142 loss=0.0416\n",
      "[E017] batch 0060/142 loss=0.0364\n",
      "[E017] batch 0070/142 loss=0.0273\n",
      "[E017] batch 0080/142 loss=0.0331\n",
      "[E017] batch 0090/142 loss=0.0335\n",
      "[E017] batch 0100/142 loss=0.0232\n",
      "[E017] batch 0110/142 loss=0.0472\n",
      "[E017] batch 0120/142 loss=0.0355\n",
      "[E017] batch 0130/142 loss=0.0427\n",
      "[E017] batch 0140/142 loss=0.0168\n",
      "[E017] batch 0142/142 loss=0.0208\n",
      "[E017] mean_loss=0.0299 (66.4s)\n",
      "\n",
      "[E018] batch 0010/142 loss=0.0327\n",
      "[E018] batch 0020/142 loss=0.0265\n",
      "[E018] batch 0030/142 loss=0.0232\n",
      "[E018] batch 0040/142 loss=0.0187\n",
      "[E018] batch 0050/142 loss=0.0379\n",
      "[E018] batch 0060/142 loss=0.0291\n",
      "[E018] batch 0070/142 loss=0.0324\n",
      "[E018] batch 0080/142 loss=0.0273\n",
      "[E018] batch 0090/142 loss=0.0214\n",
      "[E018] batch 0100/142 loss=0.0180\n",
      "[E018] batch 0110/142 loss=0.0330\n",
      "[E018] batch 0120/142 loss=0.0309\n",
      "[E018] batch 0130/142 loss=0.0236\n",
      "[E018] batch 0140/142 loss=0.0300\n",
      "[E018] batch 0142/142 loss=0.0250\n",
      "[E018] mean_loss=0.0286 (66.6s)\n",
      "\n",
      "[E019] batch 0010/142 loss=0.0316\n",
      "[E019] batch 0020/142 loss=0.0325\n",
      "[E019] batch 0030/142 loss=0.0344\n",
      "[E019] batch 0040/142 loss=0.0307\n",
      "[E019] batch 0050/142 loss=0.0360\n",
      "[E019] batch 0060/142 loss=0.0334\n",
      "[E019] batch 0070/142 loss=0.0265\n",
      "[E019] batch 0080/142 loss=0.0301\n",
      "[E019] batch 0090/142 loss=0.0369\n",
      "[E019] batch 0100/142 loss=0.0250\n",
      "[E019] batch 0110/142 loss=0.0161\n",
      "[E019] batch 0120/142 loss=0.0151\n",
      "[E019] batch 0130/142 loss=0.0253\n",
      "[E019] batch 0140/142 loss=0.0260\n",
      "[E019] batch 0142/142 loss=0.0351\n",
      "[E019] mean_loss=0.0299 (66.0s)\n",
      "\n",
      "[E020] batch 0010/142 loss=0.0269\n",
      "[E020] batch 0020/142 loss=0.0413\n",
      "[E020] batch 0030/142 loss=0.0277\n",
      "[E020] batch 0040/142 loss=0.0179\n",
      "[E020] batch 0050/142 loss=0.0294\n",
      "[E020] batch 0060/142 loss=0.0241\n",
      "[E020] batch 0070/142 loss=0.0229\n",
      "[E020] batch 0080/142 loss=0.0232\n",
      "[E020] batch 0090/142 loss=0.0364\n",
      "[E020] batch 0100/142 loss=0.0259\n",
      "[E020] batch 0110/142 loss=0.0155\n",
      "[E020] batch 0120/142 loss=0.0164\n",
      "[E020] batch 0130/142 loss=0.0331\n",
      "[E020] batch 0140/142 loss=0.0453\n",
      "[E020] batch 0142/142 loss=0.0269\n",
      "[E020] mean_loss=0.0294 (66.9s)\n",
      "\n",
      "[E021] batch 0010/142 loss=0.0345\n",
      "[E021] batch 0020/142 loss=0.0234\n",
      "[E021] batch 0030/142 loss=0.0271\n",
      "[E021] batch 0040/142 loss=0.0182\n",
      "[E021] batch 0050/142 loss=0.0225\n",
      "[E021] batch 0060/142 loss=0.0290\n",
      "[E021] batch 0070/142 loss=0.0375\n",
      "[E021] batch 0080/142 loss=0.0286\n",
      "[E021] batch 0090/142 loss=0.0382\n",
      "[E021] batch 0100/142 loss=0.0281\n",
      "[E021] batch 0110/142 loss=0.0181\n",
      "[E021] batch 0120/142 loss=0.0368\n",
      "[E021] batch 0130/142 loss=0.0229\n",
      "[E021] batch 0140/142 loss=0.0365\n",
      "[E021] batch 0142/142 loss=0.0650\n",
      "[E021] mean_loss=0.0289 (66.3s)\n",
      "\n",
      "[E022] batch 0010/142 loss=0.0293\n",
      "[E022] batch 0020/142 loss=0.0325\n",
      "[E022] batch 0030/142 loss=0.0292\n",
      "[E022] batch 0040/142 loss=0.0205\n",
      "[E022] batch 0050/142 loss=0.0302\n",
      "[E022] batch 0060/142 loss=0.0294\n",
      "[E022] batch 0070/142 loss=0.0382\n",
      "[E022] batch 0080/142 loss=0.0262\n",
      "[E022] batch 0090/142 loss=0.0198\n",
      "[E022] batch 0100/142 loss=0.0336\n",
      "[E022] batch 0110/142 loss=0.0233\n",
      "[E022] batch 0120/142 loss=0.0366\n",
      "[E022] batch 0130/142 loss=0.0264\n",
      "[E022] batch 0140/142 loss=0.0237\n",
      "[E022] batch 0142/142 loss=0.0193\n",
      "[E022] mean_loss=0.0284 (66.8s)\n",
      "\n",
      "[E023] batch 0010/142 loss=0.0252\n",
      "[E023] batch 0020/142 loss=0.0281\n",
      "[E023] batch 0030/142 loss=0.0363\n",
      "[E023] batch 0040/142 loss=0.0175\n",
      "[E023] batch 0050/142 loss=0.0208\n",
      "[E023] batch 0060/142 loss=0.0140\n",
      "[E023] batch 0070/142 loss=0.0239\n",
      "[E023] batch 0080/142 loss=0.0270\n",
      "[E023] batch 0090/142 loss=0.0257\n",
      "[E023] batch 0100/142 loss=0.0346\n",
      "[E023] batch 0110/142 loss=0.0318\n",
      "[E023] batch 0120/142 loss=0.0338\n",
      "[E023] batch 0130/142 loss=0.0236\n",
      "[E023] batch 0140/142 loss=0.0209\n",
      "[E023] batch 0142/142 loss=0.0398\n",
      "[E023] mean_loss=0.0267 (67.1s)\n",
      "\n",
      "[E024] batch 0010/142 loss=0.0357\n",
      "[E024] batch 0020/142 loss=0.0201\n",
      "[E024] batch 0030/142 loss=0.0284\n",
      "[E024] batch 0040/142 loss=0.0473\n",
      "[E024] batch 0050/142 loss=0.0232\n",
      "[E024] batch 0060/142 loss=0.0208\n",
      "[E024] batch 0070/142 loss=0.0298\n",
      "[E024] batch 0080/142 loss=0.0200\n",
      "[E024] batch 0090/142 loss=0.0143\n",
      "[E024] batch 0100/142 loss=0.0441\n",
      "[E024] batch 0110/142 loss=0.0232\n",
      "[E024] batch 0120/142 loss=0.0377\n",
      "[E024] batch 0130/142 loss=0.0142\n",
      "[E024] batch 0140/142 loss=0.0174\n",
      "[E024] batch 0142/142 loss=0.0154\n",
      "[E024] mean_loss=0.0276 (66.7s)\n",
      "\n",
      "[E025] batch 0010/142 loss=0.0190\n",
      "[E025] batch 0020/142 loss=0.0357\n",
      "[E025] batch 0030/142 loss=0.0163\n",
      "[E025] batch 0040/142 loss=0.0264\n",
      "[E025] batch 0050/142 loss=0.0323\n",
      "[E025] batch 0060/142 loss=0.0248\n",
      "[E025] batch 0070/142 loss=0.0216\n",
      "[E025] batch 0080/142 loss=0.0176\n",
      "[E025] batch 0090/142 loss=0.0240\n",
      "[E025] batch 0100/142 loss=0.0382\n",
      "[E025] batch 0110/142 loss=0.0292\n",
      "[E025] batch 0120/142 loss=0.0311\n",
      "[E025] batch 0130/142 loss=0.0286\n",
      "[E025] batch 0140/142 loss=0.0330\n",
      "[E025] batch 0142/142 loss=0.0156\n",
      "[E025] mean_loss=0.0275 (66.4s)\n",
      "\n",
      "[E026] batch 0010/142 loss=0.0250\n",
      "[E026] batch 0020/142 loss=0.0206\n",
      "[E026] batch 0030/142 loss=0.0242\n",
      "[E026] batch 0040/142 loss=0.0177\n",
      "[E026] batch 0050/142 loss=0.0269\n",
      "[E026] batch 0060/142 loss=0.0176\n",
      "[E026] batch 0070/142 loss=0.0213\n",
      "[E026] batch 0080/142 loss=0.0194\n",
      "[E026] batch 0090/142 loss=0.0262\n",
      "[E026] batch 0100/142 loss=0.0224\n",
      "[E026] batch 0110/142 loss=0.0361\n",
      "[E026] batch 0120/142 loss=0.0310\n",
      "[E026] batch 0130/142 loss=0.0274\n",
      "[E026] batch 0140/142 loss=0.0305\n",
      "[E026] batch 0142/142 loss=0.0210\n",
      "[E026] mean_loss=0.0265 (67.1s)\n",
      "\n",
      "[E027] batch 0010/142 loss=0.0352\n",
      "[E027] batch 0020/142 loss=0.0231\n",
      "[E027] batch 0030/142 loss=0.0191\n",
      "[E027] batch 0040/142 loss=0.0249\n",
      "[E027] batch 0050/142 loss=0.0237\n",
      "[E027] batch 0060/142 loss=0.0226\n",
      "[E027] batch 0070/142 loss=0.0160\n",
      "[E027] batch 0080/142 loss=0.0169\n",
      "[E027] batch 0090/142 loss=0.0223\n",
      "[E027] batch 0100/142 loss=0.0312\n",
      "[E027] batch 0110/142 loss=0.0380\n",
      "[E027] batch 0120/142 loss=0.0338\n",
      "[E027] batch 0130/142 loss=0.0260\n",
      "[E027] batch 0140/142 loss=0.0235\n",
      "[E027] batch 0142/142 loss=0.0726\n",
      "[E027] mean_loss=0.0261 (66.8s)\n",
      "\n",
      "[E028] batch 0010/142 loss=0.0242\n",
      "[E028] batch 0020/142 loss=0.0396\n",
      "[E028] batch 0030/142 loss=0.0229\n",
      "[E028] batch 0040/142 loss=0.0241\n",
      "[E028] batch 0050/142 loss=0.0226\n",
      "[E028] batch 0060/142 loss=0.0387\n",
      "[E028] batch 0070/142 loss=0.0201\n",
      "[E028] batch 0080/142 loss=0.0286\n",
      "[E028] batch 0090/142 loss=0.0187\n",
      "[E028] batch 0100/142 loss=0.0237\n",
      "[E028] batch 0110/142 loss=0.0353\n",
      "[E028] batch 0120/142 loss=0.0368\n",
      "[E028] batch 0130/142 loss=0.0243\n",
      "[E028] batch 0140/142 loss=0.0298\n",
      "[E028] batch 0142/142 loss=0.0223\n",
      "[E028] mean_loss=0.0269 (67.1s)\n",
      "\n",
      "[E029] batch 0010/142 loss=0.0131\n",
      "[E029] batch 0020/142 loss=0.0213\n",
      "[E029] batch 0030/142 loss=0.0205\n",
      "[E029] batch 0040/142 loss=0.0227\n",
      "[E029] batch 0050/142 loss=0.0179\n",
      "[E029] batch 0060/142 loss=0.0180\n",
      "[E029] batch 0070/142 loss=0.0265\n",
      "[E029] batch 0080/142 loss=0.0223\n",
      "[E029] batch 0090/142 loss=0.0364\n",
      "[E029] batch 0100/142 loss=0.0401\n",
      "[E029] batch 0110/142 loss=0.0269\n",
      "[E029] batch 0120/142 loss=0.0213\n",
      "[E029] batch 0130/142 loss=0.0188\n",
      "[E029] batch 0140/142 loss=0.0342\n",
      "[E029] batch 0142/142 loss=0.0510\n",
      "[E029] mean_loss=0.0260 (68.3s)\n",
      "\n",
      "[E030] batch 0010/142 loss=0.0303\n",
      "[E030] batch 0020/142 loss=0.0228\n",
      "[E030] batch 0030/142 loss=0.0308\n",
      "[E030] batch 0040/142 loss=0.0246\n",
      "[E030] batch 0050/142 loss=0.0180\n",
      "[E030] batch 0060/142 loss=0.0284\n",
      "[E030] batch 0070/142 loss=0.0173\n",
      "[E030] batch 0080/142 loss=0.0265\n",
      "[E030] batch 0090/142 loss=0.0257\n",
      "[E030] batch 0100/142 loss=0.0408\n",
      "[E030] batch 0110/142 loss=0.0433\n",
      "[E030] batch 0120/142 loss=0.0180\n",
      "[E030] batch 0130/142 loss=0.0281\n",
      "[E030] batch 0140/142 loss=0.0277\n",
      "[E030] batch 0142/142 loss=0.0095\n",
      "[E030] mean_loss=0.0258 (68.0s)\n",
      "\n",
      "[E031] batch 0010/142 loss=0.0225\n",
      "[E031] batch 0020/142 loss=0.0311\n",
      "[E031] batch 0030/142 loss=0.0335\n",
      "[E031] batch 0040/142 loss=0.0129\n",
      "[E031] batch 0050/142 loss=0.0138\n",
      "[E031] batch 0060/142 loss=0.0256\n",
      "[E031] batch 0070/142 loss=0.0353\n",
      "[E031] batch 0080/142 loss=0.0207\n",
      "[E031] batch 0090/142 loss=0.0182\n",
      "[E031] batch 0100/142 loss=0.0143\n",
      "[E031] batch 0110/142 loss=0.0176\n",
      "[E031] batch 0120/142 loss=0.0152\n",
      "[E031] batch 0130/142 loss=0.0220\n",
      "[E031] batch 0140/142 loss=0.0187\n",
      "[E031] batch 0142/142 loss=0.0667\n",
      "[E031] mean_loss=0.0245 (67.0s)\n",
      "\n",
      "[E032] batch 0010/142 loss=0.0385\n",
      "[E032] batch 0020/142 loss=0.0295\n",
      "[E032] batch 0030/142 loss=0.0277\n",
      "[E032] batch 0040/142 loss=0.0314\n",
      "[E032] batch 0050/142 loss=0.0290\n",
      "[E032] batch 0060/142 loss=0.0225\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T10:52:31.719660Z",
     "start_time": "2025-05-21T10:52:22.962157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import collections\n",
    "from robotics.gym_pusht.envs.pusht import PushTEnv\n",
    "\n",
    "env = PushTEnv(obs_type=\"pixels\", render_mode=\"human\", goal_pose=\"random\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "obs_deque = collections.deque(\n",
    "    [obs] * obs_horizon, maxlen=obs_horizon)\n",
    "\n",
    "# save visualization and rewards\n",
    "imgs = [env.render(mode='rgb_array')]\n",
    "rewards = list()\n",
    "done = False\n",
    "step_idx = 0\n",
    "\n",
    "while not done:\n",
    "    input_imgs ="
   ],
   "id": "e8b5d39c57901f97",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 794030, 794031, 794032, 794033) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mEmpty\u001B[39m                                     Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/diffusion/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1284\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._try_get_data\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m   1283\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1284\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_data_queue\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1285\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/diffusion/lib/python3.12/queue.py:179\u001B[39m, in \u001B[36mQueue.get\u001B[39m\u001B[34m(self, block, timeout)\u001B[39m\n\u001B[32m    178\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m remaining <= \u001B[32m0.0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m179\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[32m    180\u001B[39m \u001B[38;5;28mself\u001B[39m.not_empty.wait(remaining)\n",
      "\u001B[31mEmpty\u001B[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m     22\u001B[39m epoch_loss = \u001B[38;5;28mlist\u001B[39m()\n\u001B[32m     23\u001B[39m \u001B[38;5;66;03m# batch loop\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnbatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# data normalized in dataset\u001B[39;49;00m\n\u001B[32m     26\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# device transfer\u001B[39;49;00m\n\u001B[32m     27\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[32;43m33\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     29\u001B[39m \u001B[43m    \u001B[49m\u001B[43mobs_img\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mnbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mimg_obs\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43mobs_horizon\u001B[49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/diffusion/lib/python3.12/site-packages/torch/utils/data/dataloader.py:490\u001B[39m, in \u001B[36mDataLoader.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    488\u001B[39m         \u001B[38;5;28mself\u001B[39m._iterator = \u001B[38;5;28mself\u001B[39m._get_iterator()\n\u001B[32m    489\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m490\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_iterator\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_reset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    491\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._iterator\n\u001B[32m    492\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/diffusion/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1263\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._reset\u001B[39m\u001B[34m(self, loader, first_iter)\u001B[39m\n\u001B[32m   1261\u001B[39m resume_iteration_cnt = \u001B[38;5;28mself\u001B[39m._num_workers\n\u001B[32m   1262\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m resume_iteration_cnt > \u001B[32m0\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m1263\u001B[39m     return_idx, return_data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1264\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(return_idx, _utils.worker._ResumeIteration):\n\u001B[32m   1265\u001B[39m         \u001B[38;5;28;01massert\u001B[39;00m return_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/diffusion/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1443\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._get_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1441\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m   1442\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory_thread.is_alive():\n\u001B[32m-> \u001B[39m\u001B[32m1443\u001B[39m         success, data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1444\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[32m   1445\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/diffusion/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1297\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._try_get_data\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m   1295\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(failed_workers) > \u001B[32m0\u001B[39m:\n\u001B[32m   1296\u001B[39m     pids_str = \u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m.join(\u001B[38;5;28mstr\u001B[39m(w.pid) \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m failed_workers)\n\u001B[32m-> \u001B[39m\u001B[32m1297\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   1298\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mDataLoader worker (pid(s) \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpids_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m) exited unexpectedly\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1299\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m   1300\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, queue.Empty):\n\u001B[32m   1301\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[31mRuntimeError\u001B[39m: DataLoader worker (pid(s) 794030, 794031, 794032, 794033) exited unexpectedly"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6476c1228450862c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
