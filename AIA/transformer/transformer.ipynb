{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 0) Prologue: What you'll build\n",
    "\n",
    "Goal: implement and *understand* attention — first in **NumPy**, then mirror it in **PyTorch**, and finally use it in a tiny training task.\n",
    "\n",
    "Plan:\n",
    "1) NumPy: stable softmax → causal mask → Scaled Dot-Product Attention (SDPA) → Multi-Head Attention (MHA).\n",
    "2) PyTorch: SDPA → compare with `torch.nn.functional.scaled_dot_product_attention` → your own `MyMHA` vs `nn.MultiheadAttention`.\n",
    "3) Mini task: tiny copy task or bigram LM to verify it learns.\n",
    "\n",
    "Each code cell contains function stubs that raise `NotImplementedError`. Your job: implement them and run the checks.\n"
   ],
   "id": "74e6156eb286e070"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Tiny demo sizes (feel free to adjust)\n",
    "B = 2          # batch size\n",
    "T = 6          # sequence length\n",
    "d_model = 32   # model dimension\n",
    "n_heads = 4    # number of heads\n",
    "d_head = d_model // n_heads\n",
    "\n",
    "# Dummy inputs\n",
    "x_np = np.random.randn(B, T, d_model).astype(np.float32)\n",
    "x_t  = torch.tensor(x_np, dtype=torch.float32)\n",
    "\n",
    "print(\"Shapes:\", x_np.shape, x_t.shape)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1) NumPy: numerically stable softmax\n",
    "\n",
    "**Task:** Implement a numerically stable softmax along a given axis:\n",
    "- subtract the maximum along that axis\n",
    "- exponentiate\n",
    "- normalize by the sum\n",
    "\n",
    "**Checks:** Sum over the axis should be ~1; values in (0, 1)."
   ],
   "id": "1b4105deb61bf1cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def softmax_stable(x: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Implement numerically stable softmax along a given axis:\n",
    "    - subtract max along axis\n",
    "    - exponentiate\n",
    "    - normalize by sum\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement stable softmax in NumPy\")\n",
    "\n",
    "\n",
    "# --- Minimal checks (run after implementation) ---\n",
    "# y = softmax_stable(np.random.randn(2,3,4).astype(np.float32), axis=-1)\n",
    "# assert y.shape == (2,3,4)\n",
    "# assert np.allclose(y.sum(axis=-1), 1.0, atol=1e-5)\n",
    "# assert (y > 0).all() and (y < 1).all()\n"
   ],
   "id": "43c3428c3599d34a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2) NumPy: causal mask (no peeking ahead)\n",
    "\n",
    "**Task:** Create a causal mask of shape `[T, T]` that forbids attending to the future.\n",
    "- If `dtype=\"bool\"`: `True` means \"masked out\".\n",
    "- If `dtype=\"float\"`: 0.0 for keep, `-inf` for masked positions.\n",
    "\n",
    "**Hint:** `np.triu` helps."
   ],
   "id": "542bef7eb0875aeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def causal_mask_np(T: int, dtype: str = \"bool\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create an upper-triangular causal mask of shape [T, T] that masks j > i.\n",
    "    dtype:\n",
    "      - \"bool\"  -> boolean mask (True means \"mask out\")\n",
    "      - \"float\" -> float mask with 0 for keep and -inf for masked positions\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement causal mask for NumPy\")\n",
    "\n",
    "\n",
    "# --- Minimal checks ---\n",
    "# m_bool = causal_mask_np(T, dtype=\"bool\")\n",
    "# assert m_bool.shape == (T, T) and m_bool.dtype == np.bool_\n",
    "# assert m_bool[0,1] == True and m_bool[1,0] == False\n",
    "# m_flt = causal_mask_np(T, dtype=\"float\")\n",
    "# assert m_flt.shape == (T, T)"
   ],
   "id": "a077c419c8f84842"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3) NumPy: Scaled Dot-Product Attention (SDPA)\n",
    "\n",
    "**Task:** Implement `scaled_dot_product_attention_np(Q, K, V, mask=None)`.\n",
    "\n",
    "Steps:\n",
    "1) `scores = Q @ K^T / sqrt(d_k)` → shape `[B, T, T]`\n",
    "2) Apply mask:\n",
    "   - if float mask: add directly (0.0 or `-inf`)\n",
    "   - if bool mask: add a large negative number (e.g., `-1e9`) where `True`\n",
    "3) `probs = softmax_stable(scores, axis=-1)`\n",
    "4) `out = probs @ V`\n",
    "\n",
    "Return `(out, probs)`. Shapes: `out [B,T,d]`, `probs [B,T,T]`."
   ],
   "id": "1bb85020f997ccc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def scaled_dot_product_attention_np(Q: np.ndarray,\n",
    "                                    K: np.ndarray,\n",
    "                                    V: np.ndarray,\n",
    "                                    mask: np.ndarray | None = None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention in NumPy.\n",
    "    Shapes:\n",
    "      Q,K,V: [B, T, d_k]   => scores: [B, T, T]   => out: [B, T, d_v] (here d_v=d_k)\n",
    "    Return: out, attn_probs\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement NumPy SDPA\")\n",
    "\n",
    "\n",
    "# --- Minimal shape check after implementation ---\n",
    "# Q = np.random.randn(B, T, d_head).astype(np.float32)\n",
    "# K = np.random.randn(B, T, d_head).astype(np.float32)\n",
    "# V = np.random.randn(B, T, d_head).astype(np.float32)\n",
    "# cm = causal_mask_np(T, dtype=\"float\")\n",
    "# out, p = scaled_dot_product_attention_np(Q, K, V, mask=cm)\n",
    "# assert out.shape == (B, T, d_head)\n",
    "# assert p.shape == (B, T, T)"
   ],
   "id": "df5d8c310edaddf3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4) NumPy: split/combine heads\n",
    "\n",
    "**Task:** Implement:\n",
    "- `split_heads_np([B,T,d_model]) -> [B,n_heads,T,d_head]`\n",
    "- `combine_heads_np([B,n_heads,T,d_head]) -> [B,T,d_model]`\n",
    "\n",
    "**Note:** Prefer reshaping/transposing without copies when possible."
   ],
   "id": "62f39e0e21c9cd25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def split_heads_np(x: np.ndarray, n_heads: int) -> np.ndarray:\n",
    "    \"\"\"Reshape [B,T,d_model] -> [B,n_heads,T,d_head] without copying if possible.\"\"\"\n",
    "    raise NotImplementedError(\"Implement split_heads for NumPy\")\n",
    "\n",
    "\n",
    "def combine_heads_np(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Reshape [B,n_heads,T,d_head] -> [B,T,d_model].\"\"\"\n",
    "    raise NotImplementedError(\"Implement combine_heads for NumPy\")\n",
    "\n",
    "\n",
    "# --- Minimal checks ---\n",
    "# a = np.random.randn(B, T, d_model).astype(np.float32)\n",
    "# h = split_heads_np(a, n_heads)\n",
    "# assert h.shape == (B, n_heads, T, d_head)\n",
    "# a2 = combine_heads_np(h)\n",
    "# assert a2.shape == (B, T, d_model)\n",
    "# assert np.allclose(a, a2)"
   ],
   "id": "316a0ecc3101a304"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5) NumPy: full Multi-Head Attention\n",
    "\n",
    "**Task:** Implement `multi_head_attention_np` using linear projections `Wq,Wk,Wv,Wo`.\n",
    "\n",
    "Steps:\n",
    "1) `Q = x @ Wq`, `K = x @ Wk`, `V = x @ Wv`\n",
    "2) split into heads\n",
    "3) SDPA per head (same mask for all heads)\n",
    "4) combine heads, then `out = out @ Wo`\n",
    "\n",
    "Return `(out, attn_probs)` (you may return per-head probabilities or an average)."
   ],
   "id": "e3892ae58fdca13e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fixed random weights for reproducibility\n",
    "Wq = np.random.randn(d_model, d_model).astype(np.float32) / math.sqrt(d_model)\n",
    "Wk = np.random.randn(d_model, d_model).astype(np.float32) / math.sqrt(d_model)\n",
    "Wv = np.random.randn(d_model, d_model).astype(np.float32) / math.sqrt(d_model)\n",
    "Wo = np.random.randn(d_model, d_model).astype(np.float32) / math.sqrt(d_model)\n",
    "\n",
    "def multi_head_attention_np(x: np.ndarray,\n",
    "                            Wq: np.ndarray, Wk: np.ndarray, Wv: np.ndarray, Wo: np.ndarray,\n",
    "                            mask: np.ndarray | None = None):\n",
    "    \"\"\"\n",
    "    Implement Multi-Head Attention with NumPy.\n",
    "    Return: (out, attn_probs_merged_or_list)\n",
    "    - You may return per-head probs or an averaged [B,T,T].\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement NumPy MHA\")\n",
    "\n",
    "\n",
    "# --- Minimal checks ---\n",
    "# cm = causal_mask_np(T, dtype=\"float\")\n",
    "# out_np, probs_np = multi_head_attention_np(x_np, Wq, Wk, Wv, Wo, mask=cm)\n",
    "# assert out_np.shape == (B, T, d_model)"
   ],
   "id": "15060f3cd7c9a6f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6) PyTorch: SDPA by hand and compare to reference\n",
    "\n",
    "**Task:** Implement `scaled_dot_product_attention_torch(Q,K,V, attn_mask=None)` and compare it with\n",
    "`torch.nn.functional.scaled_dot_product_attention` (no dropout).\n",
    "\n",
    "**Hints:**\n",
    "- Manual: `scores = (Q @ K.transpose(-2,-1)) / sqrt(d)` → masked fill → softmax → `@ V`.\n",
    "- `attn_mask` can be `bool` (True=mask) or `float` (`-inf` where masked)."
   ],
   "id": "3064245ac520bc73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def scaled_dot_product_attention_torch(Q: torch.Tensor,\n",
    "                                       K: torch.Tensor,\n",
    "                                       V: torch.Tensor,\n",
    "                                       attn_mask: torch.Tensor | None = None):\n",
    "    \"\"\"\n",
    "    Implement SDPA in PyTorch. Return (out, probs).\n",
    "    Shapes: Q,K,V [B,T,d], attn_mask [T,T] or [B,1,T,T]\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement PyTorch SDPA\")\n",
    "\n",
    "\n",
    "# --- Comparison harness (run after implementation) ---\n",
    "# Q = torch.randn(B, T, d_head)\n",
    "# K = torch.randn(B, T, d_head)\n",
    "# V = torch.randn(B, T, d_head)\n",
    "# cm_bool = torch.from_numpy(causal_mask_np(T, dtype=\"bool\"))\n",
    "# out_ref = F.scaled_dot_product_attention(Q, K, V, attn_mask=cm_bool, dropout_p=0.0, is_causal=False)\n",
    "# out_my, p_my = scaled_dot_product_attention_torch(Q, K, V, attn_mask=cm_bool)\n",
    "# assert torch.allclose(out_ref, out_my, atol=1e-5), \"Your SDPA doesn't match F.sdpa\""
   ],
   "id": "52e63c2a6f4255a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 7) PyTorch: your own Multi-Head Attention vs `nn.MultiheadAttention`\n",
    "\n",
    "**Task:** Implement `MyMHA` (Q/K/V/O linear layers; split/combine; SDPA).\n",
    "Then compare outputs with `nn.MultiheadAttention` **after copying weights**.\n",
    "\n",
    "**Tip:** `nn.MultiheadAttention` packs QKV in `in_proj_weight`/`in_proj_bias`. You can copy those slices into your module."
   ],
   "id": "8431ec559dab0c32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MyMHA(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        # Define projections: in->Q,K,V, and out->O\n",
    "        raise NotImplementedError(\"Define linear layers for Q,K,V,O\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: torch.Tensor | None = None):\n",
    "        \"\"\"\n",
    "        x: [B,T,d_model]\n",
    "        Return: out [B,T,d_model], attn_probs [B, n_heads, T, T] (optional)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement forward: project -> split -> SDPA -> combine -> out\")\n",
    "\n",
    "\n",
    "# --- Comparison (run after implementation & weight copy) ---\n",
    "# mha_ref = nn.MultiheadAttention(d_model, n_heads, batch_first=True, bias=True)\n",
    "# my = MyMHA(d_model, n_heads, bias=True)\n",
    "# # TODO: copy weights from mha_ref into my\n",
    "# cm_bool = torch.from_numpy(causal_mask_np(T, dtype=\"bool\"))\n",
    "# out_ref, _ = mha_ref(x_t, x_t, x_t, attn_mask=cm_bool)          # batch_first=True\n",
    "# out_my, _ = my(x_t, attn_mask=cm_bool)\n",
    "# assert torch.allclose(out_ref, out_my, atol=1e-4), \"Mismatch between MyMHA and nn.MultiheadAttention\""
   ],
   "id": "fca395dc1789fbab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 8) Mini task: copy-task or tiny bigram LM\n",
    "\n",
    "Choose one:\n",
    "\n",
    "**Copy task:** input is a sequence over {0,1,2,3}; predict the same sequence shifted by 1.\n",
    "**Bigram LM:** given a tiny text corpus, predict next token.\n",
    "\n",
    "**Tasks:**\n",
    "1) Implement `TinyEmbedding`, `TinyBlock` (MHA + FFN), `TinyLMHead`.\n",
    "2) Write a simple training loop (cross-entropy, Adam).\n",
    "3) Verify that the loss decreases and predictions improve."
   ],
   "id": "1892e49d0e52a450"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TinyEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int):\n",
    "        super().__init__()\n",
    "        raise NotImplementedError(\"Create token embedding and optional positional embedding\")\n",
    "\n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        idx: [B,T] -> return [B,T,d_model]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement forward for embeddings\")\n",
    "\n",
    "\n",
    "class TinyBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, mlp_ratio: float = 4.0, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        raise NotImplementedError(\"Create MyMHA (or nn.MultiheadAttention), LayerNorms, and MLP\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: torch.Tensor | None = None) -> torch.Tensor:\n",
    "        raise NotImplementedError(\"Implement Transformer block forward\")\n",
    "\n",
    "\n",
    "class TinyLMHead(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        raise NotImplementedError(\"Create output projection to vocab\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [B,T,d_model] -> logits: [B,T,vocab]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement forward\")\n",
    "\n",
    "\n",
    "class TinyModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, n_heads: int, n_layers: int):\n",
    "        super().__init__()\n",
    "        raise NotImplementedError(\"Assemble Embedding -> n*Blocks -> Head\")\n",
    "\n",
    "    def forward(self, idx: torch.Tensor, attn_mask: torch.Tensor | None = None):\n",
    "        raise NotImplementedError(\"Implement forward to produce logits\")\n",
    "\n",
    "\n",
    "# --- Simple dataset for copy task (example skeleton) ---\n",
    "# def make_copy_batch(B:int, T:int, vocab:int=4):\n",
    "#     data = torch.randint(low=0, high=vocab, size=(B,T))\n",
    "#     x = data.clone()\n",
    "#     y = data.clone()  # predict same with shift by 1 later in loss\n",
    "#     return x, y\n",
    "#\n",
    "# --- Training loop sketch (fill in) ---\n",
    "# model = TinyModel(vocab_size=4, d_model=32, n_heads=4, n_layers=2)\n",
    "# opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "# for step in range(1000):\n",
    "#     x,y = make_copy_batch(B=32, T=16, vocab=4)\n",
    "#     logits = model(x)  # [B,T,vocab]\n",
    "#     loss = F.cross_entropy(logits[:,:-1].reshape(-1, 4), y[:,1:].reshape(-1))\n",
    "#     opt.zero_grad(); loss.backward(); opt.step()\n",
    "#     if step % 100 == 0:\n",
    "#         print(step, float(loss))"
   ],
   "id": "1054ce27b837c2cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 9) Debugging checklist\n",
    "\n",
    "- **Masks:** Verify that position *t* never attends to *t+1…* under a causal mask.\n",
    "- **Parity tests:** Match your PyTorch SDPA against `F.scaled_dot_product_attention` on random tensors and multiple sizes.\n",
    "- **MyMHA vs nn.MultiheadAttention:** After copying weights, outputs should match within a small tolerance.\n",
    "- **Training dynamics:** If loss doesn't drop, re-check normalization, mask broadcasting, and dimension reshapes/transposes."
   ],
   "id": "51713500605fa8fc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
