{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-17T16:32:38.774674Z",
     "start_time": "2025-02-17T16:32:34.873030Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset\n",
    "\n",
    "Every instance of the dataset for supervised learning consist of two parts:\n",
    "\n",
    "1. **Input features** are different values that are used to perform inference and make decisions. Initially these values might be represented by any data type, but before puting them into model, needs to be transformed into numerical formats.\n",
    "$$\n",
    "i \\in \\mathbb{R}^{\\text{\\# features}}\n",
    "$$\n",
    "\n",
    "2. **Targets** are the actual values that the model needs to predict, given the input.\n",
    "$$\n",
    "t \\in \\mathbb{R}^{\\text{\\# outputs}}\n",
    "$$\n",
    "\n",
    "For this assignment The single label classification dataset from Kaggle will be used."
   ],
   "id": "3fe62c5e6508dd34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:09:52.462172Z",
     "start_time": "2025-02-17T17:09:52.409256Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv(\"./data/drug200.csv.xls\")",
   "id": "3459486c6e90adc4",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:09:52.791339Z",
     "start_time": "2025-02-17T17:09:52.777731Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "214a4e1c4d40f5ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Age Sex      BP Cholesterol  Na_to_K   Drug\n",
       "0   23   F    HIGH        HIGH 25.35500  drugY\n",
       "1   47   M     LOW        HIGH 13.09300  drugC\n",
       "2   47   M     LOW        HIGH 10.11400  drugC\n",
       "3   28   F  NORMAL        HIGH  7.79800  drugX\n",
       "4   61   F     LOW        HIGH 18.04300  drugY"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>BP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Na_to_K</th>\n",
       "      <th>Drug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>25.35500</td>\n",
       "      <td>drugY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>M</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>13.09300</td>\n",
       "      <td>drugC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>M</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>10.11400</td>\n",
       "      <td>drugC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>F</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>7.79800</td>\n",
       "      <td>drugX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>F</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>18.04300</td>\n",
       "      <td>drugY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:10:02.045695Z",
     "start_time": "2025-02-17T17:10:01.966623Z"
    }
   },
   "cell_type": "code",
   "source": "df",
   "id": "52449703dc1676b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Age Sex      BP Cholesterol  Na_to_K   Drug\n",
       "0     23   F    HIGH        HIGH 25.35500  drugY\n",
       "1     47   M     LOW        HIGH 13.09300  drugC\n",
       "2     47   M     LOW        HIGH 10.11400  drugC\n",
       "3     28   F  NORMAL        HIGH  7.79800  drugX\n",
       "4     61   F     LOW        HIGH 18.04300  drugY\n",
       "..   ...  ..     ...         ...      ...    ...\n",
       "195   56   F     LOW        HIGH 11.56700  drugC\n",
       "196   16   M     LOW        HIGH 12.00600  drugC\n",
       "197   52   M  NORMAL        HIGH  9.89400  drugX\n",
       "198   23   M  NORMAL      NORMAL 14.02000  drugX\n",
       "199   40   F     LOW      NORMAL 11.34900  drugX\n",
       "\n",
       "[200 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>BP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Na_to_K</th>\n",
       "      <th>Drug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>25.35500</td>\n",
       "      <td>drugY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>M</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>13.09300</td>\n",
       "      <td>drugC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>M</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>10.11400</td>\n",
       "      <td>drugC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>F</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>7.79800</td>\n",
       "      <td>drugX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>F</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>18.04300</td>\n",
       "      <td>drugY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>56</td>\n",
       "      <td>F</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>11.56700</td>\n",
       "      <td>drugC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>12.00600</td>\n",
       "      <td>drugC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>52</td>\n",
       "      <td>M</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>9.89400</td>\n",
       "      <td>drugX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>14.02000</td>\n",
       "      <td>drugX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>40</td>\n",
       "      <td>F</td>\n",
       "      <td>LOW</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>11.34900</td>\n",
       "      <td>drugX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "In the most basic overview ANN consist of multiple layers with same structure. These layers are defined with the parameters weights matrix $W$ and bias vector $b$. The structure of each layer looks as follows:\n",
    "\n",
    "- **Fully connected** layer servers as main place of computation in the basic ANN.\n",
    "$$\n",
    "z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)}\n",
    "$$\n",
    "Where $W \\in \\mathbb{R}^{\\text{\\# inputs} \\times \\text{\\# outputs}}$ and $b \\in \\mathbb{R}^{\\text{\\# outputs}}$ are learnable.\n",
    "\n",
    "- **Activation function** is the next step after linear transformation. Activation function should be non-linear, since otherwise all the model will be just the composition of linear transformation, which is equivalent to single matrix multiplication, which is obviously not capable to cover some dependencies of the inputs.\n",
    "Here the ReLu function will be used:\n",
    "\n",
    "$$\n",
    "a = \\text{ReLU}(z)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(z) =\n",
    "\\begin{cases}\n",
    "z, & \\text{if } z > 0 \\\\\n",
    "0, & \\text{if } z \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **Output layer** of the model depends on the task, for classification, the last layer should have number of nodes equal to number of classes in the dataset.\n",
    "    After all the previous layers done, these nodes will take some raw numbers as the result of all previous computations, which are called logits.\n",
    "    Since the task is classification, we want to get the probability distribution of every class given input $P(x = c | input), \\quad \\text{for }c = 1,2, \\cdots, n $. The probability distribution should follow main rules:\n",
    "  The probability distribution should follow main rules:\n",
    "\n",
    "$$\n",
    "1. \\text{Non-negativity: } P(x) \\geq 0, \\quad \\forall x\n",
    "$$\n",
    "\n",
    "$$\n",
    "2. \\text{Normalization: } \\sum_x P(x) = 1 \\quad \\text{(discrete case)} \\quad \\text{or} \\quad \\int P(x) dx = 1 \\quad \\text{(continuous case)}\n",
    "$$\n",
    "\n",
    "In case of single label classification the softmax function is used to get the distribution from logits:\n",
    "$$\n",
    "\\text{softmax}(z_k) = \\frac{e^{z_k}}{\\sum_{i} e^{z_i}}\n",
    "$$\n",
    "\n",
    "For numerical stability, we also want to subtract $\\text{max}{(z)}$ before applying softmax.\n",
    "\n",
    "After all steps reviewed we could asemble the Model architecture, we will have 1 hidden layer and 3 output classes:\n",
    "\n",
    "<img src=\"math/architecture.jpg\" alt=\"Local Image\" width=\"1000\">\n"
   ],
   "id": "3b3b18096f36d52f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "\n",
    "The key idea how the model train in the supervised approach requires few concepts that will be covered in this section.\n",
    "The high level idea is to use actual target $y$ for given instance and predicted value $\\hat y$ and compute $Loss(y,\\hat y)$ some function that outputs measure of how good the model predicted this instance, then we could use calculus to compute how to change model parameters to minimize the Loss function."
   ],
   "id": "c1fe8521e0ddcb8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cross-Entropy\n",
    "Cross-Entropy is general idea that allows to numerically estimate how much two distributions align.\n",
    "\n",
    "To build intuition about this concept, let's introduce the idea of **surprise**:\n",
    "\n",
    "1. We experience **more surprise** when we sample a **rare event** and **less surprise** when an event has a **high probability**. This suggests that surprise is **inversely proportional to probability**.\n",
    "2. Additionally, if we sample a rare event **twice**, our surprise should **double**.\n",
    "\n",
    "#### Formalizing Surprise\n",
    "\n",
    "Suppose we define surprise as $s(x)$, and consider an event $x = a$ with probability:\n",
    "\n",
    "$$\n",
    "p(x = a) = 0.2\n",
    "$$\n",
    "\n",
    "If the surprise for this event is:\n",
    "\n",
    "$$\n",
    "s(x = a) = m\n",
    "$$\n",
    "\n",
    "Then, if we sample the same event twice:\n",
    "\n",
    "$$\n",
    "p(x = a \\cap x = a) = 0.2^2 = 0.04\n",
    "$$\n",
    "\n",
    "The corresponding surprise should double:\n",
    "\n",
    "$$\n",
    "s(x = a \\cap x = a) = 2m\n",
    "$$\n",
    "\n",
    "Since multiplication of probabilities corresponds to **addition of surprises**, a natural definition of surprise is:\n",
    "\n",
    "$$\n",
    "s(x) = \\log p(\\frac{1}{x})\n",
    "$$\n",
    "\n",
    "Once we have the idea of surprise defined, we could move to the Entropy.\n",
    "\n",
    "The value of Entropy might be seen as expected surprise measure of the distribution:\n",
    "\n",
    "$$\n",
    "H(P) = \\mathbb {E}[log(\\frac{1}{p})] = \\sum_i{p_i \\cdot \\log \\frac{1}{p_i}}\n",
    "$$\n",
    "\n",
    "Generally this value is not too big, since we kind of weight the surprise with probability and \"know what to expect\". When we know all the underlying distribution the computed surprise for every even is computed according to its actual probability.\n",
    "\n",
    "This concept alone is not so powerful, but it allows us to introduce the tool that measures how good some Distribution $Q$ estimates the average surprise, when sampling from Distribution P, this is called Cross-Entropy.\n",
    "\n",
    "## Cross-Entropy\n",
    "Cross-Entropy is the function of 2 Probability Distributions that quantifies the expectation of surprise, when observing the random process generated from $P$ but believing it comes from $Q$.\n",
    "\n",
    "$$\n",
    "H(P,Q) = \\sum_i{p_i \\cdot \\log \\frac{1}{q_i}}\n",
    "$$\n",
    "\n",
    "When we consider the $H(P,P)$ it naturally equals to just entropy of $P$:\n",
    "\n",
    "$$\n",
    "H(P,P) = \\sum_i{p_i \\cdot \\log \\frac{1}{p_i}} = H(P)\n",
    "$$\n",
    "\n",
    "The key property that I won't be proving here is that:\n",
    "\n",
    "$$\n",
    "\\forall Q,P : H(P) \\leq H(P,Q)\n",
    "$$\n",
    "\n",
    "For any mode lthe Cross-Entropy can never be lower than the Entropy of the underlying distribution.\n",
    "\n",
    "---\n",
    "### Additional Notes\n",
    "Also considering the topic of the Cross-Entropy I want to mention the idea of Kullbackâ€“Leibler Divergence, this idea allows to measure how much the $H(P,Q)$ differs from $H(P)$.\n",
    "\n",
    "$$\n",
    "H(P,Q) = \\sum_i{p_i \\cdot \\log \\frac{1}{q_i}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "H(P) = \\sum_i{p_i \\cdot \\log \\frac{1}{p_i}}\n",
    "$$\n",
    "\n",
    "Then the formula for KL Divergence looks as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{D}_{KL} &= H(P,Q) - H(P) = \\\\\n",
    "    &= \\sum_i{p_s(\\log \\frac{1}{q_i} - \\log \\frac{1}{p_i})} = \\\\\n",
    "    &= \\sum_i{p_s \\log \\frac{p_i}{q_i}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "However, for our case it is not useful since:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_{KL} = H(P,Q) - H(P)\n",
    "$$\n",
    "\n",
    "The P is underlaying distribution and it doesn't depend on the model parameters, so minimizing the $\\mathcal{D}_{KL}$ is equivalent to $H(P,Q)$.\n",
    "\n"
   ],
   "id": "ca01fbf7800177b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Complete Loss Function\n",
    "\n",
    "Now we have discussed all the concepts to set up the Loss Function for the training process, we could simply use the Cross-Entropy, but since we are dealing with single label classification, we could perform one step that will simplify future math derivations.\n",
    "\n",
    "Firstly lets denote concreate variables we are going to use:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "t &- \\text{encoded value of actual class} \\\\\n",
    "\\hat{y} &- \\text{vector of predicted probabilities for each class, where } \\hat{y}_i = \\text{probability of class } i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Using these notation the formula for Loss will look as follows:\n",
    "\n",
    "$$\n",
    "Loss(t, \\hat y) = \\sum_s p_s \\log {\\frac{1}{\\hat y_s}}\n",
    "$$\n",
    "\n",
    "In our case the $P$ is described with actual value $t$ in every sample, meaning we have $e_t(i) =\n",
    "\\begin{cases}\n",
    "1, & \\text{if } i = t \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$, where $t$ is actual ground-truth and all other items are $0$.\n",
    "\n",
    "So using this detailed the Loss function might be simplified to:\n",
    "\n",
    "$$\n",
    "Loss(t, \\hat y) = \\log {\\frac{1}{\\hat y_t}}\n",
    "$$\n",
    "\n",
    "To simplify future derivations, rewrite the log:\n",
    "\n",
    "$$\n",
    "Loss(t, \\hat y) = - \\log {\\hat y_t}\n",
    "$$\n",
    "\n",
    "This means for each train sample to loss  is just the nagative logarithm of the target class probability. This formula is called Negative Log Likelihood which is final loss function for training process(not considering the regularization terms)."
   ],
   "id": "3dd2423002350509"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# A bit of Math\n",
    "\n",
    "Now we could move to the derivation of model params in training process, the next step is to apply chain rule and find:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(l)}},\\frac{\\partial L}{\\partial b^{(l)}}, \\quad \\text{for each layer } l\n",
    "$$\n",
    "\n",
    "These partial derivatives shows how the change in the weight effect the value of loss, the gradient represents the vector of the fastest growth, since the optimization mostly consider minimization task, we will consider moving gradually changing the value of parameters in the opposite direction of the gradient."
   ],
   "id": "ff17eebc81962ee0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Chain Rule\n",
    "\n",
    "To find the value of weights and bias, we need:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^3} = \\frac{\\partial L}{\\partial \\hat y} \\cdot \\frac{\\partial \\hat y}{\\partial \\hat z} \\cdot \\frac{\\partial z}{\\partial a_3} \\cdot \\frac{\\partial a_3}{\\partial W_3}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^3} = \\frac{\\partial L}{\\partial \\hat y} \\cdot \\frac{\\partial \\hat y}{\\partial \\hat z} \\cdot \\frac{\\partial z}{\\partial a_3} \\cdot \\frac{\\partial a_3}{\\partial b_3}\n",
    "$$\n",
    "\n",
    "Firstly we want to compute value of $\\frac{\\partial L}{\\partial z}$ so then we could directly move towards counting the actual gradient of model Params.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial \\hat y} \\cdot \\frac{\\partial \\hat y}{\\partial \\hat z}\n",
    "$$"
   ],
   "id": "47446ca5a832cf6d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 1. $\\frac{\\partial L}{\\partial \\hat y}$\n",
    "\n",
    "let's consider variables and function that are used at this step:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&t - \\text{encoded value of actual class} \\\\\n",
    "&\\hat{y} - \\text{vector of predicted probabilities for each class, where } \\hat{y}_i = \\text{probability of class } i \\\\\n",
    "&\\text{Loss}(t, \\hat{y}) = -\\log \\hat{y}_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then it is clear that:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat y} = \\begin{cases}\n",
    "-\\frac{1}{\\hat y_i}, & \\text{if } i = t \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ],
   "id": "73db2c93c627ef5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 2. $\\frac{\\partial \\hat y}{\\partial z}$\n",
    "\n",
    "Now we need to find $\\frac{\\partial \\hat y}{\\partial z}$ how the result of softmax $\\hat y$ depends on the input $z$(logits).\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum e^{z_j}}, \\quad \\text{for } i = 1,2, \\dots, n\n",
    "$$\n",
    "\n",
    "where we consider two cases: $i = k$ and $i \\neq k$.\n",
    "\n",
    "Also lets denote the denominator as:\n",
    "$$\n",
    "D = \\sum e^{z_j}\n",
    "$$\n",
    "\n",
    "Then the softmax formula looks:\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{D}, \\quad \\text{for } i = 1,2, \\dots, n\n",
    "$$\n",
    "\n",
    "### 2.1 $ i = k$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\hat y_i}{\\partial z_i} &= \\frac{D \\cdot \\frac{\\partial}{\\partial z_i}e^{z_i} - e^{z_i}\\cdot \\frac{\\partial}{\\partial z_i}D}{D^2} = \\frac{D \\cdot e^{z_i} - (e^{z_i})^2}{D^2} = \\\\\n",
    "&= \\frac{e^{z_i} \\cdot (D - e^{z_i})}{D^2} = \\frac{e^{z_i}}{D} \\cdot \\frac{D - e^{z_i}}{D} = \\hat y_i \\cdot (1 - \\hat y_i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So for this case:\n",
    "$$\n",
    "\\frac{\\partial \\hat y_i}{\\partial z_i} = \\hat y_i \\cdot (1 - \\hat y_i)\n",
    "$$\n",
    "\n",
    "### 2.2 $i \\neq k$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\hat y_i}{\\partial z_k} &= \\frac{D \\cdot \\frac{\\partial}{\\partial z_k}e^{z_i} - e^{z_i}\\cdot \\frac{\\partial}{\\partial z_k}D}{D^2} = \\frac{- e^{z_i} \\cdot e^{z_k}}{D^2} = \\\\\n",
    "&= \\frac{- e^{z_i}}{D} \\cdot \\frac{e^{z_k}}{D} = - \\hat y_i \\cdot \\hat y_k\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For this case:\n",
    "$$\n",
    "\\frac{\\partial \\hat y_i}{\\partial z_k} = - \\hat y_i \\cdot \\hat y_k\n",
    "$$\n",
    "\n",
    "After covering these 2 cases, we could write the general formula for Jacobian of $\\frac{\\partial \\hat y_i}{\\partial z_k}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\frac{\\partial \\hat y_i}{\\partial z_k} = \\hat y_i \\cdot (\\delta_{ik} - \\hat y_k) \\\\\n",
    "&\\text{Where: } \\delta_{ik} - \\text{Krocker delta, meanining } \\begin{cases}\n",
    "1, & \\text{if } i = k \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "50328760a586164b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. $\\frac{\\partial L}{\\partial z}$\n",
    "\n",
    "Now the final step to finish the back propagation of Loss function is to chain $\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial\\hat y} \\cdot \\frac{\\partial \\hat y}{\\partial z}$\n",
    "\n",
    "to properly apply chain rule, will transpose ${\\frac{\\partial \\hat y}{\\partial \\hat y}}^T$ to get row-vector, then:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z} = {\\frac{\\partial \\hat y}{\\partial \\hat y}}^T \\cdot \\frac{\\partial \\hat y}{\\partial z}\n",
    "$$\n",
    "\n",
    "Then the operation looks:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z} = \\begin{bmatrix} \\frac{\\partial L}{\\partial \\hat y_1} & \\frac{\\partial L}{\\partial \\hat y_2} & \\cdots & \\frac{\\partial L}{\\partial \\hat y_n} \\end{bmatrix}  \\begin{bmatrix}\n",
    "\\frac{\\partial \\hat y_1}{\\partial \\hat z_1} & \\frac{\\partial \\hat y_1}{\\partial \\hat z_2} & \\cdots & \\frac{\\partial \\hat y_1}{\\partial \\hat z_n} \\\\\n",
    "\\frac{\\partial \\hat y_2}{\\partial \\hat z_1} & \\frac{\\partial \\hat y_2}{\\partial \\hat z_2} & \\cdots & \\frac{\\partial \\hat y_2}{\\partial \\hat z_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial \\hat y_n}{\\partial \\hat z_1} & \\frac{\\partial \\hat y_n}{\\partial \\hat z_2} & \\cdots & \\frac{\\partial \\hat y_n}{\\partial \\hat z_n} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "then:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i} = \\sum_j \\frac{\\partial L}{\\partial \\hat y_j} \\cdot \\frac{\\partial \\hat y_j}{\\partial a_i}\n",
    "$$\n",
    "\n",
    "Since $\\frac{\\partial L}{\\partial \\hat y_j} = 0$ for every value of $j$ except the target class $j = t$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i} = \\frac{\\partial L}{\\partial \\hat y_j} \\cdot \\frac{\\partial \\hat y_j}{\\partial a_i} = - \\frac{1}{\\hat y_t} \\cdot \\hat y_t (\\delta_{it} - \\hat y_i) = \\hat y_i - \\delta_{it}\n",
    "$$\n",
    "\n",
    "Now we are finally done with derivating the output layer and got the result $\\frac{\\partial L}{\\partial z} =  \\hat y - e_t$, this means that the gradient that is going to be used in the fully connected layers, that come from the Loss is just subtracting $1$ from the probability of target class and keeping others without changes. In the next section I will cover how to find $\\frac{\\partial L}{\\partial W}$ and  $\\frac{\\partial L}{\\partial b}$ within layer and how to pass the gradient to previous layer.\n",
    "\n"
   ],
   "id": "f935cc7f6d750649"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. $\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "We now move on to computing the gradients with respect to the parameters of the fully connected layer that produced the logits. Recall that the logits are obtained via a linear transformation:\n",
    "\n",
    "$$\n",
    "z = Wx + b\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $W$ is the weight matrix,\n",
    "- $b$ is the bias vector,\n",
    "- $x$ is the input vector to this layer.\n",
    "\n",
    "We have already derived that the gradient of the loss with respect to the logits is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z} = \\hat{y} - e_t\n",
    "$$\n",
    "\n",
    "Here, $\\hat{y}$ is the vector of predicted probabilities (the output of the softmax), and $e_t$ is the one-hot encoded vector of the true label.\n",
    "\n",
    "### Gradient with Respect to $W$\n",
    "\n",
    "Using the chain rule, the gradient of the loss with respect to $W$ is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W}\n",
    "$$\n",
    "\n",
    "Since the transformation is linear ($z = Wx + b$), the derivative of $z$ with respect to $W$ is straightforward. For each element $W_{ij}$ of the weight matrix, the corresponding element of $z$ satisfies:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_i}{\\partial W_{ij}} = x_j\n",
    "$$\n",
    "\n",
    "Thus, in matrix form, the full gradient with respect to $W$ is obtained by multiplying the column vector $\\frac{\\partial L}{\\partial z}$ by the row vector $x^T$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial z} x^T = (\\hat{y} - e_t) \\, x^T\n",
    "$$\n",
    "\n",
    "This expression shows that each element of the gradient matrix $\\frac{\\partial L}{\\partial W}$ is given by the product of the error term for that output and the corresponding input feature.\n",
    "\n",
    "### Gradient with Respect to $b$\n",
    "\n",
    "Similarly, for the bias vector $b$, we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial b}\n",
    "$$\n",
    "\n",
    "Because $z = Wx + b$ and the derivative of $z$ with respect to $b$ is 1 (for each component, since the bias is added directly), we obtain:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial z}\n",
    "$$\n"
   ],
   "id": "7e46f0aa2a8b154d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. $\\frac{\\partial L}{\\partial x}$\n",
    "\n",
    "Now, to propagate the gradient back to the previous layer, we need to compute the gradient of the loss with respect to the input $x$ of the current layer.\n",
    "\n",
    "Recall that the logits are computed as:\n",
    "$$\n",
    "z = Wx + b\n",
    "$$\n",
    "\n",
    "We have already derived that:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z} = \\hat{y} - e_t\n",
    "$$\n",
    "\n",
    "Using the chain rule, the gradient of the loss with respect to $x$ is given by:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial x}\n",
    "$$\n",
    "\n",
    "Since $z = Wx + b$, the derivative $\\frac{\\partial z}{\\partial x}$ is the weight matrix $W$. Taking dimensions into account, we obtain:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial z} W^T  = (\\hat{y} - e_t) W^T\n",
    "$$\n",
    "\n",
    "This gradient, $\\frac{\\partial L}{\\partial x}$, is then passed to the previous layer, allowing the network to propagate the error backward and update its parameters accordingly.\n"
   ],
   "id": "599c516e90d2b474"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "Now we finally have all the formulas and tools to train our model using Gradient Descent. We will now describe the process for a network with three layers. For simplicity, assume we have:\n",
    "\n",
    "- **Layer 1:** First (hidden) layer\n",
    "- **Layer 2:** Second (hidden) layer\n",
    "- **Layer 3:** Output layer\n",
    "\n",
    "---\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "The forward pass is executed as follows:\n",
    "\n",
    "1. **Layer 1:**\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   z^{(1)} &= W^{(1)} x + b^{(1)} \\\\\n",
    "   a^{(1)} &= f^{(1)}(z^{(1)})\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "   where $f^{(1)}$ is the activation function of Layer 1.\n",
    "\n",
    "2. **Layer 2:**\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   z^{(2)} &= W^{(2)} a^{(1)} + b^{(2)} \\\\\n",
    "   a^{(2)} &= f^{(2)}(z^{(2)})\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "   where $f^{(2)}$ is the activation function of Layer 2.\n",
    "\n",
    "3. **Layer 3 (Output Layer):**\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   z^{(3)} &= W^{(3)} a^{(2)} + b^{(3)} \\\\\n",
    "   \\hat{y} &= \\text{softmax}(z^{(3)})\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "   The predicted probabilities $\\hat{y}$ are obtained by applying the softmax function to the logits $z^{(3)}$.\n",
    "\n",
    "4. **Loss Computation:**\n",
    "   The loss is computed using the true target $t$:\n",
    "   $$\n",
    "   L = -\\log \\hat{y}_t\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### Backward Pass\n",
    "\n",
    "The backward pass involves computing gradients for each layer, starting from the output and propagating backward.\n",
    "\n",
    "1. **Output Layer (Layer 3):**\n",
    "\n",
    "   - **Gradient with respect to logits:**\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial z^{(3)}} = \\hat{y} - e_t\n",
    "     $$\n",
    "\n",
    "   - **Gradients for parameters:**\n",
    "     $$\n",
    "     \\begin{aligned}\n",
    "     \\frac{\\partial L}{\\partial W^{(3)}} &= \\frac{\\partial L}{\\partial z^{(3)}} \\, (a^{(2)})^T \\\\\n",
    "     \\frac{\\partial L}{\\partial b^{(3)}} &= \\frac{\\partial L}{\\partial z^{(3)}}\n",
    "     \\end{aligned}\n",
    "     $$\n",
    "\n",
    "   - **Gradient to propagate to Layer 2:**\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial a^{(2)}} = \\left(W^{(3)}\\right)^T \\frac{\\partial L}{\\partial z^{(3)}}\n",
    "     $$\n",
    "\n",
    "2. **Second Layer (Layer 2):**\n",
    "\n",
    "   - **Gradient with respect to pre-activation:**\n",
    "     Apply the derivative of the activation function $f^{(2)}$ (denoted by $f'^{(2)}$):\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial z^{(2)}} = \\frac{\\partial L}{\\partial a^{(2)}} \\odot f'^{(2)}(z^{(2)})\n",
    "     $$\n",
    "     where $\\odot$ denotes Hadamard Product multiplication.\n",
    "\n",
    "   - **Gradients for parameters:**\n",
    "     $$\n",
    "     \\begin{aligned}\n",
    "     \\frac{\\partial L}{\\partial W^{(2)}} &= \\frac{\\partial L}{\\partial z^{(2)}} \\, (a^{(1)})^T \\\\\n",
    "     \\frac{\\partial L}{\\partial b^{(2)}} &= \\frac{\\partial L}{\\partial z^{(2)}}\n",
    "     \\end{aligned}\n",
    "     $$\n",
    "\n",
    "   - **Gradient to propagate to Layer 1:**\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial a^{(1)}} = \\left(W^{(2)}\\right)^T \\frac{\\partial L}{\\partial z^{(2)}}\n",
    "     $$\n",
    "\n",
    "3. **First Layer (Layer 1):**\n",
    "\n",
    "   - **Gradient with respect to pre-activation:**\n",
    "     Similarly, apply the derivative of $f^{(1)}$:\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial z^{(1)}} = \\frac{\\partial L}{\\partial a^{(1)}} \\odot f'^{(1)}(z^{(1)})\n",
    "     $$\n",
    "\n",
    "   - **Gradients for parameters:**\n",
    "     $$\n",
    "     \\begin{aligned}\n",
    "     \\frac{\\partial L}{\\partial W^{(1)}} &= \\frac{\\partial L}{\\partial z^{(1)}} \\, x^T \\\\\n",
    "     \\frac{\\partial L}{\\partial b^{(1)}} &= \\frac{\\partial L}{\\partial z^{(1)}}\n",
    "     \\end{aligned}\n",
    "     $$\n",
    "\n",
    "---\n",
    "\n",
    "### Parameter Update\n",
    "\n",
    "After computing all the gradients, the network parameters are updated using Gradient Descent. For each layer $l$ (where $l = 1,2,3$):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W^{(l)} &\\leftarrow W^{(l)} - \\alpha \\, \\frac{\\partial L}{\\partial W^{(l)}} \\\\\n",
    "b^{(l)} &\\leftarrow b^{(l)} - \\alpha \\, \\frac{\\partial L}{\\partial b^{(l)}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where, $\\alpha$ is the learning rate.\n"
   ],
   "id": "364ed86c7f18b669"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Now lets finally do some coding)\n",
    "\n",
    "I will start with data preprocessing, I won't be doing data exploration here, since in the assignment it is already specified to use Artificial Neural Network and not other algorithms."
   ],
   "id": "1d5040f1c5a1d6b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:10:07.547191Z",
     "start_time": "2025-02-17T17:10:07.503573Z"
    }
   },
   "cell_type": "code",
   "source": "df",
   "id": "c4566440c081fb55",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Age Sex      BP Cholesterol  Na_to_K   Drug\n",
       "0     23   F    HIGH        HIGH 25.35500  drugY\n",
       "1     47   M     LOW        HIGH 13.09300  drugC\n",
       "2     47   M     LOW        HIGH 10.11400  drugC\n",
       "3     28   F  NORMAL        HIGH  7.79800  drugX\n",
       "4     61   F     LOW        HIGH 18.04300  drugY\n",
       "..   ...  ..     ...         ...      ...    ...\n",
       "195   56   F     LOW        HIGH 11.56700  drugC\n",
       "196   16   M     LOW        HIGH 12.00600  drugC\n",
       "197   52   M  NORMAL        HIGH  9.89400  drugX\n",
       "198   23   M  NORMAL      NORMAL 14.02000  drugX\n",
       "199   40   F     LOW      NORMAL 11.34900  drugX\n",
       "\n",
       "[200 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>BP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Na_to_K</th>\n",
       "      <th>Drug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>25.35500</td>\n",
       "      <td>drugY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>M</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>13.09300</td>\n",
       "      <td>drugC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>M</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>10.11400</td>\n",
       "      <td>drugC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>F</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>7.79800</td>\n",
       "      <td>drugX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>F</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>18.04300</td>\n",
       "      <td>drugY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>56</td>\n",
       "      <td>F</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>11.56700</td>\n",
       "      <td>drugC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>12.00600</td>\n",
       "      <td>drugC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>52</td>\n",
       "      <td>M</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>9.89400</td>\n",
       "      <td>drugX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>14.02000</td>\n",
       "      <td>drugX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>40</td>\n",
       "      <td>F</td>\n",
       "      <td>LOW</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>11.34900</td>\n",
       "      <td>drugX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Firstly will encode the string values and create mapping to use it in future.",
   "id": "3a5bb0353e545eb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:10:08.710203Z",
     "start_time": "2025-02-17T17:10:08.670580Z"
    }
   },
   "cell_type": "code",
   "source": "processed_df = df.copy()",
   "id": "93311c4596e1c99d",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:10:09.377845Z",
     "start_time": "2025-02-17T17:10:09.345046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mapping = {}\n",
    "\n",
    "for col in processed_df.columns:\n",
    "    if processed_df[col].dtype == 'object' or str(processed_df[col].dtype).startswith('category'):\n",
    "        processed_df[col] = processed_df[col].astype('category')\n",
    "        mapping[col] = dict(enumerate(processed_df[col].cat.categories))\n",
    "        processed_df[col] = processed_df[col].cat.codes\n",
    "\n",
    "for col, map_dict in mapping.items():\n",
    "    print(f\"Mapping for {col}:\\n{map_dict}\\n\")\n"
   ],
   "id": "22b277fdf17cdc72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping for Sex:\n",
      "{0: 'F', 1: 'M'}\n",
      "\n",
      "Mapping for BP:\n",
      "{0: 'HIGH', 1: 'LOW', 2: 'NORMAL'}\n",
      "\n",
      "Mapping for Cholesterol:\n",
      "{0: 'HIGH', 1: 'NORMAL'}\n",
      "\n",
      "Mapping for Drug:\n",
      "{0: 'drugA', 1: 'drugB', 2: 'drugC', 3: 'drugX', 4: 'drugY'}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:10:09.909149Z",
     "start_time": "2025-02-17T17:10:09.892488Z"
    }
   },
   "cell_type": "code",
   "source": "processed_df",
   "id": "77c45b0f20e0ff56",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Age  Sex  BP  Cholesterol  Na_to_K  Drug\n",
       "0     23    0   0            0 25.35500     4\n",
       "1     47    1   1            0 13.09300     2\n",
       "2     47    1   1            0 10.11400     2\n",
       "3     28    0   2            0  7.79800     3\n",
       "4     61    0   1            0 18.04300     4\n",
       "..   ...  ...  ..          ...      ...   ...\n",
       "195   56    0   1            0 11.56700     2\n",
       "196   16    1   1            0 12.00600     2\n",
       "197   52    1   2            0  9.89400     3\n",
       "198   23    1   2            1 14.02000     3\n",
       "199   40    0   1            1 11.34900     3\n",
       "\n",
       "[200 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>BP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Na_to_K</th>\n",
       "      <th>Drug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.35500</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.09300</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.11400</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7.79800</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18.04300</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.56700</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.00600</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9.89400</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14.02000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11.34900</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next step is to convert the dataframe into ndarray and init the data loader class",
   "id": "74a8698c12727237"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:10:12.331973Z",
     "start_time": "2025-02-17T17:10:12.310298Z"
    }
   },
   "cell_type": "code",
   "source": "data = processed_df.values",
   "id": "e0fdf4ea8818dafe",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:10:12.936054Z",
     "start_time": "2025-02-17T17:10:12.924227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "\n",
    "indices = np.arange(x.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "x,y = x[indices],y[indices]\n",
    "\n",
    "train_ratio = 0.8\n",
    "split_idx = int(x.shape[0] * train_ratio)\n",
    "\n",
    "x_train, x_test = x[:split_idx], x[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]"
   ],
   "id": "bd63717616a5ba2a",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:10:13.433032Z",
     "start_time": "2025-02-17T17:10:13.426606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, x, y, batch_size = 32):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples = x.shape[0]\n",
    "        self.indices = np.arange(x.shape[0])\n",
    "\n",
    "    def __iter__(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "        self.current_index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self. current_index>= self.num_samples:\n",
    "            raise StopIteration\n",
    "        batch_indices = self.indices[self.current_index: self.current_index + self.batch_size]\n",
    "        batch_x = self.x[batch_indices]\n",
    "        batch_y = self.y[batch_indices]\n",
    "        self.current_index += self.batch_size\n",
    "        return batch_x, batch_y"
   ],
   "id": "257ea1a0af1111d5",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:10:13.994783Z",
     "start_time": "2025-02-17T17:10:13.985237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(x_train, y_train, batch_size=32)\n",
    "test_loader = DataLoader(x_test, y_test, batch_size=32)"
   ],
   "id": "88794bc928a50c17",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:10:14.325356Z",
     "start_time": "2025-02-17T17:10:14.322570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for x,y in train_loader:\n",
    "    print(x.shape, y.shape)"
   ],
   "id": "241072874fef265d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 5) (32,)\n",
      "(32, 5) (32,)\n",
      "(32, 5) (32,)\n",
      "(32, 5) (32,)\n",
      "(32, 5) (32,)\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:11:20.582672Z",
     "start_time": "2025-02-17T17:11:20.461934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Model:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.w1 = np.random.randn(input_size, hidden_size).astype('float32') * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size)).astype('float32')\n",
    "        self.w2 = np.random.randn(hidden_size, hidden_size).astype('float32') * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, hidden_size)).astype('float32')\n",
    "        self.w3 = np.random.randn(hidden_size, output_size).astype('float32') * np.sqrt(2.0 / hidden_size)\n",
    "        self.b3 = np.zeros((1, output_size)).astype('float32')\n",
    "\n",
    "        self.x = None\n",
    "        self.z1 = None\n",
    "        self.z2 = None\n",
    "        self.z3 = None\n",
    "        self.a1 = None\n",
    "        self.a2 = None\n",
    "        self.a3 = None\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(z, 0)\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype('float32')\n",
    "\n",
    "    def softmax(self, z):\n",
    "        z = z - np.max(z, axis = 1, keepdims = True)\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis = 1, keepdims = True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "\n",
    "\n",
    "        self.z1 = np.dot(x, self.w1) + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "\n",
    "        self.z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        self.a2 = self.relu(self.z2)\n",
    "\n",
    "        self.z3 = np.dot(self.a2, self.w3) + self.b3\n",
    "        self.a3 = self.softmax(self.z3)\n",
    "\n",
    "        return self.a3\n",
    "\n",
    "    def compute_loss(self, y, y_hat):\n",
    "        n = y.shape[0]\n",
    "\n",
    "        eps = 1e-9\n",
    "\n",
    "        neg_log = - np.log(y_hat[np.arange(n), y.astype(int)] + eps)\n",
    "\n",
    "        return np.sum(neg_log) / n\n",
    "\n",
    "    def backward(self, y, y_hat):\n",
    "        n = y.shape[0]\n",
    "\n",
    "        y_onehot = np.zeros_like(y_hat)\n",
    "        y_onehot[np.arange(n),y.astype(int)] = 1\n",
    "\n",
    "        dz3 = y_hat - y_onehot\n",
    "        dw3 = np.dot(self.a2.T, dz3)\n",
    "        db3 = np.sum(dz3, axis = 0, keepdims = True)\n",
    "\n",
    "        da2 = np.dot(dz3, self.w3.T)\n",
    "        dz2 = da2 * self.relu_derivative(self.z2)\n",
    "        dw2 = np.dot(self.a1.T, dz2)\n",
    "        db2 = np.sum(dz2, axis = 0, keepdims = True)\n",
    "\n",
    "        da1 = np.dot(dz2, self.w2.T)\n",
    "        dz1 = da1 * self.relu_derivative(self.z1)\n",
    "        dw1 = np.dot(self.x.T, dz1)\n",
    "        db1 = np.sum(dz1, axis = 0, keepdims = True)\n",
    "\n",
    "        grads = {\n",
    "            'dw1': dw1,\n",
    "            'dw2': dw2,\n",
    "            'dw3': dw3,\n",
    "            'db1': db1,\n",
    "            'db2': db2,\n",
    "            'db3': db3\n",
    "        }\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def train(self, lr, epochs, data_loader):\n",
    "        loss_list = []\n",
    "        for epoch in range(epochs):\n",
    "            acc_loss = 0\n",
    "            for x, y in data_loader:\n",
    "                predictions = self.forward(x)\n",
    "\n",
    "                loss = self.compute_loss(y, predictions)\n",
    "\n",
    "                acc_loss += loss\n",
    "\n",
    "                grads = self.backward(y, predictions)\n",
    "\n",
    "                self.update_params(grads, lr)\n",
    "\n",
    "            print(f'epoch: {epoch}, loss: {loss}')\n",
    "            loss_list.append(loss)\n",
    "\n",
    "        plt.plot(loss_list)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    def update_params(self, grads, lr):\n",
    "        self.w1 -= lr * grads['dw1']\n",
    "        self.w2 -= lr * grads['dw2']\n",
    "        self.w3 -= lr * grads['dw3']\n",
    "        self.b1 -= lr * grads['db1']\n",
    "        self.b2 -= lr * grads['db2']\n",
    "        self.b3 -= lr * grads['db3']\n",
    "\n",
    "\n",
    "\n",
    "model = Model(5, 128, 5)\n",
    "model.train(lr=0.0001, epochs = 10, data_loader = train_loader)"
   ],
   "id": "50d5b7aa53f18e6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 15.513337833383233\n",
      "epoch: 1, loss: 1.220776140045593\n",
      "epoch: 2, loss: 0.9148288347517513\n",
      "epoch: 3, loss: 1.1379738876481555\n",
      "epoch: 4, loss: 1.1392946249999865\n",
      "epoch: 5, loss: 1.2906081544307202\n",
      "epoch: 6, loss: 1.3248725948525522\n",
      "epoch: 7, loss: 1.3090006997476202\n",
      "epoch: 8, loss: 0.8412142767266736\n",
      "epoch: 9, loss: 1.0787200960131926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:11:23.738234Z",
     "start_time": "2025-02-17T17:11:23.689816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "for x, y in test_loader:\n",
    "    y_hat = model.forward(x)\n",
    "    preds = np.argmax(y_hat, axis=1)\n",
    "    y = y.astype(int)\n",
    "    all_y_true.extend(y)\n",
    "    all_y_pred.extend(preds)\n",
    "\n",
    "all_y_true = np.array(all_y_true)\n",
    "all_y_pred = np.array(all_y_pred)\n",
    "\n",
    "accuracy = accuracy_score(all_y_true, all_y_pred)\n",
    "precision = precision_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(all_y_true, all_y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_y_true, all_y_pred, zero_division=0))\n",
    "\n"
   ],
   "id": "317c2582e3547f3a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics:\n",
      "Accuracy:  0.6500\n",
      "Precision: 0.4833\n",
      "Recall:    0.6500\n",
      "F1 Score:  0.5518\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0  0  0  0  6]\n",
      " [ 0  0  0  2  0]\n",
      " [ 0  0  0  0  2]\n",
      " [ 0  0  0  4  3]\n",
      " [ 0  0  0  1 22]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         6\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.57      0.57      0.57         7\n",
      "           4       0.67      0.96      0.79        23\n",
      "\n",
      "    accuracy                           0.65        40\n",
      "   macro avg       0.25      0.31      0.27        40\n",
      "weighted avg       0.48      0.65      0.55        40\n",
      "\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T16:35:51.965851Z",
     "start_time": "2025-02-17T16:35:51.960362Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "74566ff7b3905ef6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T16:35:52.041249Z",
     "start_time": "2025-02-17T16:35:52.038828Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "82b0d5546584b79d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "941bd8c2a64c1c4f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
