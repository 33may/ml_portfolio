{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-17T16:32:38.774674Z",
     "start_time": "2025-02-17T16:32:34.873030Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset\n",
    "\n",
    "Every instance of the dataset for supervised learning consist of two parts:\n",
    "\n",
    "1. **Input features** are different values that are used to perform inference and make decisions. Initially these values might be represented by any data type, but before puting them into model, needs to be transformed into numerical formats.\n",
    "$$\n",
    "i \\in \\mathbb{R}^{\\text{\\# features}}\n",
    "$$\n",
    "\n",
    "2. **Targets** are the actual values that the model needs to predict, given the input.\n",
    "$$\n",
    "t \\in \\mathbb{R}^{\\text{\\# outputs}}\n",
    "$$\n",
    "\n",
    "For this assignment The single label classification Iris dataset will be used."
   ],
   "id": "3fe62c5e6508dd34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:14:29.876676Z",
     "start_time": "2025-02-17T17:14:29.828492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target"
   ],
   "id": "3459486c6e90adc4",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:14:30.346912Z",
     "start_time": "2025-02-17T17:14:30.336770Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "214a4e1c4d40f5ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0            5.10000           3.50000            1.40000           0.20000   \n",
       "1            4.90000           3.00000            1.40000           0.20000   \n",
       "2            4.70000           3.20000            1.30000           0.20000   \n",
       "3            4.60000           3.10000            1.50000           0.20000   \n",
       "4            5.00000           3.60000            1.40000           0.20000   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.10000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>1.40000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.90000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>1.40000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.70000</td>\n",
       "      <td>3.20000</td>\n",
       "      <td>1.30000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.60000</td>\n",
       "      <td>3.10000</td>\n",
       "      <td>1.50000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.00000</td>\n",
       "      <td>3.60000</td>\n",
       "      <td>1.40000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:14:35.202514Z",
     "start_time": "2025-02-17T17:14:35.176373Z"
    }
   },
   "cell_type": "code",
   "source": "df",
   "id": "52449703dc1676b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0              5.10000           3.50000            1.40000           0.20000   \n",
       "1              4.90000           3.00000            1.40000           0.20000   \n",
       "2              4.70000           3.20000            1.30000           0.20000   \n",
       "3              4.60000           3.10000            1.50000           0.20000   \n",
       "4              5.00000           3.60000            1.40000           0.20000   \n",
       "..                 ...               ...                ...               ...   \n",
       "145            6.70000           3.00000            5.20000           2.30000   \n",
       "146            6.30000           2.50000            5.00000           1.90000   \n",
       "147            6.50000           3.00000            5.20000           2.00000   \n",
       "148            6.20000           3.40000            5.40000           2.30000   \n",
       "149            5.90000           3.00000            5.10000           1.80000   \n",
       "\n",
       "     target  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "..      ...  \n",
       "145       2  \n",
       "146       2  \n",
       "147       2  \n",
       "148       2  \n",
       "149       2  \n",
       "\n",
       "[150 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.10000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>1.40000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.90000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>1.40000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.70000</td>\n",
       "      <td>3.20000</td>\n",
       "      <td>1.30000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.60000</td>\n",
       "      <td>3.10000</td>\n",
       "      <td>1.50000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.00000</td>\n",
       "      <td>3.60000</td>\n",
       "      <td>1.40000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.70000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>5.20000</td>\n",
       "      <td>2.30000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.30000</td>\n",
       "      <td>2.50000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>1.90000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.50000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>5.20000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.20000</td>\n",
       "      <td>3.40000</td>\n",
       "      <td>5.40000</td>\n",
       "      <td>2.30000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.90000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>5.10000</td>\n",
       "      <td>1.80000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "In the most basic overview ANN consist of multiple layers with same structure. These layers are defined with the parameters weights matrix $W$ and bias vector $b$. The structure of each layer looks as follows:\n",
    "\n",
    "- **Fully connected** layer servers as main place of computation in the basic ANN.\n",
    "$$\n",
    "z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)}\n",
    "$$\n",
    "Where $W \\in \\mathbb{R}^{\\text{\\# inputs} \\times \\text{\\# outputs}}$ and $b \\in \\mathbb{R}^{\\text{\\# outputs}}$ are learnable.\n",
    "\n",
    "- **Activation function** is the next step after linear transformation. Activation function should be non-linear, since otherwise all the model will be just the composition of linear transformation, which is equivalent to single matrix multiplication, which is obviously not capable to cover some dependencies of the inputs.\n",
    "Here the ReLu function will be used:\n",
    "\n",
    "$$\n",
    "a = \\text{ReLU}(z)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(z) =\n",
    "\\begin{cases}\n",
    "z, & \\text{if } z > 0 \\\\\n",
    "0, & \\text{if } z \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **Output layer** of the model depends on the task, for classification, the last layer should have number of nodes equal to number of classes in the dataset.\n",
    "    After all the previous layers done, these nodes will take some raw numbers as the result of all previous computations, which are called logits.\n",
    "    Since the task is classification, we want to get the probability distribution of every class given input $P(x = c | input), \\quad \\text{for }c = 1,2, \\cdots, n $. The probability distribution should follow main rules:\n",
    "  The probability distribution should follow main rules:\n",
    "\n",
    "$$\n",
    "1. \\text{Non-negativity: } P(x) \\geq 0, \\quad \\forall x\n",
    "$$\n",
    "\n",
    "$$\n",
    "2. \\text{Normalization: } \\sum_x P(x) = 1 \\quad \\text{(discrete case)} \\quad \\text{or} \\quad \\int P(x) dx = 1 \\quad \\text{(continuous case)}\n",
    "$$\n",
    "\n",
    "In case of single label classification the softmax function is used to get the distribution from logits:\n",
    "$$\n",
    "\\text{softmax}(z_k) = \\frac{e^{z_k}}{\\sum_{i} e^{z_i}}\n",
    "$$\n",
    "\n",
    "For numerical stability, we also want to subtract $\\text{max}{(z)}$ before applying softmax.\n",
    "\n",
    "After all steps reviewed we could asemble the Model architecture, we will have 1 hidden layer and 3 output classes:\n",
    "\n",
    "<img src=\"math/architecture.jpg\" alt=\"Local Image\" width=\"1000\">\n"
   ],
   "id": "3b3b18096f36d52f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "\n",
    "The key idea how the model train in the supervised approach requires few concepts that will be covered in this section.\n",
    "The high level idea is to use actual target $y$ for given instance and predicted value $\\hat y$ and compute $Loss(y,\\hat y)$ some function that outputs measure of how good the model predicted this instance, then we could use calculus to compute how to change model parameters to minimize the Loss function."
   ],
   "id": "c1fe8521e0ddcb8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cross-Entropy\n",
    "Cross-Entropy is general idea that allows to numerically estimate how much two distributions align.\n",
    "\n",
    "To build intuition about this concept, let's introduce the idea of **surprise**:\n",
    "\n",
    "1. We experience **more surprise** when we sample a **rare event** and **less surprise** when an event has a **high probability**. This suggests that surprise is **inversely proportional to probability**.\n",
    "2. Additionally, if we sample a rare event **twice**, our surprise should **double**.\n",
    "\n",
    "#### Formalizing Surprise\n",
    "\n",
    "Suppose we define surprise as $s(x)$, and consider an event $x = a$ with probability:\n",
    "\n",
    "$$\n",
    "p(x = a) = 0.2\n",
    "$$\n",
    "\n",
    "If the surprise for this event is:\n",
    "\n",
    "$$\n",
    "s(x = a) = m\n",
    "$$\n",
    "\n",
    "Then, if we sample the same event twice:\n",
    "\n",
    "$$\n",
    "p(x = a \\cap x = a) = 0.2^2 = 0.04\n",
    "$$\n",
    "\n",
    "The corresponding surprise should double:\n",
    "\n",
    "$$\n",
    "s(x = a \\cap x = a) = 2m\n",
    "$$\n",
    "\n",
    "Since multiplication of probabilities corresponds to **addition of surprises**, a natural definition of surprise is:\n",
    "\n",
    "$$\n",
    "s(x) = \\log p(\\frac{1}{x})\n",
    "$$\n",
    "\n",
    "Once we have the idea of surprise defined, we could move to the Entropy.\n",
    "\n",
    "The value of Entropy might be seen as expected surprise measure of the distribution:\n",
    "\n",
    "$$\n",
    "H(P) = \\mathbb {E}[log(\\frac{1}{p})] = \\sum_i{p_i \\cdot \\log \\frac{1}{p_i}}\n",
    "$$\n",
    "\n",
    "Generally this value is not too big, since we kind of weight the surprise with probability and \"know what to expect\". When we know all the underlying distribution the computed surprise for every even is computed according to its actual probability.\n",
    "\n",
    "This concept alone is not so powerful, but it allows us to introduce the tool that measures how good some Distribution $Q$ estimates the average surprise, when sampling from Distribution P, this is called Cross-Entropy.\n",
    "\n",
    "## Cross-Entropy\n",
    "Cross-Entropy is the function of 2 Probability Distributions that quantifies the expectation of surprise, when observing the random process generated from $P$ but believing it comes from $Q$.\n",
    "\n",
    "$$\n",
    "H(P,Q) = \\sum_i{p_i \\cdot \\log \\frac{1}{q_i}}\n",
    "$$\n",
    "\n",
    "When we consider the $H(P,P)$ it naturally equals to just entropy of $P$:\n",
    "\n",
    "$$\n",
    "H(P,P) = \\sum_i{p_i \\cdot \\log \\frac{1}{p_i}} = H(P)\n",
    "$$\n",
    "\n",
    "The key property that I won't be proving here is that:\n",
    "\n",
    "$$\n",
    "\\forall Q,P : H(P) \\leq H(P,Q)\n",
    "$$\n",
    "\n",
    "For any mode lthe Cross-Entropy can never be lower than the Entropy of the underlying distribution.\n",
    "\n",
    "---\n",
    "### Additional Notes\n",
    "Also considering the topic of the Cross-Entropy I want to mention the idea of Kullback–Leibler Divergence, this idea allows to measure how much the $H(P,Q)$ differs from $H(P)$.\n",
    "\n",
    "$$\n",
    "H(P,Q) = \\sum_i{p_i \\cdot \\log \\frac{1}{q_i}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "H(P) = \\sum_i{p_i \\cdot \\log \\frac{1}{p_i}}\n",
    "$$\n",
    "\n",
    "Then the formula for KL Divergence looks as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{D}_{KL} &= H(P,Q) - H(P) = \\\\\n",
    "    &= \\sum_i{p_s(\\log \\frac{1}{q_i} - \\log \\frac{1}{p_i})} = \\\\\n",
    "    &= \\sum_i{p_s \\log \\frac{p_i}{q_i}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "However, for our case it is not useful since:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_{KL} = H(P,Q) - H(P)\n",
    "$$\n",
    "\n",
    "The P is underlaying distribution and it doesn't depend on the model parameters, so minimizing the $\\mathcal{D}_{KL}$ is equivalent to $H(P,Q)$.\n",
    "\n"
   ],
   "id": "ca01fbf7800177b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Complete Loss Function\n",
    "\n",
    "Now we have discussed all the concepts to set up the Loss Function for the training process, we could simply use the Cross-Entropy, but since we are dealing with single label classification, we could perform one step that will simplify future math derivations.\n",
    "\n",
    "Firstly lets denote concreate variables we are going to use:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "t &- \\text{encoded value of actual class} \\\\\n",
    "\\hat{y} &- \\text{vector of predicted probabilities for each class, where } \\hat{y}_i = \\text{probability of class } i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Using these notation the formula for Loss will look as follows:\n",
    "\n",
    "$$\n",
    "Loss(t, \\hat y) = \\sum_s p_s \\log {\\frac{1}{\\hat y_s}}\n",
    "$$\n",
    "\n",
    "In our case the $P$ is described with actual value $t$ in every sample, meaning we have $e_t(i) =\n",
    "\\begin{cases}\n",
    "1, & \\text{if } i = t \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$, where $t$ is actual ground-truth and all other items are $0$.\n",
    "\n",
    "So using this detailed the Loss function might be simplified to:\n",
    "\n",
    "$$\n",
    "Loss(t, \\hat y) = \\log {\\frac{1}{\\hat y_t}}\n",
    "$$\n",
    "\n",
    "To simplify future derivations, rewrite the log:\n",
    "\n",
    "$$\n",
    "Loss(t, \\hat y) = - \\log {\\hat y_t}\n",
    "$$\n",
    "\n",
    "This means for each train sample to loss  is just the nagative logarithm of the target class probability. This formula is called Negative Log Likelihood which is final loss function for training process(not considering the regularization terms)."
   ],
   "id": "3dd2423002350509"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# A bit of Math\n",
    "\n",
    "Now we could move to the derivation of model params in training process, the next step is to apply chain rule and find:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(l)}},\\frac{\\partial L}{\\partial b^{(l)}}, \\quad \\text{for each layer } l\n",
    "$$\n",
    "\n",
    "These partial derivatives shows how the change in the weight effect the value of loss, the gradient represents the vector of the fastest growth, since the optimization mostly consider minimization task, we will consider moving gradually changing the value of parameters in the opposite direction of the gradient."
   ],
   "id": "ff17eebc81962ee0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Chain Rule\n",
    "\n",
    "To find the value of weights and bias, we need:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^3} = \\frac{\\partial L}{\\partial \\hat y} \\cdot \\frac{\\partial \\hat y}{\\partial \\hat z} \\cdot \\frac{\\partial z}{\\partial a_3} \\cdot \\frac{\\partial a_3}{\\partial W_3}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^3} = \\frac{\\partial L}{\\partial \\hat y} \\cdot \\frac{\\partial \\hat y}{\\partial \\hat z} \\cdot \\frac{\\partial z}{\\partial a_3} \\cdot \\frac{\\partial a_3}{\\partial b_3}\n",
    "$$\n",
    "\n",
    "Firstly we want to compute value of $\\frac{\\partial L}{\\partial z}$ so then we could directly move towards counting the actual gradient of model Params.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial \\hat y} \\cdot \\frac{\\partial \\hat y}{\\partial \\hat z}\n",
    "$$"
   ],
   "id": "47446ca5a832cf6d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 1. $\\frac{\\partial L}{\\partial \\hat y}$\n",
    "\n",
    "let's consider variables and function that are used at this step:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&t - \\text{encoded value of actual class} \\\\\n",
    "&\\hat{y} - \\text{vector of predicted probabilities for each class, where } \\hat{y}_i = \\text{probability of class } i \\\\\n",
    "&\\text{Loss}(t, \\hat{y}) = -\\log \\hat{y}_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then it is clear that:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat y} = \\begin{cases}\n",
    "-\\frac{1}{\\hat y_i}, & \\text{if } i = t \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ],
   "id": "73db2c93c627ef5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 2. $\\frac{\\partial \\hat y}{\\partial z}$\n",
    "\n",
    "Now we need to find $\\frac{\\partial \\hat y}{\\partial z}$ how the result of softmax $\\hat y$ depends on the input $z$(logits).\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum e^{z_j}}, \\quad \\text{for } i = 1,2, \\dots, n\n",
    "$$\n",
    "\n",
    "where we consider two cases: $i = k$ and $i \\neq k$.\n",
    "\n",
    "Also lets denote the denominator as:\n",
    "$$\n",
    "D = \\sum e^{z_j}\n",
    "$$\n",
    "\n",
    "Then the softmax formula looks:\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{D}, \\quad \\text{for } i = 1,2, \\dots, n\n",
    "$$\n",
    "\n",
    "### 2.1 $ i = k$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\hat y_i}{\\partial z_i} &= \\frac{D \\cdot \\frac{\\partial}{\\partial z_i}e^{z_i} - e^{z_i}\\cdot \\frac{\\partial}{\\partial z_i}D}{D^2} = \\frac{D \\cdot e^{z_i} - (e^{z_i})^2}{D^2} = \\\\\n",
    "&= \\frac{e^{z_i} \\cdot (D - e^{z_i})}{D^2} = \\frac{e^{z_i}}{D} \\cdot \\frac{D - e^{z_i}}{D} = \\hat y_i \\cdot (1 - \\hat y_i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So for this case:\n",
    "$$\n",
    "\\frac{\\partial \\hat y_i}{\\partial z_i} = \\hat y_i \\cdot (1 - \\hat y_i)\n",
    "$$\n",
    "\n",
    "### 2.2 $i \\neq k$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\hat y_i}{\\partial z_k} &= \\frac{D \\cdot \\frac{\\partial}{\\partial z_k}e^{z_i} - e^{z_i}\\cdot \\frac{\\partial}{\\partial z_k}D}{D^2} = \\frac{- e^{z_i} \\cdot e^{z_k}}{D^2} = \\\\\n",
    "&= \\frac{- e^{z_i}}{D} \\cdot \\frac{e^{z_k}}{D} = - \\hat y_i \\cdot \\hat y_k\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For this case:\n",
    "$$\n",
    "\\frac{\\partial \\hat y_i}{\\partial z_k} = - \\hat y_i \\cdot \\hat y_k\n",
    "$$\n",
    "\n",
    "After covering these 2 cases, we could write the general formula for Jacobian of $\\frac{\\partial \\hat y_i}{\\partial z_k}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\frac{\\partial \\hat y_i}{\\partial z_k} = \\hat y_i \\cdot (\\delta_{ik} - \\hat y_k) \\\\\n",
    "&\\text{Where: } \\delta_{ik} - \\text{Krocker delta, meanining } \\begin{cases}\n",
    "1, & \\text{if } i = k \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "50328760a586164b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. $\\frac{\\partial L}{\\partial z}$\n",
    "\n",
    "Now the final step to finish the back propagation of Loss function is to chain $\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial\\hat y} \\cdot \\frac{\\partial \\hat y}{\\partial z}$\n",
    "\n",
    "to properly apply chain rule, will transpose ${\\frac{\\partial \\hat y}{\\partial \\hat y}}^T$ to get row-vector, then:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z} = {\\frac{\\partial \\hat y}{\\partial \\hat y}}^T \\cdot \\frac{\\partial \\hat y}{\\partial z}\n",
    "$$\n",
    "\n",
    "Then the operation looks:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z} = \\begin{bmatrix} \\frac{\\partial L}{\\partial \\hat y_1} & \\frac{\\partial L}{\\partial \\hat y_2} & \\cdots & \\frac{\\partial L}{\\partial \\hat y_n} \\end{bmatrix}  \\begin{bmatrix}\n",
    "\\frac{\\partial \\hat y_1}{\\partial \\hat z_1} & \\frac{\\partial \\hat y_1}{\\partial \\hat z_2} & \\cdots & \\frac{\\partial \\hat y_1}{\\partial \\hat z_n} \\\\\n",
    "\\frac{\\partial \\hat y_2}{\\partial \\hat z_1} & \\frac{\\partial \\hat y_2}{\\partial \\hat z_2} & \\cdots & \\frac{\\partial \\hat y_2}{\\partial \\hat z_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial \\hat y_n}{\\partial \\hat z_1} & \\frac{\\partial \\hat y_n}{\\partial \\hat z_2} & \\cdots & \\frac{\\partial \\hat y_n}{\\partial \\hat z_n} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "then:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i} = \\sum_j \\frac{\\partial L}{\\partial \\hat y_j} \\cdot \\frac{\\partial \\hat y_j}{\\partial a_i}\n",
    "$$\n",
    "\n",
    "Since $\\frac{\\partial L}{\\partial \\hat y_j} = 0$ for every value of $j$ except the target class $j = t$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i} = \\frac{\\partial L}{\\partial \\hat y_j} \\cdot \\frac{\\partial \\hat y_j}{\\partial a_i} = - \\frac{1}{\\hat y_t} \\cdot \\hat y_t (\\delta_{it} - \\hat y_i) = \\hat y_i - \\delta_{it}\n",
    "$$\n",
    "\n",
    "Now we are finally done with derivating the output layer and got the result $\\frac{\\partial L}{\\partial z} =  \\hat y - e_t$, this means that the gradient that is going to be used in the fully connected layers, that come from the Loss is just subtracting $1$ from the probability of target class and keeping others without changes. In the next section I will cover how to find $\\frac{\\partial L}{\\partial W}$ and  $\\frac{\\partial L}{\\partial b}$ within layer and how to pass the gradient to previous layer.\n",
    "\n"
   ],
   "id": "f935cc7f6d750649"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. $\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "We now move on to computing the gradients with respect to the parameters of the fully connected layer that produced the logits. Recall that the logits are obtained via a linear transformation:\n",
    "\n",
    "$$\n",
    "z = Wx + b\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $W$ is the weight matrix,\n",
    "- $b$ is the bias vector,\n",
    "- $x$ is the input vector to this layer.\n",
    "\n",
    "We have already derived that the gradient of the loss with respect to the logits is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z} = \\hat{y} - e_t\n",
    "$$\n",
    "\n",
    "Here, $\\hat{y}$ is the vector of predicted probabilities (the output of the softmax), and $e_t$ is the one-hot encoded vector of the true label.\n",
    "\n",
    "### Gradient with Respect to $W$\n",
    "\n",
    "Using the chain rule, the gradient of the loss with respect to $W$ is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W}\n",
    "$$\n",
    "\n",
    "Since the transformation is linear ($z = Wx + b$), the derivative of $z$ with respect to $W$ is straightforward. For each element $W_{ij}$ of the weight matrix, the corresponding element of $z$ satisfies:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_i}{\\partial W_{ij}} = x_j\n",
    "$$\n",
    "\n",
    "Thus, in matrix form, the full gradient with respect to $W$ is obtained by multiplying the column vector $\\frac{\\partial L}{\\partial z}$ by the row vector $x^T$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial z} x^T = (\\hat{y} - e_t) \\, x^T\n",
    "$$\n",
    "\n",
    "This expression shows that each element of the gradient matrix $\\frac{\\partial L}{\\partial W}$ is given by the product of the error term for that output and the corresponding input feature.\n",
    "\n",
    "### Gradient with Respect to $b$\n",
    "\n",
    "Similarly, for the bias vector $b$, we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial b}\n",
    "$$\n",
    "\n",
    "Because $z = Wx + b$ and the derivative of $z$ with respect to $b$ is 1 (for each component, since the bias is added directly), we obtain:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial z}\n",
    "$$\n"
   ],
   "id": "7e46f0aa2a8b154d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. $\\frac{\\partial L}{\\partial x}$\n",
    "\n",
    "Now, to propagate the gradient back to the previous layer, we need to compute the gradient of the loss with respect to the input $x$ of the current layer.\n",
    "\n",
    "Recall that the logits are computed as:\n",
    "$$\n",
    "z = Wx + b\n",
    "$$\n",
    "\n",
    "We have already derived that:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z} = \\hat{y} - e_t\n",
    "$$\n",
    "\n",
    "Using the chain rule, the gradient of the loss with respect to $x$ is given by:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial x}\n",
    "$$\n",
    "\n",
    "Since $z = Wx + b$, the derivative $\\frac{\\partial z}{\\partial x}$ is the weight matrix $W$. Taking dimensions into account, we obtain:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial z} W^T  = (\\hat{y} - e_t) W^T\n",
    "$$\n",
    "\n",
    "This gradient, $\\frac{\\partial L}{\\partial x}$, is then passed to the previous layer, allowing the network to propagate the error backward and update its parameters accordingly.\n"
   ],
   "id": "599c516e90d2b474"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "Now we finally have all the formulas and tools to train our model using Gradient Descent. We will now describe the process for a network with three layers. For simplicity, assume we have:\n",
    "\n",
    "- **Layer 1:** First (hidden) layer\n",
    "- **Layer 2:** Second (hidden) layer\n",
    "- **Layer 3:** Output layer\n",
    "\n",
    "---\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "The forward pass is executed as follows:\n",
    "\n",
    "1. **Layer 1:**\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   z^{(1)} &= W^{(1)} x + b^{(1)} \\\\\n",
    "   a^{(1)} &= f^{(1)}(z^{(1)})\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "   where $f^{(1)}$ is the activation function of Layer 1.\n",
    "\n",
    "2. **Layer 2:**\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   z^{(2)} &= W^{(2)} a^{(1)} + b^{(2)} \\\\\n",
    "   a^{(2)} &= f^{(2)}(z^{(2)})\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "   where $f^{(2)}$ is the activation function of Layer 2.\n",
    "\n",
    "3. **Layer 3 (Output Layer):**\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   z^{(3)} &= W^{(3)} a^{(2)} + b^{(3)} \\\\\n",
    "   \\hat{y} &= \\text{softmax}(z^{(3)})\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "   The predicted probabilities $\\hat{y}$ are obtained by applying the softmax function to the logits $z^{(3)}$.\n",
    "\n",
    "4. **Loss Computation:**\n",
    "   The loss is computed using the true target $t$:\n",
    "   $$\n",
    "   L = -\\log \\hat{y}_t\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### Backward Pass\n",
    "\n",
    "The backward pass involves computing gradients for each layer, starting from the output and propagating backward.\n",
    "\n",
    "1. **Output Layer (Layer 3):**\n",
    "\n",
    "   - **Gradient with respect to logits:**\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial z^{(3)}} = \\hat{y} - e_t\n",
    "     $$\n",
    "\n",
    "   - **Gradients for parameters:**\n",
    "     $$\n",
    "     \\begin{aligned}\n",
    "     \\frac{\\partial L}{\\partial W^{(3)}} &= \\frac{\\partial L}{\\partial z^{(3)}} \\, (a^{(2)})^T \\\\\n",
    "     \\frac{\\partial L}{\\partial b^{(3)}} &= \\frac{\\partial L}{\\partial z^{(3)}}\n",
    "     \\end{aligned}\n",
    "     $$\n",
    "\n",
    "   - **Gradient to propagate to Layer 2:**\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial a^{(2)}} = \\left(W^{(3)}\\right)^T \\frac{\\partial L}{\\partial z^{(3)}}\n",
    "     $$\n",
    "\n",
    "2. **Second Layer (Layer 2):**\n",
    "\n",
    "   - **Gradient with respect to pre-activation:**\n",
    "     Apply the derivative of the activation function $f^{(2)}$ (denoted by $f'^{(2)}$):\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial z^{(2)}} = \\frac{\\partial L}{\\partial a^{(2)}} \\odot f'^{(2)}(z^{(2)})\n",
    "     $$\n",
    "     where $\\odot$ denotes Hadamard Product multiplication.\n",
    "\n",
    "   - **Gradients for parameters:**\n",
    "     $$\n",
    "     \\begin{aligned}\n",
    "     \\frac{\\partial L}{\\partial W^{(2)}} &= \\frac{\\partial L}{\\partial z^{(2)}} \\, (a^{(1)})^T \\\\\n",
    "     \\frac{\\partial L}{\\partial b^{(2)}} &= \\frac{\\partial L}{\\partial z^{(2)}}\n",
    "     \\end{aligned}\n",
    "     $$\n",
    "\n",
    "   - **Gradient to propagate to Layer 1:**\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial a^{(1)}} = \\left(W^{(2)}\\right)^T \\frac{\\partial L}{\\partial z^{(2)}}\n",
    "     $$\n",
    "\n",
    "3. **First Layer (Layer 1):**\n",
    "\n",
    "   - **Gradient with respect to pre-activation:**\n",
    "     Similarly, apply the derivative of $f^{(1)}$:\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial z^{(1)}} = \\frac{\\partial L}{\\partial a^{(1)}} \\odot f'^{(1)}(z^{(1)})\n",
    "     $$\n",
    "\n",
    "   - **Gradients for parameters:**\n",
    "     $$\n",
    "     \\begin{aligned}\n",
    "     \\frac{\\partial L}{\\partial W^{(1)}} &= \\frac{\\partial L}{\\partial z^{(1)}} \\, x^T \\\\\n",
    "     \\frac{\\partial L}{\\partial b^{(1)}} &= \\frac{\\partial L}{\\partial z^{(1)}}\n",
    "     \\end{aligned}\n",
    "     $$\n",
    "\n",
    "---\n",
    "\n",
    "### Parameter Update\n",
    "\n",
    "After computing all the gradients, the network parameters are updated using Gradient Descent. For each layer $l$ (where $l = 1,2,3$):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W^{(l)} &\\leftarrow W^{(l)} - \\alpha \\, \\frac{\\partial L}{\\partial W^{(l)}} \\\\\n",
    "b^{(l)} &\\leftarrow b^{(l)} - \\alpha \\, \\frac{\\partial L}{\\partial b^{(l)}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where, $\\alpha$ is the learning rate.\n"
   ],
   "id": "364ed86c7f18b669"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Now lets finally do some coding)\n",
    "\n",
    "I will start with data preprocessing, I won't be doing data exploration here, since in the assignment it is already specified to use Artificial Neural Network and not other algorithms."
   ],
   "id": "1d5040f1c5a1d6b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:14:40.011224Z",
     "start_time": "2025-02-17T17:14:39.986726Z"
    }
   },
   "cell_type": "code",
   "source": "df",
   "id": "c4566440c081fb55",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0              5.10000           3.50000            1.40000           0.20000   \n",
       "1              4.90000           3.00000            1.40000           0.20000   \n",
       "2              4.70000           3.20000            1.30000           0.20000   \n",
       "3              4.60000           3.10000            1.50000           0.20000   \n",
       "4              5.00000           3.60000            1.40000           0.20000   \n",
       "..                 ...               ...                ...               ...   \n",
       "145            6.70000           3.00000            5.20000           2.30000   \n",
       "146            6.30000           2.50000            5.00000           1.90000   \n",
       "147            6.50000           3.00000            5.20000           2.00000   \n",
       "148            6.20000           3.40000            5.40000           2.30000   \n",
       "149            5.90000           3.00000            5.10000           1.80000   \n",
       "\n",
       "     target  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "..      ...  \n",
       "145       2  \n",
       "146       2  \n",
       "147       2  \n",
       "148       2  \n",
       "149       2  \n",
       "\n",
       "[150 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.10000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>1.40000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.90000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>1.40000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.70000</td>\n",
       "      <td>3.20000</td>\n",
       "      <td>1.30000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.60000</td>\n",
       "      <td>3.10000</td>\n",
       "      <td>1.50000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.00000</td>\n",
       "      <td>3.60000</td>\n",
       "      <td>1.40000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.70000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>5.20000</td>\n",
       "      <td>2.30000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.30000</td>\n",
       "      <td>2.50000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>1.90000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.50000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>5.20000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.20000</td>\n",
       "      <td>3.40000</td>\n",
       "      <td>5.40000</td>\n",
       "      <td>2.30000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.90000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>5.10000</td>\n",
       "      <td>1.80000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Firstly will encode the string values and create mapping to use it in future.",
   "id": "3a5bb0353e545eb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:14:42.427781Z",
     "start_time": "2025-02-17T17:14:42.406399Z"
    }
   },
   "cell_type": "code",
   "source": "processed_df = df.copy()",
   "id": "93311c4596e1c99d",
   "outputs": [],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:14:43.177557Z",
     "start_time": "2025-02-17T17:14:43.169810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mapping = {}\n",
    "\n",
    "for col in processed_df.columns:\n",
    "    if processed_df[col].dtype == 'object' or str(processed_df[col].dtype).startswith('category'):\n",
    "        processed_df[col] = processed_df[col].astype('category')\n",
    "        mapping[col] = dict(enumerate(processed_df[col].cat.categories))\n",
    "        processed_df[col] = processed_df[col].cat.codes\n",
    "\n",
    "for col, map_dict in mapping.items():\n",
    "    print(f\"Mapping for {col}:\\n{map_dict}\\n\")\n"
   ],
   "id": "22b277fdf17cdc72",
   "outputs": [],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:14:44.017028Z",
     "start_time": "2025-02-17T17:14:44.004090Z"
    }
   },
   "cell_type": "code",
   "source": "processed_df",
   "id": "77c45b0f20e0ff56",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0              5.10000           3.50000            1.40000           0.20000   \n",
       "1              4.90000           3.00000            1.40000           0.20000   \n",
       "2              4.70000           3.20000            1.30000           0.20000   \n",
       "3              4.60000           3.10000            1.50000           0.20000   \n",
       "4              5.00000           3.60000            1.40000           0.20000   \n",
       "..                 ...               ...                ...               ...   \n",
       "145            6.70000           3.00000            5.20000           2.30000   \n",
       "146            6.30000           2.50000            5.00000           1.90000   \n",
       "147            6.50000           3.00000            5.20000           2.00000   \n",
       "148            6.20000           3.40000            5.40000           2.30000   \n",
       "149            5.90000           3.00000            5.10000           1.80000   \n",
       "\n",
       "     target  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "..      ...  \n",
       "145       2  \n",
       "146       2  \n",
       "147       2  \n",
       "148       2  \n",
       "149       2  \n",
       "\n",
       "[150 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.10000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>1.40000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.90000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>1.40000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.70000</td>\n",
       "      <td>3.20000</td>\n",
       "      <td>1.30000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.60000</td>\n",
       "      <td>3.10000</td>\n",
       "      <td>1.50000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.00000</td>\n",
       "      <td>3.60000</td>\n",
       "      <td>1.40000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.70000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>5.20000</td>\n",
       "      <td>2.30000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.30000</td>\n",
       "      <td>2.50000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>1.90000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.50000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>5.20000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.20000</td>\n",
       "      <td>3.40000</td>\n",
       "      <td>5.40000</td>\n",
       "      <td>2.30000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.90000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>5.10000</td>\n",
       "      <td>1.80000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next step is to convert the dataframe into ndarray and init the data loader class",
   "id": "74a8698c12727237"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:14:45.662965Z",
     "start_time": "2025-02-17T17:14:45.648590Z"
    }
   },
   "cell_type": "code",
   "source": "data = processed_df.values",
   "id": "e0fdf4ea8818dafe",
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:14:46.319291Z",
     "start_time": "2025-02-17T17:14:46.311913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "\n",
    "indices = np.arange(x.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "x,y = x[indices],y[indices]\n",
    "\n",
    "train_ratio = 0.8\n",
    "split_idx = int(x.shape[0] * train_ratio)\n",
    "\n",
    "x_train, x_test = x[:split_idx], x[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]"
   ],
   "id": "bd63717616a5ba2a",
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:14:47.087896Z",
     "start_time": "2025-02-17T17:14:47.081694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, x, y, batch_size = 32):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples = x.shape[0]\n",
    "        self.indices = np.arange(x.shape[0])\n",
    "\n",
    "    def __iter__(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "        self.current_index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self. current_index>= self.num_samples:\n",
    "            raise StopIteration\n",
    "        batch_indices = self.indices[self.current_index: self.current_index + self.batch_size]\n",
    "        batch_x = self.x[batch_indices]\n",
    "        batch_y = self.y[batch_indices]\n",
    "        self.current_index += self.batch_size\n",
    "        return batch_x, batch_y"
   ],
   "id": "257ea1a0af1111d5",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:14:47.822982Z",
     "start_time": "2025-02-17T17:14:47.816779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(x_train, y_train, batch_size=32)\n",
    "test_loader = DataLoader(x_test, y_test, batch_size=32)"
   ],
   "id": "88794bc928a50c17",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Numpy Implementation\n",
    "\n",
    "Firstly The model is implemented in numpy, primary to test if all the formulas from math block are correct. The next section I will also implement the model using pyTorch."
   ],
   "id": "877968f1f65b6198"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:21:47.569841Z",
     "start_time": "2025-02-17T17:21:47.482785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Model:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.w1 = np.random.randn(input_size, hidden_size).astype('float32') * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size)).astype('float32')\n",
    "        self.w2 = np.random.randn(hidden_size, hidden_size).astype('float32') * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, hidden_size)).astype('float32')\n",
    "        self.w3 = np.random.randn(hidden_size, output_size).astype('float32') * np.sqrt(2.0 / hidden_size)\n",
    "        self.b3 = np.zeros((1, output_size)).astype('float32')\n",
    "\n",
    "        self.x = None\n",
    "        self.z1 = None\n",
    "        self.z2 = None\n",
    "        self.z3 = None\n",
    "        self.a1 = None\n",
    "        self.a2 = None\n",
    "        self.a3 = None\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(z, 0)\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype('float32')\n",
    "\n",
    "    def softmax(self, z):\n",
    "        z = z - np.max(z, axis = 1, keepdims = True)\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis = 1, keepdims = True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "\n",
    "\n",
    "        self.z1 = np.dot(x, self.w1) + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "\n",
    "        self.z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        self.a2 = self.relu(self.z2)\n",
    "\n",
    "        self.z3 = np.dot(self.a2, self.w3) + self.b3\n",
    "        self.a3 = self.softmax(self.z3)\n",
    "\n",
    "        return self.a3\n",
    "\n",
    "    def compute_loss(self, y, y_hat):\n",
    "        n = y.shape[0]\n",
    "\n",
    "        eps = 1e-9\n",
    "\n",
    "        neg_log = - np.log(y_hat[np.arange(n), y.astype(int)] + eps)\n",
    "\n",
    "        return np.sum(neg_log) / n\n",
    "\n",
    "    def backward(self, y, y_hat):\n",
    "        n = y.shape[0]\n",
    "\n",
    "        y_onehot = np.zeros_like(y_hat)\n",
    "        y_onehot[np.arange(n),y.astype(int)] = 1\n",
    "\n",
    "        dz3 = y_hat - y_onehot\n",
    "        dw3 = np.dot(self.a2.T, dz3)\n",
    "        db3 = np.sum(dz3, axis = 0, keepdims = True)\n",
    "\n",
    "        da2 = np.dot(dz3, self.w3.T)\n",
    "        dz2 = da2 * self.relu_derivative(self.z2)\n",
    "        dw2 = np.dot(self.a1.T, dz2)\n",
    "        db2 = np.sum(dz2, axis = 0, keepdims = True)\n",
    "\n",
    "        da1 = np.dot(dz2, self.w2.T)\n",
    "        dz1 = da1 * self.relu_derivative(self.z1)\n",
    "        dw1 = np.dot(self.x.T, dz1)\n",
    "        db1 = np.sum(dz1, axis = 0, keepdims = True)\n",
    "\n",
    "        grads = {\n",
    "            'dw1': dw1,\n",
    "            'dw2': dw2,\n",
    "            'dw3': dw3,\n",
    "            'db1': db1,\n",
    "            'db2': db2,\n",
    "            'db3': db3\n",
    "        }\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def train(self, lr, epochs, data_loader):\n",
    "        loss_list = []\n",
    "        for epoch in range(epochs):\n",
    "            acc_loss = 0\n",
    "            for x, y in data_loader:\n",
    "                predictions = self.forward(x)\n",
    "\n",
    "                loss = self.compute_loss(y, predictions)\n",
    "\n",
    "                acc_loss += loss\n",
    "\n",
    "                grads = self.backward(y, predictions)\n",
    "\n",
    "                self.update_params(grads, lr)\n",
    "\n",
    "            # print(f'epoch: {epoch}, loss: {loss}')\n",
    "            loss_list.append(loss)\n",
    "\n",
    "        plt.plot(loss_list)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    def update_params(self, grads, lr):\n",
    "        self.w1 -= lr * grads['dw1']\n",
    "        self.w2 -= lr * grads['dw2']\n",
    "        self.w3 -= lr * grads['dw3']\n",
    "        self.b1 -= lr * grads['db1']\n",
    "        self.b2 -= lr * grads['db2']\n",
    "        self.b3 -= lr * grads['db3']"
   ],
   "id": "46f41009efe75c15",
   "outputs": [],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:28:04.288216Z",
     "start_time": "2025-02-17T17:28:04.071943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "numpy_model = Model(4, 128, 3)\n",
    "numpy_model.train(lr=0.0001, epochs = 200, data_loader = train_loader)"
   ],
   "id": "50d5b7aa53f18e6e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "351ce3e1f71465c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:30:18.913565Z",
     "start_time": "2025-02-17T17:30:18.895336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "def eval_model(model, py_torch=False):\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    for x, y in test_loader:\n",
    "        if py_torch:\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "            y = torch.tensor(y, dtype=torch.long)\n",
    "        y_hat = model.forward(x)\n",
    "\n",
    "        if py_torch:\n",
    "            preds = torch.argmax(y_hat, dim=1).detach().cpu().numpy()\n",
    "            y = y.detach().cpu().numpy() if isinstance(y, torch.Tensor) else y\n",
    "        else:\n",
    "            preds = np.argmax(y_hat, axis=1)\n",
    "\n",
    "        y = np.array(y, dtype=int)\n",
    "        all_y_true.extend(y)\n",
    "        all_y_pred.extend(preds)\n",
    "\n",
    "    all_y_true = np.array(all_y_true)\n",
    "    all_y_pred = np.array(all_y_pred)\n",
    "\n",
    "    accuracy = accuracy_score(all_y_true, all_y_pred)\n",
    "    precision = precision_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(all_y_true, all_y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_y_true, all_y_pred, zero_division=0))\n"
   ],
   "id": "68cd7052b3dc374c",
   "outputs": [],
   "execution_count": 153
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:30:19.367082Z",
     "start_time": "2025-02-17T17:30:19.323717Z"
    }
   },
   "cell_type": "code",
   "source": "eval_model(numpy_model)",
   "id": "5fcc99808cb4823f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics:\n",
      "Accuracy:  0.9000\n",
      "Precision: 0.9250\n",
      "Recall:    0.9000\n",
      "F1 Score:  0.8983\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11  0  0]\n",
      " [ 0  7  3]\n",
      " [ 0  0  9]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       1.00      0.70      0.82        10\n",
      "           2       0.75      1.00      0.86         9\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.92      0.90      0.89        30\n",
      "weighted avg       0.93      0.90      0.90        30\n",
      "\n"
     ]
    }
   ],
   "execution_count": 154
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PyTorch Implementation",
   "id": "38e4610c50308b7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:30:21.855346Z",
     "start_time": "2025-02-17T17:30:21.848041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x) # return logits; use CrossEntropyLoss during training\n"
   ],
   "id": "941bd8c2a64c1c4f",
   "outputs": [],
   "execution_count": 155
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:32:21.148846Z",
     "start_time": "2025-02-17T17:32:20.732182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "pytorh_model = BasicModel(input_size=4, hidden_size=128, output_size=3)\n",
    "optimizer = optim.Adam(pytorh_model.parameters(), lr=0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x = torch.tensor(batch_x, dtype=torch.float32)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = pytorh_model(batch_x)\n",
    "        loss = loss_fn(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / (train_loader.num_samples // train_loader.batch_size)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n"
   ],
   "id": "59ab474ad141fa69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 1.4415\n",
      "Epoch 2/200, Loss: 1.4016\n",
      "Epoch 3/200, Loss: 1.3755\n",
      "Epoch 4/200, Loss: 1.3506\n",
      "Epoch 5/200, Loss: 1.3293\n",
      "Epoch 6/200, Loss: 1.3064\n",
      "Epoch 7/200, Loss: 1.2872\n",
      "Epoch 8/200, Loss: 1.2634\n",
      "Epoch 9/200, Loss: 1.2448\n",
      "Epoch 10/200, Loss: 1.2219\n",
      "Epoch 11/200, Loss: 1.2033\n",
      "Epoch 12/200, Loss: 1.1876\n",
      "Epoch 13/200, Loss: 1.1679\n",
      "Epoch 14/200, Loss: 1.1506\n",
      "Epoch 15/200, Loss: 1.1364\n",
      "Epoch 16/200, Loss: 1.1206\n",
      "Epoch 17/200, Loss: 1.1028\n",
      "Epoch 18/200, Loss: 1.0848\n",
      "Epoch 19/200, Loss: 1.0705\n",
      "Epoch 20/200, Loss: 1.0590\n",
      "Epoch 21/200, Loss: 1.0387\n",
      "Epoch 22/200, Loss: 1.0217\n",
      "Epoch 23/200, Loss: 1.0101\n",
      "Epoch 24/200, Loss: 0.9918\n",
      "Epoch 25/200, Loss: 0.9800\n",
      "Epoch 26/200, Loss: 0.9711\n",
      "Epoch 27/200, Loss: 0.9494\n",
      "Epoch 28/200, Loss: 0.9354\n",
      "Epoch 29/200, Loss: 0.9278\n",
      "Epoch 30/200, Loss: 0.9088\n",
      "Epoch 31/200, Loss: 0.8948\n",
      "Epoch 32/200, Loss: 0.8867\n",
      "Epoch 33/200, Loss: 0.8670\n",
      "Epoch 34/200, Loss: 0.8596\n",
      "Epoch 35/200, Loss: 0.8408\n",
      "Epoch 36/200, Loss: 0.8332\n",
      "Epoch 37/200, Loss: 0.8180\n",
      "Epoch 38/200, Loss: 0.8107\n",
      "Epoch 39/200, Loss: 0.7949\n",
      "Epoch 40/200, Loss: 0.7822\n",
      "Epoch 41/200, Loss: 0.7755\n",
      "Epoch 42/200, Loss: 0.7676\n",
      "Epoch 43/200, Loss: 0.7547\n",
      "Epoch 44/200, Loss: 0.7395\n",
      "Epoch 45/200, Loss: 0.7332\n",
      "Epoch 46/200, Loss: 0.7265\n",
      "Epoch 47/200, Loss: 0.7188\n",
      "Epoch 48/200, Loss: 0.7084\n",
      "Epoch 49/200, Loss: 0.6992\n",
      "Epoch 50/200, Loss: 0.6928\n",
      "Epoch 51/200, Loss: 0.6743\n",
      "Epoch 52/200, Loss: 0.6717\n",
      "Epoch 53/200, Loss: 0.6649\n",
      "Epoch 54/200, Loss: 0.6575\n",
      "Epoch 55/200, Loss: 0.6488\n",
      "Epoch 56/200, Loss: 0.6422\n",
      "Epoch 57/200, Loss: 0.6340\n",
      "Epoch 58/200, Loss: 0.6271\n",
      "Epoch 59/200, Loss: 0.6189\n",
      "Epoch 60/200, Loss: 0.6151\n",
      "Epoch 61/200, Loss: 0.6005\n",
      "Epoch 62/200, Loss: 0.5968\n",
      "Epoch 63/200, Loss: 0.5975\n",
      "Epoch 64/200, Loss: 0.5838\n",
      "Epoch 65/200, Loss: 0.5838\n",
      "Epoch 66/200, Loss: 0.5766\n",
      "Epoch 67/200, Loss: 0.5689\n",
      "Epoch 68/200, Loss: 0.5630\n",
      "Epoch 69/200, Loss: 0.5540\n",
      "Epoch 70/200, Loss: 0.5552\n",
      "Epoch 71/200, Loss: 0.5465\n",
      "Epoch 72/200, Loss: 0.5381\n",
      "Epoch 73/200, Loss: 0.5365\n",
      "Epoch 74/200, Loss: 0.5293\n",
      "Epoch 75/200, Loss: 0.5209\n",
      "Epoch 76/200, Loss: 0.5188\n",
      "Epoch 77/200, Loss: 0.5133\n",
      "Epoch 78/200, Loss: 0.5113\n",
      "Epoch 79/200, Loss: 0.5006\n",
      "Epoch 80/200, Loss: 0.5008\n",
      "Epoch 81/200, Loss: 0.4975\n",
      "Epoch 82/200, Loss: 0.4935\n",
      "Epoch 83/200, Loss: 0.4840\n",
      "Epoch 84/200, Loss: 0.4822\n",
      "Epoch 85/200, Loss: 0.4757\n",
      "Epoch 86/200, Loss: 0.4677\n",
      "Epoch 87/200, Loss: 0.4643\n",
      "Epoch 88/200, Loss: 0.4597\n",
      "Epoch 89/200, Loss: 0.4595\n",
      "Epoch 90/200, Loss: 0.4564\n",
      "Epoch 91/200, Loss: 0.4471\n",
      "Epoch 92/200, Loss: 0.4405\n",
      "Epoch 93/200, Loss: 0.4402\n",
      "Epoch 94/200, Loss: 0.4387\n",
      "Epoch 95/200, Loss: 0.4354\n",
      "Epoch 96/200, Loss: 0.4312\n",
      "Epoch 97/200, Loss: 0.4243\n",
      "Epoch 98/200, Loss: 0.4279\n",
      "Epoch 99/200, Loss: 0.4237\n",
      "Epoch 100/200, Loss: 0.4126\n",
      "Epoch 101/200, Loss: 0.4137\n",
      "Epoch 102/200, Loss: 0.4097\n",
      "Epoch 103/200, Loss: 0.4098\n",
      "Epoch 104/200, Loss: 0.3991\n",
      "Epoch 105/200, Loss: 0.3989\n",
      "Epoch 106/200, Loss: 0.3925\n",
      "Epoch 107/200, Loss: 0.3877\n",
      "Epoch 108/200, Loss: 0.3927\n",
      "Epoch 109/200, Loss: 0.3849\n",
      "Epoch 110/200, Loss: 0.3763\n",
      "Epoch 111/200, Loss: 0.3792\n",
      "Epoch 112/200, Loss: 0.3734\n",
      "Epoch 113/200, Loss: 0.3694\n",
      "Epoch 114/200, Loss: 0.3655\n",
      "Epoch 115/200, Loss: 0.3674\n",
      "Epoch 116/200, Loss: 0.3606\n",
      "Epoch 117/200, Loss: 0.3576\n",
      "Epoch 118/200, Loss: 0.3590\n",
      "Epoch 119/200, Loss: 0.3493\n",
      "Epoch 120/200, Loss: 0.3523\n",
      "Epoch 121/200, Loss: 0.3495\n",
      "Epoch 122/200, Loss: 0.3435\n",
      "Epoch 123/200, Loss: 0.3414\n",
      "Epoch 124/200, Loss: 0.3317\n",
      "Epoch 125/200, Loss: 0.3358\n",
      "Epoch 126/200, Loss: 0.3320\n",
      "Epoch 127/200, Loss: 0.3296\n",
      "Epoch 128/200, Loss: 0.3231\n",
      "Epoch 129/200, Loss: 0.3256\n",
      "Epoch 130/200, Loss: 0.3239\n",
      "Epoch 131/200, Loss: 0.3178\n",
      "Epoch 132/200, Loss: 0.3197\n",
      "Epoch 133/200, Loss: 0.3227\n",
      "Epoch 134/200, Loss: 0.3129\n",
      "Epoch 135/200, Loss: 0.3085\n",
      "Epoch 136/200, Loss: 0.3026\n",
      "Epoch 137/200, Loss: 0.3075\n",
      "Epoch 138/200, Loss: 0.3001\n",
      "Epoch 139/200, Loss: 0.2991\n",
      "Epoch 140/200, Loss: 0.2919\n",
      "Epoch 141/200, Loss: 0.2964\n",
      "Epoch 142/200, Loss: 0.2954\n",
      "Epoch 143/200, Loss: 0.2896\n",
      "Epoch 144/200, Loss: 0.2871\n",
      "Epoch 145/200, Loss: 0.2839\n",
      "Epoch 146/200, Loss: 0.2837\n",
      "Epoch 147/200, Loss: 0.2751\n",
      "Epoch 148/200, Loss: 0.2787\n",
      "Epoch 149/200, Loss: 0.2715\n",
      "Epoch 150/200, Loss: 0.2697\n",
      "Epoch 151/200, Loss: 0.2757\n",
      "Epoch 152/200, Loss: 0.2724\n",
      "Epoch 153/200, Loss: 0.2638\n",
      "Epoch 154/200, Loss: 0.2646\n",
      "Epoch 155/200, Loss: 0.2637\n",
      "Epoch 156/200, Loss: 0.2557\n",
      "Epoch 157/200, Loss: 0.2572\n",
      "Epoch 158/200, Loss: 0.2612\n",
      "Epoch 159/200, Loss: 0.2558\n",
      "Epoch 160/200, Loss: 0.2505\n",
      "Epoch 161/200, Loss: 0.2548\n",
      "Epoch 162/200, Loss: 0.2476\n",
      "Epoch 163/200, Loss: 0.2463\n",
      "Epoch 164/200, Loss: 0.2448\n",
      "Epoch 165/200, Loss: 0.2449\n",
      "Epoch 166/200, Loss: 0.2375\n",
      "Epoch 167/200, Loss: 0.2384\n",
      "Epoch 168/200, Loss: 0.2355\n",
      "Epoch 169/200, Loss: 0.2331\n",
      "Epoch 170/200, Loss: 0.2367\n",
      "Epoch 171/200, Loss: 0.2322\n",
      "Epoch 172/200, Loss: 0.2297\n",
      "Epoch 173/200, Loss: 0.2302\n",
      "Epoch 174/200, Loss: 0.2258\n",
      "Epoch 175/200, Loss: 0.2260\n",
      "Epoch 176/200, Loss: 0.2297\n",
      "Epoch 177/200, Loss: 0.2225\n",
      "Epoch 178/200, Loss: 0.2194\n",
      "Epoch 179/200, Loss: 0.2195\n",
      "Epoch 180/200, Loss: 0.2178\n",
      "Epoch 181/200, Loss: 0.2254\n",
      "Epoch 182/200, Loss: 0.2165\n",
      "Epoch 183/200, Loss: 0.2141\n",
      "Epoch 184/200, Loss: 0.2136\n",
      "Epoch 185/200, Loss: 0.2153\n",
      "Epoch 186/200, Loss: 0.2089\n",
      "Epoch 187/200, Loss: 0.2127\n",
      "Epoch 188/200, Loss: 0.2072\n",
      "Epoch 189/200, Loss: 0.2129\n",
      "Epoch 190/200, Loss: 0.2059\n",
      "Epoch 191/200, Loss: 0.2029\n",
      "Epoch 192/200, Loss: 0.2042\n",
      "Epoch 193/200, Loss: 0.2080\n",
      "Epoch 194/200, Loss: 0.2020\n",
      "Epoch 195/200, Loss: 0.1997\n",
      "Epoch 196/200, Loss: 0.1988\n",
      "Epoch 197/200, Loss: 0.2020\n",
      "Epoch 198/200, Loss: 0.1956\n",
      "Epoch 199/200, Loss: 0.2003\n",
      "Epoch 200/200, Loss: 0.1925\n"
     ]
    }
   ],
   "execution_count": 166
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T17:32:21.181127Z",
     "start_time": "2025-02-17T17:32:21.154808Z"
    }
   },
   "cell_type": "code",
   "source": "eval_model(pytorh_model, True)",
   "id": "bfd88cee3ca51139",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics:\n",
      "Accuracy:  0.9667\n",
      "Precision: 0.9700\n",
      "Recall:    0.9667\n",
      "F1 Score:  0.9667\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  0  9]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "execution_count": 167
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "\n",
    "In this assignment, the primary focus was on delving into the mathematical derivations that form the backbone of artificial neural networks (ANNs). Rather than showcasing an optimized training process or extensive exploratory analysis, my intent was to work through the fundamental principles—particularly matrix calculus—that underpin the operations of ANNs. Given the vast scope of ANN theory, it’s inevitable that some theoretical aspects may have been overlooked or could be expanded upon further. The foundation of ANNs is a rich and complex topic, and the theory can be integrated into many subtopics; however, for this assignment, I concentrated on the core mathematical derivations. This approach reflects my goal of deepening my understanding of the theoretical constructs behind neural networks, which is essential for advancing in ML engineering beyond simply applying pre-built frameworks to solve tasks."
   ],
   "id": "dda5f0345792d2dd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
