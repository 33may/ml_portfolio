{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T11:51:31.507624Z",
     "start_time": "2025-04-17T11:51:31.482984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from anyio import sleep\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "5cf96a5db390e0a6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lander\n",
    "\n",
    "Initially I had multiple test runs and notebooks that was used to understand the domain of the RL problems. I figured out after the first run that the agent for PushT doesnt train. I decided to test my algorithms on the easier env from gym, particularly LunarLander with continuous action space\n",
    "\n",
    "Luckily the training algorithm I have implemented DDPG doesnt requires any adjustments to be implemented on other envs."
   ],
   "id": "143c8cee1f02b632"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-17T15:03:48.808419Z",
     "start_time": "2025-04-17T15:03:48.778773Z"
    }
   },
   "source": [
    "from AIA.rl.lander.models import Agent\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "# Initialise the environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\", continuous=True)\n",
    "\n",
    "agent = Agent(alpha=1e-3, beta=1e-3, input_dims=8, tau=0.001,\n",
    "              batch_size=64, n_actions=2, noise=0.15, expert_data=None)\n",
    "\n",
    "#agent.load_models()\n",
    "np.random.seed(0)\n",
    "\n",
    "score_history = []"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-17T15:03:49.163314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(1000):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        act = agent.choose_action(obs)\n",
    "        new_state, reward, done, info, _ = env.step(act)\n",
    "        agent.remember(obs, act, reward, new_state, int(done))\n",
    "        agent.learn()\n",
    "        score += reward\n",
    "        obs = new_state\n",
    "        #env.render()\n",
    "    score_history.append(score)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print('episode ', i, 'score %.2f' % score,\n",
    "              'trailing 100 games avg %.3f' % np.mean(score_history[-100:]))"
   ],
   "id": "eb678c7e57367859",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score -285.74 trailing 100 games avg -285.743\n",
      "episode  50 score -575.93 trailing 100 games avg -720.878\n",
      "episode  100 score -250.23 trailing 100 games avg -503.169\n",
      "episode  150 score -561.45 trailing 100 games avg -245.664\n",
      "episode  200 score 193.58 trailing 100 games avg -157.381\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from AIA.rl.lander.plot import plotLearning\n",
    "\n",
    "plotLearning(score_history, window=100)"
   ],
   "id": "b4789741e548be50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Push T test 1\n",
    "\n",
    "After the test on lunar lander and small restructuring of algorith I tested again on the custom push T env"
   ],
   "id": "162441fa86dc5ec5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from AIA.rl.lander.plot import plotLearning\n",
    "from AIA.rl.lander.models import Agent\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from AIA.rl.lander.envs.pusht import PushTEnv\n",
    "\n",
    "\n",
    "# Initialise the environment\n",
    "env = PushTEnv(render_mode=\"rgb_array\")\n",
    "\n",
    "agent = Agent(alpha=1e-3, beta=1e-3, noise=25, input_dims=5, tau=0.001,\n",
    "              batch_size=64, n_actions=2)"
   ],
   "id": "c0f00638c40799da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#agent.load_models()\n",
    "np.random.seed(0)\n",
    "\n",
    "score_history = []"
   ],
   "id": "13d3723dba28ba2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(10000):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    prev_reward = 0\n",
    "    for t in range(400):\n",
    "        act = agent.choose_action(obs)\n",
    "        new_state, reward, done, info, _ = env.step(act)\n",
    "\n",
    "        prev_t_pos = obs[2:3]\n",
    "        new_t_pos = new_state[2:3]\n",
    "\n",
    "        d_move = np.sqrt(np.sum((prev_t_pos - new_t_pos)**2))\n",
    "\n",
    "        d_move_reward = d_move / 1000\n",
    "\n",
    "        buffer_reward = reward\n",
    "\n",
    "        reward -= prev_reward\n",
    "        reward += d_move_reward\n",
    "\n",
    "        prev_reward = buffer_reward\n",
    "\n",
    "        agent.remember(obs, act, reward, new_state, int(done))\n",
    "        agent.learn()\n",
    "        score += reward\n",
    "        obs = new_state\n",
    "        #env.render()\n",
    "    score_history.append(score)\n",
    "\n",
    "    #if i % 25 == 0:\n",
    "    #    agent.save_models()\n",
    "\n",
    "    print(f'episode , {i} score {(score):3f} trailing 100 games avg {np.mean(score_history[-100:]):3f}' )\n"
   ],
   "id": "cbed51e83b0c6e9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "env.close()",
   "id": "c916acfcf14a5b26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The agent doesnt learn, due to the high complexity of env.",
   "id": "25a5499333eb10eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Push T test 2\n",
    "\n",
    "Here I have setup the env to record the demonstrations of successful push T trajectories. Also I have done research on the questions how to solve the problem of sparse rewards. I have found the paper named HER which is promising approach, which I will test in future."
   ],
   "id": "11c341e3387a5181"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "import pickle\n",
    "from AIA.rl.lander.plot import plotLearning\n",
    "from AIA.rl.lander.models import Agent\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n"
   ],
   "id": "8f3b2a4de3d06f6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "env = PushTEnv(obs_type=\"state\", render_mode=\"rgb_array\")\n",
    "input_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.shape[0]\n",
    "\n",
    "    # load expert demonstrations saved as list of (s,a,r,s2,d)\n",
    "with open(\"demonstrations.pkl\", \"rb\") as f:\n",
    "    expert_transitions = pickle.load(f)\n",
    "\n",
    "agent = Agent(\n",
    "        alpha=2e-4, beta=2e-4,\n",
    "        input_dims=input_dim, n_actions=n_actions,\n",
    "        tau=0.001, gamma=0.99,\n",
    "        max_size=1_000_000, batch_size=256, noise=0.2,\n",
    "        expert_data=expert_transitions,\n",
    "        expert_ratio=0.25\n",
    ")"
   ],
   "id": "6c982d358f3453",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pre-Train from expert",
   "id": "c9583149627393bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def behaviour_clone(actor, demo, epochs=10, lr=1e-3, batch_size=256):\n",
    "    \"\"\"\n",
    "    Supervised pre‑training of the actor on expert (s→a) pairs\n",
    "    using mini‑batch SGD.\n",
    "    demo: list of (state, action, reward, next_state, done)\n",
    "    \"\"\"\n",
    "    device = actor.device\n",
    "    opt    = optim.Adam(actor.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    N = len(demo)\n",
    "\n",
    "    # pre‑stack everything once\n",
    "    all_states = torch.tensor([t[0] for t in demo],\n",
    "                              dtype=torch.float32, device=device)\n",
    "    all_actions = torch.tensor([t[1] for t in demo],\n",
    "                               dtype=torch.float32, device=device)\n",
    "\n",
    "    actor.train()\n",
    "    for ep in range(1, epochs+1):\n",
    "        perm = torch.randperm(N, device=device)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i in range(0, N, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            states = all_states[idx]\n",
    "            acts   = all_actions[idx]\n",
    "\n",
    "            pred = actor(states)\n",
    "            loss = loss_fn(pred, acts)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            epoch_loss += loss.item() * idx.size(0)\n",
    "\n",
    "        avg_loss = epoch_loss / N\n",
    "        if ep % max(1, epochs//10) == 0:\n",
    "            print(f\"[BC] Epoch {ep}/{epochs}, avg loss={avg_loss:.6f}\")\n",
    "\n",
    "    actor.eval()\n",
    "\n",
    "\n",
    "def pre_train_critic(critic, demo, gamma=0.99, epochs=10, batch_size=256, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Monte‑Carlo pre‑training of critic on expert episodes,\n",
    "    using mini‑batch SGD.\n",
    "    demo: list of (state, action, reward, next_state, done) in sequence order\n",
    "    \"\"\"\n",
    "    device = critic.device\n",
    "    opt    = optim.Adam(critic.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # 1) compute returns G_t\n",
    "    returns = []\n",
    "    G = 0.0\n",
    "    for (s,a,r,d,s2) in reversed(demo):\n",
    "        if d:\n",
    "            G = 0.0\n",
    "        G = r + gamma * G\n",
    "        returns.append(G)\n",
    "    returns = returns[::-1]\n",
    "\n",
    "    # 2) pre‑stack tensors\n",
    "    N = len(demo)\n",
    "    states  = torch.tensor([t[0] for t in demo],\n",
    "                           dtype=torch.float32, device=device)\n",
    "    actions = torch.tensor([t[1] for t in demo],\n",
    "                           dtype=torch.float32, device=device)\n",
    "    targets = torch.tensor(returns,\n",
    "                           dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "    critic.train()\n",
    "    for ep in range(1, epochs+1):\n",
    "        perm = torch.randperm(N, device=device)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i in range(0, N, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            s_batch = states[idx]\n",
    "            a_batch = actions[idx]\n",
    "            y_batch = targets[idx]\n",
    "\n",
    "            q_pred = critic(s_batch, a_batch)\n",
    "            loss   = loss_fn(q_pred, y_batch)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            epoch_loss += loss.item() * idx.size(0)\n",
    "\n",
    "        avg_loss = epoch_loss / N\n",
    "        print(f\"[Critic‑MC] Epoch {ep}/{epochs}, avg loss={avg_loss:.6f}\")\n",
    "\n",
    "    critic.eval()\n"
   ],
   "id": "424694aa70a5a8ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Pre‑train critic\n",
    "pre_train_critic(agent.critic,\n",
    "                 demo=expert_transitions,\n",
    "                 gamma=0.99,\n",
    "                 epochs=200,\n",
    "                 batch_size=256,\n",
    "                 lr=1e-4)"
   ],
   "id": "30d2abd8fb842fc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2) Pre‑train actor\n",
    "behaviour_clone(agent.actor,\n",
    "                demo=expert_transitions,\n",
    "                epochs=2000,\n",
    "                lr=1e-3)"
   ],
   "id": "1e8c565f89254284",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testy Pre-train results",
   "id": "eae77ce01e28978f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "env =  PushTEnv(obs_type=\"state\", render_mode=\"human\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "\n",
    "for i in range(200):\n",
    "\n",
    "    action = agent.choose_action(obs, eval=True)\n",
    "\n",
    "    nxt, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    obs = nxt\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ],
   "id": "15746f3a98e2be71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main Train",
   "id": "eb62b4fb64d1df23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "score_history = []",
   "id": "70fab3688c61be78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# env = PushTEnv(obs_type=\"state\", render_mode=\"rgb_array\")\n",
    "env = PushTEnv(obs_type=\"state\", render_mode=\"human\")"
   ],
   "id": "162a813bb9b12c60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for ep in range(1, 10001):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0.0\n",
    "    prev_reward = 0.0\n",
    "    for i in range(200):\n",
    "        env.render()\n",
    "        action = agent.choose_action(obs)\n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        prev_t_pos = obs[2:3]\n",
    "        new_t_pos = new_state[2:3]\n",
    "\n",
    "        d_move = np.sqrt(np.sum((prev_t_pos - new_t_pos)**2))\n",
    "\n",
    "        d_move_reward = d_move / 1000\n",
    "\n",
    "        buffer_reward = reward\n",
    "\n",
    "        reward -= prev_reward\n",
    "        reward += d_move_reward\n",
    "\n",
    "        prev_reward = buffer_reward\n",
    "\n",
    "        agent.memory.store_transition(obs, action, reward, new_state, done)\n",
    "        agent.learn()\n",
    "\n",
    "        obs = new_state\n",
    "        score += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    score_history.append(score)\n",
    "    if ep % 1 == 0:\n",
    "        avg = np.mean(score_history[-100:])\n",
    "        print(f\"Episode {ep:5d}  Score: {score:.2f}  100‑ep avg: {avg:.2f}\")"
   ],
   "id": "1183bc91ed2a58f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f0142a4b4cc61f2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "env.close()",
   "id": "a92f963d2b7f755",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a789cd30fc04c16a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
