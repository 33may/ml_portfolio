{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T18:12:24.953330Z",
     "start_time": "2025-04-17T18:12:24.937128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "6ae714e86569711e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RL Part 2\n",
    "\n",
    "In this part, I have learned a lot of new concepts and deepened my understanding of reinforcement learning through hands-on implementation. Initially, this was not the assignment I intended to submit. I began by attempting to recreate the DeepMind paper *\"Exploring Faster Matrix Multiplication Algorithms with Reinforcement Learning\"*.\n",
    "\n",
    "I managed to implement most of the core components, including:\n",
    "- the custom environment,\n",
    "- the neural network architecture (Torso, Value Head, Policy Head),\n",
    "- and the Monte Carlo Tree Search (MCTS) module, which served as the primary mechanism for exploring the state space in game-like environments.\n",
    "\n",
    "However, integrating all these components into a working system proved to be complex. The final step—putting everything together—required significant tuning and debugging, which made it infeasible to complete within the current timeframe. As a result, I decided to postpone this ambitious task and instead focus on completing a more classical RL example for the assignment.\n",
    "\n",
    "It is worth noting, though, that I have already grasped the key RL component of AlphaTensor, namely MCTS. The remaining challenge largely concerns architectural choices and making the model work reliably, which I plan to return to in the future.\n"
   ],
   "id": "63e2fa444b93e494"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Continuous Control in Reinforcement Learning\n",
    "\n",
    "In this notebook, I will use environments from the `gym` library, specifically the Lunar Lander, and a custom environment named `pushT`. The `pushT` environment is not part of the standard `gym` suite; I forked it from the Hugging Face repository and modified the vanilla version to incorporate additional functionalities, aligning it more closely with my task requirements.\n",
    "\n",
    "The primary difference between the reinforcement learning (RL) approaches covered in the AP semester and Part 1 lies in the exploration of agent operations within continuous action spaces.\n",
    "\n",
    "I will begin with stating my initial ideas and the rationale behind them, followed by a more formal definition of the tools and algorithms employed.\n",
    "\n",
    "To navigate the environment successfully, the agent interacts through actions. The policy is a function $a = \\pi(s)$ that takes the state as input and outputs actions. There are two ways to implement the policy:\n",
    "\n",
    "1. **Deterministic Policy**: The function returns a single action.\n",
    "2. **Stochastic Policy**: The function maps the state to a probability distribution over the action space.\n",
    "\n",
    "The stochastic approach resembles Q-learning, where we had probabilities of actions given a state. This method intuitively represents exploration by sampling from a modified probability distribution (e.g., Boltzmann). From a mathematical perspective, representing the policy as a probability distribution is natural. However, from a computer science point of view, deterministic policies are more practical due to limitations in representing some abstract mathematical concepts within a computer system, in this case probability distributions.\n",
    "\n",
    "Having committed to a deterministic policy, I have found the Deep Deterministic Policy Gradient (DDPG) algorithm, from the paper \"Continuous Control with Deep Reinforcement Learning\" (Lillicrap et al., 2016). The abstract of the paper described it as the solid foundation for my assignment:\n",
    "\n",
    "> \"We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving.\""
   ],
   "id": "c07eee3127e7b337"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "DDPG is an off-policy, model-free reinforcement learning algorithm tailored for environments with continuous action spaces. It use the deterministic policy gradient approach with strategies from Deep Q-Networks (DQN), such as experience replay and target networks, to enhance learning stability.\n",
    "\n",
    "### Understanding Off-Policy Learning Through Q-Function Abstraction\n",
    "\n",
    "During this assignment and exploring multiple different approaches, I have come to a deeper understanding of what off-policy learning truly entails—not just from the user perspective, but from the internal mechanics of the learning process. The key realization is that the policy can evolve over time, yet I can disregard the fact that actions were recorded under a different distribution because I utilize the Q-function. This function provides a level of abstraction that remains stable across the environment, ensuring that any optimal policy will converge to the same function, I have seen this in one of the papers called $Q_{\\pi_*}$ the actual Q function that stabilizes under the optimal policy.\n",
    "\n",
    "This abstraction allows the agent to be another function trained on this representation of the environment. If, instead, I had a training process without the Q-function—where the policy itself must encapsulate the knowledge of the environment—I would need to consider the sampling process more carefully. This is because, at the interpretative level, actions were taken based on a perception of the environment that changes as the policy evolves.\n",
    "\n",
    "This distinction highlights the essence of model-based versus model-free learning, and more specifically, the difference between off-policy and on-policy methods. In off-policy learning, like with Q-learning, the environment is not encoded within the model. The learning process can utilize data generated from a different policy than the one currently being improved. This separation allows for greater flexibility and stability, as the learning of the value function (Q-function) is decoupled from the behavior policy used to collect data.\n",
    "\n",
    "In contrast, on-policy methods require the policy being improved to be the same as the one used to collect data. This tight coupling necessitates that the policy must account for the environment's dynamics within itself, making the learning process more sensitive to changes in the policy and the environment.\n",
    "\n",
    "By leveraging the Q-function in an off-policy framework, I can maintain a stable representation of the environment's dynamics, allowing for more robust and flexible policy learning that is less sensitive to the specific data collection policy.\n",
    "\n",
    "\n",
    "### Core Components\n",
    "\n",
    "- **Actor Network**: Represents the deterministic policy $\\mu(s|\\theta^\\mu)$, mapping states to specific actions.\n",
    "\n",
    "- **Critic Network**: Estimates the action-value function $Q(s,a|\\theta^Q)$, evaluating the expected return of taking action $a$ in state $s$.\n",
    "\n",
    "- **Target Networks**: Time-delayed copies of the actor and critic networks, denoted as $\\mu'$ and $Q'$, respectively. These networks stabilize training by slowly tracking the learned networks.\n",
    "\n",
    "- **Replay Buffer**: A memory bank $\\mathcal{D}$ that stores past experiences $(s,a,r,s',d)$, enabling the reuse of data for learning and breaking correlations between sequential samples, that allows to form the independent batches and train nets in SGG way.\n",
    "\n",
    "### Algorithmic Steps\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Randomly initialize the actor and critic networks.\n",
    "   $$\n",
    "         \\mu(s|\\theta^\\mu)\n",
    "   $$\n",
    "   $$\n",
    "    Q(s,a|\\theta^Q)\n",
    "   $$\n",
    "   - Initialize target networks to match the actual:\n",
    "     $$\n",
    "     \\theta^{\\mu'} \\leftarrow \\theta^\\mu$, $\\theta^{Q'} \\leftarrow \\theta^Q\n",
    "     $$\n",
    "   - Initialize the replay buffer\n",
    "     $$\n",
    "     \\mathcal{D}\n",
    "     $$.\n",
    "\n",
    "2. **Interaction with Environment**:\n",
    "   - At each time step, select an action using the current policy with added exploration noise: $a_t = \\mu(s_t|\\theta^\\mu) + \\mathcal{N}_t$.\n",
    "   - Execute action $a_t$, observe reward $r_t$ and next state $s_{t+1}$.\n",
    "   - Store the transition $(s_t, a_t, r_t, s_{t+1})$ in $\\mathcal{D}$.\n",
    "\n",
    "3. **Learning**:\n",
    "   - Sample a minibatch of $N$ transitions $(s_i, a_i, r_i, s_{i+1}, d)$ from $\\mathcal{D}$.\n",
    "   - Compute target values:\n",
    "     $$\n",
    "     y_i = r_i + \\gamma Q'(s_{i+1}, \\mu'(s_{i+1}|\\theta^{\\mu'})|\\theta^{Q'})\n",
    "     $$\n",
    "   - Update the critic by minimizing the MSE loss:\n",
    "     $$\n",
    "     L = \\frac{1}{N} \\sum_{i=1}^{N} \\left( Q(s_i, a_i|\\theta^Q) - y_i \\right)^2\n",
    "     $$\n",
    "   - Update the actor using the sampled policy gradient:\n",
    "     $$\n",
    "     \\nabla_{\\theta^\\mu} J \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\nabla_a Q(s, a|\\theta^Q)|_{s=s_i, a=\\mu(s_i)} \\nabla_{\\theta^\\mu} \\mu(s|\\theta^\\mu)|_{s=s_i}\n",
    "     $$\n",
    "   - Update the target networks:\n",
    "     $$\n",
    "     \\theta^{Q'} \\leftarrow \\tau \\theta^Q + (1 - \\tau) \\theta^{Q'}\n",
    "     $$\n",
    "     $$\n",
    "     \\theta^{\\mu'} \\leftarrow \\tau \\theta^\\mu + (1 - \\tau) \\theta^{\\mu'}\n",
    "     $$\n",
    "     where $\\tau \\ll 1$ is the soft update parameter.\n",
    "\n",
    "\n",
    "\n",
    "This is the algorithm that I have used and implemented in this assignment, The actual code could be found in the `models.py`, the last thing that should be discussed to understand the first part is the Exploration.\n",
    "\n",
    "### Exploration Strategy\n",
    "\n",
    "Firstly I thought that since we have just vector and not the distribution over the actions, we will need some complicated way of adding the exploration, however it ended up being even easier, just adding noise to the action with mean of zero and reasonable variance is simple yet very efficient way to complete the given task.\n",
    "\n",
    "Many articles and tutorials, including DDPG use a common choice of the Ornstein-Uhlenbeck process, which generates temporally correlated noise suitable for physical control problems."
   ],
   "id": "bac4091aecb2888a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lander\n",
    "\n",
    "Initially I had multiple test runs and notebooks that was used to understand the domain of the RL problems. I figured out after the first run that the agent for PushT doesnt train. I decided to test my algorithms on the easier env from gym, particularly LunarLander with continuous action space\n",
    "\n",
    "Luckily the training algorithm I have implemented DDPG doesnt requires any adjustments to be implemented on other envs."
   ],
   "id": "fd38eb6a6bda88dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T18:12:26.556020Z",
     "start_time": "2025-04-17T18:12:25.345628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "from AIA.rl.lander.models import Agent\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\", continuous=True)\n",
    "\n",
    "agent = Agent(alpha=1e-3, beta=1e-3, input_dims=8, tau=0.001,\n",
    "              batch_size=256, n_actions=2, noise=0.15, expert_data=None)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "score_history = []"
   ],
   "id": "13644510894b5046",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T18:56:52.525459Z",
     "start_time": "2025-04-17T18:56:52.492308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(1000):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        act = agent.choose_action(obs)\n",
    "        new_state, reward, done, info, _ = env.step(act)\n",
    "        agent.remember(obs, act, reward, new_state, int(done))\n",
    "        agent.learn()\n",
    "        score += reward\n",
    "        obs = new_state\n",
    "        #env.render()\n",
    "    score_history.append(score)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(f'episode {i} | 100 eps moving avg : {np.mean(score_history[-100:]):.3f}')"
   ],
   "id": "d006fa6c04d19498",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 | 100 eps moving avg : -42.585\n",
      "episode 50 | 100 eps moving avg : -281.023\n",
      "episode 100 | 100 eps moving avg : -234.579\n",
      "episode 150 | 100 eps moving avg : -176.973\n",
      "episode 200 | 100 eps moving avg : -177.340\n",
      "episode 250 | 100 eps moving avg : -173.653\n",
      "episode 300 | 100 eps moving avg : -123.264\n",
      "episode 350 | 100 eps moving avg : -112.605\n",
      "episode 400 | 100 eps moving avg : -132.281\n",
      "episode 450 | 100 eps moving avg : -90.549\n",
      "episode 500 | 100 eps moving avg : -63.476\n",
      "episode 550 | 100 eps moving avg : -0.951\n",
      "episode 600 | 100 eps moving avg : 52.958\n",
      "episode 650 | 100 eps moving avg : 110.135\n",
      "episode 700 | 100 eps moving avg : 193.135\n",
      "episode 750 | 100 eps moving avg : 187.554\n",
      "episode 800 | 100 eps moving avg : 174.381\n",
      "episode 850 | 100 eps moving avg : 195.033\n",
      "episode 900 | 100 eps moving avg : 198.626\n",
      "episode 950 | 100 eps moving avg : 206.760\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from AIA.rl.lander.plot import plotLearning\n",
    "\n",
    "plotLearning(score_history, window=100)"
   ],
   "id": "83481f65408a2a07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Push T test 1\n",
    "\n",
    "After the test on lunar lander and small restructuring of algorith I have approved that algorithm works, so I decided to test again on the pushT to certainly conclude that the env is too complicated and some engeeniring tricks are needed to make it work"
   ],
   "id": "a6c0f33eb73aa6e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from AIA.rl.lander.models import Agent\n",
    "import numpy as np\n",
    "\n",
    "from AIA.rl.lander.envs.pusht import PushTEnv\n",
    "\n",
    "env = PushTEnv(render_mode=\"rgb_array\")\n",
    "\n",
    "agent = Agent(alpha=1e-3, beta=1e-3, noise=25, input_dims=5, tau=0.001,\n",
    "              batch_size=64, n_actions=2)"
   ],
   "id": "c41dc3151d2a3a00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "score_history = []",
   "id": "bb5863ff2596f1e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(10000):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    prev_reward = 0\n",
    "    for t in range(400):\n",
    "        act = agent.choose_action(obs)\n",
    "        new_state, reward, done, info, _ = env.step(act)\n",
    "\n",
    "        prev_t_pos = obs[2:3]\n",
    "        new_t_pos = new_state[2:3]\n",
    "\n",
    "        d_move = np.sqrt(np.sum((prev_t_pos - new_t_pos)**2))\n",
    "\n",
    "        d_move_reward = d_move / 1000\n",
    "\n",
    "        buffer_reward = reward\n",
    "\n",
    "        reward -= prev_reward\n",
    "        reward += d_move_reward\n",
    "\n",
    "        prev_reward = buffer_reward\n",
    "\n",
    "        agent.remember(obs, act, reward, new_state, int(done))\n",
    "        agent.learn()\n",
    "        score += reward\n",
    "        obs = new_state\n",
    "        #env.render()\n",
    "    score_history.append(score)\n",
    "\n",
    "    print(f'episode , {i} score {(score):3f} trailing 100 games avg {np.mean(score_history[-100:]):3f}' )\n"
   ],
   "id": "304d35ac93d079e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "env.close()",
   "id": "578e65e3a796ef6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The agent doesnt learn, due to the high complexity of env.",
   "id": "5e12ee690b0d2a75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Push T test 2\n",
    "\n",
    "Here I have setup the env to record the demonstrations of successful push T trajectories. Also I have done research on the questions how to solve the problem of sparse rewards. I have found the paper named HER which is promising approach, which I will test in future."
   ],
   "id": "cd23b79e45fc0dc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "import pickle\n",
    "from AIA.rl.lander.models import Agent\n",
    "import numpy as np\n"
   ],
   "id": "6b521422eb727ade",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "env = PushTEnv(obs_type=\"state\", render_mode=\"rgb_array\")\n",
    "input_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.shape[0]\n",
    "\n",
    "with open(\"demonstrations.pkl\", \"rb\") as f:\n",
    "    expert_transitions = pickle.load(f) # list of (s,a,r,s2,d)\n",
    "\n",
    "agent = Agent(\n",
    "        alpha=2e-4, beta=2e-4,\n",
    "        input_dims=input_dim, n_actions=n_actions,\n",
    "        tau=0.001, gamma=0.99,\n",
    "        max_size=1_000_000, batch_size=256, noise=0.2,\n",
    "        expert_data=expert_transitions,\n",
    "        expert_ratio=0.25\n",
    ")"
   ],
   "id": "933320379a2ba675",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pre-Train from expert",
   "id": "22075790500d573"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def behaviour_clone(actor, demo, epochs=10, lr=1e-3, batch_size=256):\n",
    "    \"\"\"\n",
    "    Supervised pre‑training of the actor on expert (s→a) pairs\n",
    "    using mini‑batch SGD.\n",
    "    demo: list of (state, action, reward, next_state, done)\n",
    "    \"\"\"\n",
    "    device = actor.device\n",
    "    opt    = optim.Adam(actor.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    N = len(demo)\n",
    "\n",
    "    # pre‑stack everything once\n",
    "    all_states = torch.tensor([t[0] for t in demo],\n",
    "                              dtype=torch.float32, device=device)\n",
    "    all_actions = torch.tensor([t[1] for t in demo],\n",
    "                               dtype=torch.float32, device=device)\n",
    "\n",
    "    actor.train()\n",
    "    for ep in range(1, epochs+1):\n",
    "        perm = torch.randperm(N, device=device)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i in range(0, N, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            states = all_states[idx]\n",
    "            acts   = all_actions[idx]\n",
    "\n",
    "            pred = actor(states)\n",
    "            loss = loss_fn(pred, acts)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            epoch_loss += loss.item() * idx.size(0)\n",
    "\n",
    "        avg_loss = epoch_loss / N\n",
    "        if ep % max(1, epochs//10) == 0:\n",
    "            print(f\"[BC] Epoch {ep}/{epochs}, avg loss={avg_loss:.6f}\")\n",
    "\n",
    "    actor.eval()\n",
    "\n",
    "\n",
    "def pre_train_critic(critic, demo, gamma=0.99, epochs=10, batch_size=256, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Monte‑Carlo pre‑training of critic on expert episodes,\n",
    "    using mini‑batch SGD.\n",
    "    demo: list of (state, action, reward, next_state, done) in sequence order\n",
    "    \"\"\"\n",
    "    device = critic.device\n",
    "    opt    = optim.Adam(critic.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # 1) compute returns G_t\n",
    "    returns = []\n",
    "    G = 0.0\n",
    "    for (s,a,r,d,s2) in reversed(demo):\n",
    "        if d:\n",
    "            G = 0.0\n",
    "        G = r + gamma * G\n",
    "        returns.append(G)\n",
    "    returns = returns[::-1]\n",
    "\n",
    "    # 2) pre‑stack tensors\n",
    "    N = len(demo)\n",
    "    states  = torch.tensor([t[0] for t in demo],\n",
    "                           dtype=torch.float32, device=device)\n",
    "    actions = torch.tensor([t[1] for t in demo],\n",
    "                           dtype=torch.float32, device=device)\n",
    "    targets = torch.tensor(returns,\n",
    "                           dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "    critic.train()\n",
    "    for ep in range(1, epochs+1):\n",
    "        perm = torch.randperm(N, device=device)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i in range(0, N, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            s_batch = states[idx]\n",
    "            a_batch = actions[idx]\n",
    "            y_batch = targets[idx]\n",
    "\n",
    "            q_pred = critic(s_batch, a_batch)\n",
    "            loss   = loss_fn(q_pred, y_batch)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            epoch_loss += loss.item() * idx.size(0)\n",
    "\n",
    "        avg_loss = epoch_loss / N\n",
    "        print(f\"[Critic‑MC] Epoch {ep}/{epochs}, avg loss={avg_loss:.6f}\")\n",
    "\n",
    "    critic.eval()\n"
   ],
   "id": "ba5b9f610985dd1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Pre‑train critic\n",
    "pre_train_critic(agent.critic,\n",
    "                 demo=expert_transitions,\n",
    "                 gamma=0.99,\n",
    "                 epochs=200,\n",
    "                 batch_size=256,\n",
    "                 lr=1e-4)"
   ],
   "id": "da7cec35d58b2c26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2) Pre‑train actor\n",
    "behaviour_clone(agent.actor,\n",
    "                demo=expert_transitions,\n",
    "                epochs=2000,\n",
    "                lr=1e-3)"
   ],
   "id": "89cefb9c33ec8063",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testy Pre-train results",
   "id": "948f338c319d953f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "env =  PushTEnv(obs_type=\"state\", render_mode=\"human\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "\n",
    "for i in range(200):\n",
    "\n",
    "    action = agent.choose_action(obs, eval=True)\n",
    "\n",
    "    nxt, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    obs = nxt\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ],
   "id": "cee428b78aa11d4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main Train",
   "id": "5e2f38e25bd26ec2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "score_history = []",
   "id": "1960d2563fb50ca6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# env = PushTEnv(obs_type=\"state\", render_mode=\"rgb_array\")\n",
    "env = PushTEnv(obs_type=\"state\", render_mode=\"human\")"
   ],
   "id": "545d4bdf38e79029",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for ep in range(1, 10001):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0.0\n",
    "    prev_reward = 0.0\n",
    "    for i in range(200):\n",
    "        env.render()\n",
    "        action = agent.choose_action(obs)\n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        prev_t_pos = obs[2:3]\n",
    "        new_t_pos = new_state[2:3]\n",
    "\n",
    "        d_move = np.sqrt(np.sum((prev_t_pos - new_t_pos)**2))\n",
    "\n",
    "        d_move_reward = d_move / 1000\n",
    "\n",
    "        buffer_reward = reward\n",
    "\n",
    "        reward -= prev_reward\n",
    "        reward += d_move_reward\n",
    "\n",
    "        prev_reward = buffer_reward\n",
    "\n",
    "        agent.memory.store_transition(obs, action, reward, new_state, done)\n",
    "        agent.learn()\n",
    "\n",
    "        obs = new_state\n",
    "        score += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    score_history.append(score)\n",
    "    if ep % 1 == 0:\n",
    "        avg = np.mean(score_history[-100:])\n",
    "        print(f\"Episode {ep:5d}  Score: {score:.2f}  100‑ep avg: {avg:.2f}\")"
   ],
   "id": "6696177cd51ba250",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "eed9f310aad0762f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "env.close()",
   "id": "a5e4a14e4347086c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1821c4a7a70a77a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The basic methods didnt help with push T, This is because it is way more complicated, the agent is controlling itself and need to interact with the T, This includes understanding and transforming the state vector to one that will help to navigate and locate T, plan tasks to understand where to push the T, model the underlying physics. This requires more complicated approaches. I feel sad that I didnt start with this pushT assignment from the beggining, if that was I have succesffully demonstarated how I could solve the problems in this notebook. Definetely these tasks goes for the nearrest future, because I have found the task that is closely related to robotics tasks, also I have done a job to modify the env to better fit my future goals. And also I have started to collect the dataset of the expert demonstarations, that are going to be used in my research on Immitation learning and diffusion policy from expert demonstarations.\n",
    "\n",
    "Also one of the approaches I have encountered is called her, I didnt have time to implement and test it here but I woild like to discuss it later. Its goal is to solve the rl problem of sparse rewards, by giving the agent chance to learn even on failure episodes, by pretending the goal was at the place where the episode ended.\n",
    "\n",
    "I appreciete this assignment a lot, the field of rl amuse me with variety of tricks approaches and algorithms that engeniers discover to solve tasks."
   ],
   "id": "b58a376137139c56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1107fa0a46526a05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6daea9132965654b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
