{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T11:51:31.507624Z",
     "start_time": "2025-04-17T11:51:31.482984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from anyio import sleep\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "5cf96a5db390e0a6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lander\n",
    "\n",
    "Initially I had multiple test runs and notebooks that was used to understand the domain of the RL problems. I figured out after the first run that the agent for PushT doesnt train. I decided to test my algorithms on the easier env from gym, particularly LunarLander with continuous action space\n",
    "\n",
    "Luckily the training algorithm I have implemented DDPG doesnt requires any adjustments to be implemented on other envs."
   ],
   "id": "143c8cee1f02b632"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-17T13:03:16.675170Z",
     "start_time": "2025-04-17T13:03:16.651885Z"
    }
   },
   "source": [
    "from AIA.rl.lander.models import Agent\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "# Initialise the environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\", continuous=True)\n",
    "\n",
    "agent = Agent(alpha=1e-3, beta=1e-3, input_dims=8, tau=0.001,\n",
    "              batch_size=64, n_actions=2, noise=0.15, expert_data=None)\n",
    "\n",
    "#agent.load_models()\n",
    "np.random.seed(0)\n",
    "\n",
    "score_history = []"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T13:04:33.127618Z",
     "start_time": "2025-04-17T13:03:17.297143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(1000):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        act = agent.choose_action(obs)\n",
    "        new_state, reward, done, info, _ = env.step(act)\n",
    "        agent.remember(obs, act, reward, new_state, int(done))\n",
    "        agent.learn()\n",
    "        score += reward\n",
    "        obs = new_state\n",
    "        #env.render()\n",
    "    score_history.append(score)\n",
    "\n",
    "\n",
    "    print('episode ', i, 'score %.2f' % score,\n",
    "          'trailing 100 games avg %.3f' % np.mean(score_history[-100:]))"
   ],
   "id": "eb678c7e57367859",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score -1646.10 trailing 100 games avg -1646.104\n",
      "episode  1 score -391.73 trailing 100 games avg -1018.918\n",
      "episode  2 score -511.62 trailing 100 games avg -849.819\n",
      "episode  3 score -447.63 trailing 100 games avg -749.272\n",
      "episode  4 score -296.15 trailing 100 games avg -658.648\n",
      "episode  5 score -197.84 trailing 100 games avg -581.847\n",
      "episode  6 score -140.84 trailing 100 games avg -518.846\n",
      "episode  7 score -228.67 trailing 100 games avg -482.574\n",
      "episode  8 score -279.17 trailing 100 games avg -459.973\n",
      "episode  9 score -616.69 trailing 100 games avg -475.645\n",
      "episode  10 score -402.03 trailing 100 games avg -468.952\n",
      "episode  11 score -733.63 trailing 100 games avg -491.009\n",
      "episode  12 score -642.54 trailing 100 games avg -502.665\n",
      "episode  13 score -372.00 trailing 100 games avg -493.332\n",
      "episode  14 score -564.37 trailing 100 games avg -498.067\n",
      "episode  15 score -504.14 trailing 100 games avg -498.447\n",
      "episode  16 score -474.05 trailing 100 games avg -497.012\n",
      "episode  17 score -473.96 trailing 100 games avg -495.731\n",
      "episode  18 score -505.42 trailing 100 games avg -496.241\n",
      "episode  19 score -481.19 trailing 100 games avg -495.489\n",
      "episode  20 score -201.55 trailing 100 games avg -481.492\n",
      "episode  21 score -139.95 trailing 100 games avg -465.967\n",
      "episode  22 score -232.66 trailing 100 games avg -455.823\n",
      "episode  23 score -401.90 trailing 100 games avg -453.577\n",
      "episode  24 score -273.75 trailing 100 games avg -446.384\n",
      "episode  25 score -181.61 trailing 100 games avg -436.200\n",
      "episode  26 score -176.57 trailing 100 games avg -426.584\n",
      "episode  27 score -250.53 trailing 100 games avg -420.297\n",
      "episode  28 score 35.22 trailing 100 games avg -404.589\n",
      "episode  29 score -180.89 trailing 100 games avg -397.133\n",
      "episode  30 score -172.12 trailing 100 games avg -389.874\n",
      "episode  31 score -176.19 trailing 100 games avg -383.196\n",
      "episode  32 score -158.26 trailing 100 games avg -376.380\n",
      "episode  33 score -368.40 trailing 100 games avg -376.145\n",
      "episode  34 score -164.16 trailing 100 games avg -370.089\n",
      "episode  35 score -173.80 trailing 100 games avg -364.636\n",
      "episode  36 score -241.18 trailing 100 games avg -361.299\n",
      "episode  37 score 39.71 trailing 100 games avg -350.747\n",
      "episode  38 score -176.67 trailing 100 games avg -346.283\n",
      "episode  39 score -212.33 trailing 100 games avg -342.934\n",
      "episode  40 score -246.80 trailing 100 games avg -340.590\n",
      "episode  41 score -236.25 trailing 100 games avg -338.105\n",
      "episode  42 score -322.52 trailing 100 games avg -337.743\n",
      "episode  43 score -293.15 trailing 100 games avg -336.729\n",
      "episode  44 score -309.59 trailing 100 games avg -336.126\n",
      "episode  45 score -271.45 trailing 100 games avg -334.720\n",
      "episode  46 score -277.75 trailing 100 games avg -333.508\n",
      "episode  47 score -252.73 trailing 100 games avg -331.825\n",
      "episode  48 score -478.70 trailing 100 games avg -334.823\n",
      "episode  49 score -251.34 trailing 100 games avg -333.153\n",
      "episode  50 score -311.14 trailing 100 games avg -332.721\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[33]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[32m      6\u001B[39m     act = agent.choose_action(obs)\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m     new_state, reward, done, info, _ = \u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mact\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m     agent.remember(obs, act, reward, new_state, \u001B[38;5;28mint\u001B[39m(done))\n\u001B[32m      9\u001B[39m     agent.learn()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/gymnasium/wrappers/common.py:125\u001B[39m, in \u001B[36mTimeLimit.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    112\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstep\u001B[39m(\n\u001B[32m    113\u001B[39m     \u001B[38;5;28mself\u001B[39m, action: ActType\n\u001B[32m    114\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[ObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[32m    115\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[32m    116\u001B[39m \n\u001B[32m    117\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    123\u001B[39m \n\u001B[32m    124\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     observation, reward, terminated, truncated, info = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    126\u001B[39m     \u001B[38;5;28mself\u001B[39m._elapsed_steps += \u001B[32m1\u001B[39m\n\u001B[32m    128\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._elapsed_steps >= \u001B[38;5;28mself\u001B[39m._max_episode_steps:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/gymnasium/wrappers/common.py:393\u001B[39m, in \u001B[36mOrderEnforcing.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    391\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._has_reset:\n\u001B[32m    392\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[33m\"\u001B[39m\u001B[33mCannot call env.step() before calling env.reset()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m393\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/gymnasium/core.py:327\u001B[39m, in \u001B[36mWrapper.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    323\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstep\u001B[39m(\n\u001B[32m    324\u001B[39m     \u001B[38;5;28mself\u001B[39m, action: WrapperActType\n\u001B[32m    325\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[32m    326\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m327\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/gymnasium/wrappers/common.py:285\u001B[39m, in \u001B[36mPassiveEnvChecker.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    283\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m.env, action)\n\u001B[32m    284\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m285\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/gymnasium/envs/box2d/lunar_lander.py:621\u001B[39m, in \u001B[36mLunarLander.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    607\u001B[39m         p.ApplyLinearImpulse(\n\u001B[32m    608\u001B[39m             (\n\u001B[32m    609\u001B[39m                 ox * SIDE_ENGINE_POWER * s_power,\n\u001B[32m   (...)\u001B[39m\u001B[32m    613\u001B[39m             \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    614\u001B[39m         )\n\u001B[32m    615\u001B[39m     \u001B[38;5;28mself\u001B[39m.lander.ApplyLinearImpulse(\n\u001B[32m    616\u001B[39m         (-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),\n\u001B[32m    617\u001B[39m         impulse_pos,\n\u001B[32m    618\u001B[39m         \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    619\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m621\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mworld\u001B[49m\u001B[43m.\u001B[49m\u001B[43mStep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m1.0\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m/\u001B[49m\u001B[43m \u001B[49m\u001B[43mFPS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m6\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m30\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    623\u001B[39m pos = \u001B[38;5;28mself\u001B[39m.lander.position\n\u001B[32m    624\u001B[39m vel = \u001B[38;5;28mself\u001B[39m.lander.linearVelocity\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/gymnasium/envs/box2d/lunar_lander.py:64\u001B[39m, in \u001B[36mContactDetector.BeginContact\u001B[39m\u001B[34m(self, contact)\u001B[39m\n\u001B[32m     61\u001B[39m     contactListener.\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m)\n\u001B[32m     62\u001B[39m     \u001B[38;5;28mself\u001B[39m.env = env\n\u001B[32m---> \u001B[39m\u001B[32m64\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mBeginContact\u001B[39m(\u001B[38;5;28mself\u001B[39m, contact):\n\u001B[32m     65\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m     66\u001B[39m         \u001B[38;5;28mself\u001B[39m.env.lander == contact.fixtureA.body\n\u001B[32m     67\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m.env.lander == contact.fixtureB.body\n\u001B[32m     68\u001B[39m     ):\n\u001B[32m     69\u001B[39m         \u001B[38;5;28mself\u001B[39m.env.game_over = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from AIA.rl.lander.plot import plotLearning\n",
    "\n",
    "plotLearning(score_history, window=100)"
   ],
   "id": "b4789741e548be50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Push T test 1",
   "id": "162441fa86dc5ec5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from AIA.rl.lander.plot import plotLearning\n",
    "from AIA.rl.lander.models import Agent\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from AIA.rl.lander.envs.pusht import PushTEnv\n",
    "\n",
    "\n",
    "# Initialise the environment\n",
    "env = PushTEnv(render_mode=\"rgb_array\")\n",
    "\n",
    "agent = Agent(alpha=1e-3, beta=1e-3, noise=25, input_dims=5, tau=0.001,\n",
    "              batch_size=64, n_actions=2)"
   ],
   "id": "c0f00638c40799da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#agent.load_models()\n",
    "np.random.seed(0)\n",
    "\n",
    "score_history = []"
   ],
   "id": "13d3723dba28ba2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(10000):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    prev_reward = 0\n",
    "    for t in range(400):\n",
    "        act = agent.choose_action(obs)\n",
    "        new_state, reward, done, info, _ = env.step(act)\n",
    "\n",
    "        prev_t_pos = obs[2:3]\n",
    "        new_t_pos = new_state[2:3]\n",
    "\n",
    "        d_move = np.sqrt(np.sum((prev_t_pos - new_t_pos)**2))\n",
    "\n",
    "        d_move_reward = d_move / 1000\n",
    "\n",
    "        buffer_reward = reward\n",
    "\n",
    "        reward -= prev_reward\n",
    "        reward += d_move_reward\n",
    "\n",
    "        prev_reward = buffer_reward\n",
    "\n",
    "        agent.remember(obs, act, reward, new_state, int(done))\n",
    "        agent.learn()\n",
    "        score += reward\n",
    "        obs = new_state\n",
    "        #env.render()\n",
    "    score_history.append(score)\n",
    "\n",
    "    #if i % 25 == 0:\n",
    "    #    agent.save_models()\n",
    "\n",
    "    print(f'episode , {i} score {(score):3f} trailing 100 games avg {np.mean(score_history[-100:]):3f}' )\n"
   ],
   "id": "cbed51e83b0c6e9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "env.close()",
   "id": "c916acfcf14a5b26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Push T test 2",
   "id": "11c341e3387a5181"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "import pickle\n",
    "from AIA.rl.lander.plot import plotLearning\n",
    "from AIA.rl.lander.models import Agent\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n"
   ],
   "id": "8f3b2a4de3d06f6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "env = PushTEnv(obs_type=\"state\", render_mode=\"rgb_array\")\n",
    "input_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.shape[0]\n",
    "\n",
    "    # load expert demonstrations saved as list of (s,a,r,s2,d)\n",
    "with open(\"demonstrations.pkl\", \"rb\") as f:\n",
    "    expert_transitions = pickle.load(f)\n",
    "\n",
    "agent = Agent(\n",
    "        alpha=2e-4, beta=2e-4,\n",
    "        input_dims=input_dim, n_actions=n_actions,\n",
    "        tau=0.001, gamma=0.99,\n",
    "        max_size=1_000_000, batch_size=256, noise=0.2,\n",
    "        expert_data=expert_transitions,\n",
    "        expert_ratio=0.25\n",
    ")"
   ],
   "id": "6c982d358f3453",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pre-Train from expert",
   "id": "c9583149627393bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def behaviour_clone(actor, demo, epochs=10, lr=1e-3, batch_size=256):\n",
    "    \"\"\"\n",
    "    Supervised pre‑training of the actor on expert (s→a) pairs\n",
    "    using mini‑batch SGD.\n",
    "    demo: list of (state, action, reward, next_state, done)\n",
    "    \"\"\"\n",
    "    device = actor.device\n",
    "    opt    = optim.Adam(actor.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    N = len(demo)\n",
    "\n",
    "    # pre‑stack everything once\n",
    "    all_states = torch.tensor([t[0] for t in demo],\n",
    "                              dtype=torch.float32, device=device)\n",
    "    all_actions = torch.tensor([t[1] for t in demo],\n",
    "                               dtype=torch.float32, device=device)\n",
    "\n",
    "    actor.train()\n",
    "    for ep in range(1, epochs+1):\n",
    "        perm = torch.randperm(N, device=device)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i in range(0, N, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            states = all_states[idx]\n",
    "            acts   = all_actions[idx]\n",
    "\n",
    "            pred = actor(states)\n",
    "            loss = loss_fn(pred, acts)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            epoch_loss += loss.item() * idx.size(0)\n",
    "\n",
    "        avg_loss = epoch_loss / N\n",
    "        if ep % max(1, epochs//10) == 0:\n",
    "            print(f\"[BC] Epoch {ep}/{epochs}, avg loss={avg_loss:.6f}\")\n",
    "\n",
    "    actor.eval()\n",
    "\n",
    "\n",
    "def pre_train_critic(critic, demo, gamma=0.99, epochs=10, batch_size=256, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Monte‑Carlo pre‑training of critic on expert episodes,\n",
    "    using mini‑batch SGD.\n",
    "    demo: list of (state, action, reward, next_state, done) in sequence order\n",
    "    \"\"\"\n",
    "    device = critic.device\n",
    "    opt    = optim.Adam(critic.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # 1) compute returns G_t\n",
    "    returns = []\n",
    "    G = 0.0\n",
    "    for (s,a,r,d,s2) in reversed(demo):\n",
    "        if d:\n",
    "            G = 0.0\n",
    "        G = r + gamma * G\n",
    "        returns.append(G)\n",
    "    returns = returns[::-1]\n",
    "\n",
    "    # 2) pre‑stack tensors\n",
    "    N = len(demo)\n",
    "    states  = torch.tensor([t[0] for t in demo],\n",
    "                           dtype=torch.float32, device=device)\n",
    "    actions = torch.tensor([t[1] for t in demo],\n",
    "                           dtype=torch.float32, device=device)\n",
    "    targets = torch.tensor(returns,\n",
    "                           dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "    critic.train()\n",
    "    for ep in range(1, epochs+1):\n",
    "        perm = torch.randperm(N, device=device)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i in range(0, N, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            s_batch = states[idx]\n",
    "            a_batch = actions[idx]\n",
    "            y_batch = targets[idx]\n",
    "\n",
    "            q_pred = critic(s_batch, a_batch)\n",
    "            loss   = loss_fn(q_pred, y_batch)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            epoch_loss += loss.item() * idx.size(0)\n",
    "\n",
    "        avg_loss = epoch_loss / N\n",
    "        print(f\"[Critic‑MC] Epoch {ep}/{epochs}, avg loss={avg_loss:.6f}\")\n",
    "\n",
    "    critic.eval()\n"
   ],
   "id": "424694aa70a5a8ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Pre‑train critic\n",
    "pre_train_critic(agent.critic,\n",
    "                 demo=expert_transitions,\n",
    "                 gamma=0.99,\n",
    "                 epochs=200,\n",
    "                 batch_size=256,\n",
    "                 lr=1e-4)"
   ],
   "id": "30d2abd8fb842fc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2) Pre‑train actor\n",
    "behaviour_clone(agent.actor,\n",
    "                demo=expert_transitions,\n",
    "                epochs=2000,\n",
    "                lr=1e-3)"
   ],
   "id": "1e8c565f89254284",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testy Pre-train results",
   "id": "eae77ce01e28978f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "env =  PushTEnv(obs_type=\"state\", render_mode=\"human\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "\n",
    "for i in range(200):\n",
    "\n",
    "    action = agent.choose_action(obs, eval=True)\n",
    "\n",
    "    nxt, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    obs = nxt\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ],
   "id": "15746f3a98e2be71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main Train",
   "id": "eb62b4fb64d1df23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "score_history = []",
   "id": "70fab3688c61be78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# env = PushTEnv(obs_type=\"state\", render_mode=\"rgb_array\")\n",
    "env = PushTEnv(obs_type=\"state\", render_mode=\"human\")"
   ],
   "id": "162a813bb9b12c60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for ep in range(1, 10001):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0.0\n",
    "    prev_reward = 0.0\n",
    "    for i in range(200):\n",
    "        env.render()\n",
    "        action = agent.choose_action(obs)\n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        prev_t_pos = obs[2:3]\n",
    "        new_t_pos = new_state[2:3]\n",
    "\n",
    "        d_move = np.sqrt(np.sum((prev_t_pos - new_t_pos)**2))\n",
    "\n",
    "        d_move_reward = d_move / 1000\n",
    "\n",
    "        buffer_reward = reward\n",
    "\n",
    "        reward -= prev_reward\n",
    "        reward += d_move_reward\n",
    "\n",
    "        prev_reward = buffer_reward\n",
    "\n",
    "        agent.memory.store_transition(obs, action, reward, new_state, done)\n",
    "        agent.learn()\n",
    "\n",
    "        obs = new_state\n",
    "        score += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    score_history.append(score)\n",
    "    if ep % 1 == 0:\n",
    "        avg = np.mean(score_history[-100:])\n",
    "        print(f\"Episode {ep:5d}  Score: {score:.2f}  100‑ep avg: {avg:.2f}\")"
   ],
   "id": "1183bc91ed2a58f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f0142a4b4cc61f2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "env.close()",
   "id": "a92f963d2b7f755",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a789cd30fc04c16a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
