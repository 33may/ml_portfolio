{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T11:51:31.507624Z",
     "start_time": "2025-04-17T11:51:31.482984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from anyio import sleep\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "5cf96a5db390e0a6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lander\n",
    "\n",
    "Initially I had multiple test runs and notebooks that was used to understand the domain of the RL problems. I figured out after the first run that the agent for PushT doesnt train. I decided to test my algorithms on the easier env from gym, particularly LunarLander with continuous action space\n",
    "\n",
    "Luckily the training algorithm I have implemented DDPG doesnt requires any adjustments to be implemented on other envs."
   ],
   "id": "143c8cee1f02b632"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-17T12:26:39.799068Z",
     "start_time": "2025-04-17T12:26:39.763143Z"
    }
   },
   "source": [
    "from AIA.rl.lander.models import Agent\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "# Initialise the environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\", continuous=True)\n",
    "\n",
    "agent = Agent(alpha=1e-4, beta=1e-4, input_dims=8, tau=0.001,\n",
    "              batch_size=64, n_actions=2, noise=0.15, expert_data=None)\n",
    "\n",
    "#agent.load_models()\n",
    "np.random.seed(0)\n",
    "\n",
    "score_history = []"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T12:27:18.522701Z",
     "start_time": "2025-04-17T12:26:40.327729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(1000):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        act = agent.choose_action(obs)\n",
    "        new_state, reward, done, info, _ = env.step(act)\n",
    "        agent.remember(obs, act, reward, new_state, int(done))\n",
    "        agent.learn()\n",
    "        score += reward\n",
    "        obs = new_state\n",
    "        #env.render()\n",
    "    score_history.append(score)\n",
    "\n",
    "\n",
    "    print('episode ', i, 'score %.2f' % score,\n",
    "          'trailing 100 games avg %.3f' % np.mean(score_history[-100:]))"
   ],
   "id": "eb678c7e57367859",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score -497.23 trailing 100 games avg -497.233\n",
      "episode  1 score -483.28 trailing 100 games avg -490.255\n",
      "episode  2 score -429.99 trailing 100 games avg -470.168\n",
      "episode  3 score -610.43 trailing 100 games avg -505.233\n",
      "episode  4 score -652.25 trailing 100 games avg -534.636\n",
      "episode  5 score -637.97 trailing 100 games avg -551.859\n",
      "episode  6 score -339.63 trailing 100 games avg -521.540\n",
      "episode  7 score -235.70 trailing 100 games avg -485.810\n",
      "episode  8 score -365.45 trailing 100 games avg -472.437\n",
      "episode  9 score -256.11 trailing 100 games avg -450.804\n",
      "episode  10 score -409.41 trailing 100 games avg -447.041\n",
      "episode  11 score -507.63 trailing 100 games avg -452.090\n",
      "episode  12 score -513.96 trailing 100 games avg -456.849\n",
      "episode  13 score -653.20 trailing 100 games avg -470.874\n",
      "episode  14 score -749.39 trailing 100 games avg -489.441\n",
      "episode  15 score -748.84 trailing 100 games avg -505.653\n",
      "episode  16 score -618.64 trailing 100 games avg -512.300\n",
      "episode  17 score -708.26 trailing 100 games avg -523.186\n",
      "episode  18 score -675.63 trailing 100 games avg -531.210\n",
      "episode  19 score -680.77 trailing 100 games avg -538.688\n",
      "episode  20 score -557.82 trailing 100 games avg -539.599\n",
      "episode  21 score -647.31 trailing 100 games avg -544.495\n",
      "episode  22 score -752.14 trailing 100 games avg -553.522\n",
      "episode  23 score -618.00 trailing 100 games avg -556.209\n",
      "episode  24 score -1099.91 trailing 100 games avg -577.957\n",
      "episode  25 score -910.04 trailing 100 games avg -590.729\n",
      "episode  26 score -630.67 trailing 100 games avg -592.209\n",
      "episode  27 score -1088.72 trailing 100 games avg -609.941\n",
      "episode  28 score -1262.65 trailing 100 games avg -632.449\n",
      "episode  29 score -659.28 trailing 100 games avg -633.343\n",
      "episode  30 score -845.03 trailing 100 games avg -640.172\n",
      "episode  31 score -703.74 trailing 100 games avg -642.158\n",
      "episode  32 score -769.79 trailing 100 games avg -646.026\n",
      "episode  33 score -795.49 trailing 100 games avg -650.422\n",
      "episode  34 score -673.30 trailing 100 games avg -651.076\n",
      "episode  35 score -620.81 trailing 100 games avg -650.235\n",
      "episode  36 score -593.69 trailing 100 games avg -648.707\n",
      "episode  37 score -612.64 trailing 100 games avg -647.758\n",
      "episode  38 score -626.43 trailing 100 games avg -647.211\n",
      "episode  39 score -673.79 trailing 100 games avg -647.875\n",
      "episode  40 score -841.02 trailing 100 games avg -652.586\n",
      "episode  41 score -659.46 trailing 100 games avg -652.750\n",
      "episode  42 score -944.25 trailing 100 games avg -659.529\n",
      "episode  43 score -557.46 trailing 100 games avg -657.209\n",
      "episode  44 score -730.64 trailing 100 games avg -658.841\n",
      "episode  45 score -719.54 trailing 100 games avg -660.161\n",
      "episode  46 score -1191.67 trailing 100 games avg -671.469\n",
      "episode  47 score -684.81 trailing 100 games avg -671.747\n",
      "episode  48 score -885.38 trailing 100 games avg -676.107\n",
      "episode  49 score -866.62 trailing 100 games avg -679.918\n",
      "episode  50 score -461.04 trailing 100 games avg -675.626\n",
      "episode  51 score -459.39 trailing 100 games avg -671.467\n",
      "episode  52 score -742.49 trailing 100 games avg -672.807\n",
      "episode  53 score -465.01 trailing 100 games avg -668.959\n",
      "episode  54 score -414.13 trailing 100 games avg -664.326\n",
      "episode  55 score -636.89 trailing 100 games avg -663.836\n",
      "episode  56 score -463.72 trailing 100 games avg -660.325\n",
      "episode  57 score -484.33 trailing 100 games avg -657.291\n",
      "episode  58 score -582.10 trailing 100 games avg -656.016\n",
      "episode  59 score -553.58 trailing 100 games avg -654.309\n",
      "episode  60 score -683.94 trailing 100 games avg -654.795\n",
      "episode  61 score -773.41 trailing 100 games avg -656.708\n",
      "episode  62 score -592.89 trailing 100 games avg -655.695\n",
      "episode  63 score -603.44 trailing 100 games avg -654.878\n",
      "episode  64 score -549.67 trailing 100 games avg -653.260\n",
      "episode  65 score -670.80 trailing 100 games avg -653.525\n",
      "episode  66 score -303.76 trailing 100 games avg -648.305\n",
      "episode  67 score -557.19 trailing 100 games avg -646.965\n",
      "episode  68 score -413.98 trailing 100 games avg -643.589\n",
      "episode  69 score -350.58 trailing 100 games avg -639.403\n",
      "episode  70 score -206.96 trailing 100 games avg -633.312\n",
      "episode  71 score -418.11 trailing 100 games avg -630.323\n",
      "episode  72 score -237.09 trailing 100 games avg -624.936\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      4\u001B[39m score = \u001B[32m0\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m     act = \u001B[43magent\u001B[49m\u001B[43m.\u001B[49m\u001B[43mchoose_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m     new_state, reward, done, info, _ = env.step(act)\n\u001B[32m      8\u001B[39m     agent.remember(obs, act, reward, new_state, \u001B[38;5;28mint\u001B[39m(done))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<string>:5\u001B[39m, in \u001B[36mchoose_action\u001B[39m\u001B[34m(self, observation, eval)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<string>:3\u001B[39m, in \u001B[36mforward\u001B[39m\u001B[34m(self, state)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    248\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[32m    249\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m         \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    251\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from AIA.rl.lander.plot import plotLearning\n",
    "\n",
    "plotLearning(score_history, window=100)"
   ],
   "id": "b4789741e548be50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Push T test 1",
   "id": "162441fa86dc5ec5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from AIA.rl.lander.plot import plotLearning\n",
    "from AIA.rl.lander.models import Agent\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from AIA.rl.lander.envs.pusht import PushTEnv\n",
    "\n",
    "\n",
    "# Initialise the environment\n",
    "env = PushTEnv(render_mode=\"rgb_array\")\n",
    "\n",
    "agent = Agent(alpha=1e-3, beta=1e-3, noise=25, input_dims=5, tau=0.001,\n",
    "              batch_size=64, n_actions=2)"
   ],
   "id": "c0f00638c40799da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#agent.load_models()\n",
    "np.random.seed(0)\n",
    "\n",
    "score_history = []"
   ],
   "id": "13d3723dba28ba2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(10000):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    prev_reward = 0\n",
    "    for t in range(400):\n",
    "        act = agent.choose_action(obs)\n",
    "        new_state, reward, done, info, _ = env.step(act)\n",
    "\n",
    "        prev_t_pos = obs[2:3]\n",
    "        new_t_pos = new_state[2:3]\n",
    "\n",
    "        d_move = np.sqrt(np.sum((prev_t_pos - new_t_pos)**2))\n",
    "\n",
    "        d_move_reward = d_move / 1000\n",
    "\n",
    "        buffer_reward = reward\n",
    "\n",
    "        reward -= prev_reward\n",
    "        reward += d_move_reward\n",
    "\n",
    "        prev_reward = buffer_reward\n",
    "\n",
    "        agent.remember(obs, act, reward, new_state, int(done))\n",
    "        agent.learn()\n",
    "        score += reward\n",
    "        obs = new_state\n",
    "        #env.render()\n",
    "    score_history.append(score)\n",
    "\n",
    "    #if i % 25 == 0:\n",
    "    #    agent.save_models()\n",
    "\n",
    "    print(f'episode , {i} score {(score):3f} trailing 100 games avg {np.mean(score_history[-100:]):3f}' )\n"
   ],
   "id": "cbed51e83b0c6e9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "env.close()",
   "id": "c916acfcf14a5b26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Push T test 2",
   "id": "11c341e3387a5181"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "import pickle\n",
    "from AIA.rl.lander.plot import plotLearning\n",
    "from AIA.rl.lander.models import Agent\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n"
   ],
   "id": "8f3b2a4de3d06f6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "env = PushTEnv(obs_type=\"state\", render_mode=\"rgb_array\")\n",
    "input_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.shape[0]\n",
    "\n",
    "    # load expert demonstrations saved as list of (s,a,r,s2,d)\n",
    "with open(\"demonstrations.pkl\", \"rb\") as f:\n",
    "    expert_transitions = pickle.load(f)\n",
    "\n",
    "agent = Agent(\n",
    "        alpha=2e-4, beta=2e-4,\n",
    "        input_dims=input_dim, n_actions=n_actions,\n",
    "        tau=0.001, gamma=0.99,\n",
    "        max_size=1_000_000, batch_size=256, noise=0.2,\n",
    "        expert_data=expert_transitions,\n",
    "        expert_ratio=0.25\n",
    ")"
   ],
   "id": "6c982d358f3453",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pre-Train from expert",
   "id": "c9583149627393bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def behaviour_clone(actor, demo, epochs=10, lr=1e-3, batch_size=256):\n",
    "    \"\"\"\n",
    "    Supervised pre‑training of the actor on expert (s→a) pairs\n",
    "    using mini‑batch SGD.\n",
    "    demo: list of (state, action, reward, next_state, done)\n",
    "    \"\"\"\n",
    "    device = actor.device\n",
    "    opt    = optim.Adam(actor.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    N = len(demo)\n",
    "\n",
    "    # pre‑stack everything once\n",
    "    all_states = torch.tensor([t[0] for t in demo],\n",
    "                              dtype=torch.float32, device=device)\n",
    "    all_actions = torch.tensor([t[1] for t in demo],\n",
    "                               dtype=torch.float32, device=device)\n",
    "\n",
    "    actor.train()\n",
    "    for ep in range(1, epochs+1):\n",
    "        perm = torch.randperm(N, device=device)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i in range(0, N, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            states = all_states[idx]\n",
    "            acts   = all_actions[idx]\n",
    "\n",
    "            pred = actor(states)\n",
    "            loss = loss_fn(pred, acts)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            epoch_loss += loss.item() * idx.size(0)\n",
    "\n",
    "        avg_loss = epoch_loss / N\n",
    "        if ep % max(1, epochs//10) == 0:\n",
    "            print(f\"[BC] Epoch {ep}/{epochs}, avg loss={avg_loss:.6f}\")\n",
    "\n",
    "    actor.eval()\n",
    "\n",
    "\n",
    "def pre_train_critic(critic, demo, gamma=0.99, epochs=10, batch_size=256, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Monte‑Carlo pre‑training of critic on expert episodes,\n",
    "    using mini‑batch SGD.\n",
    "    demo: list of (state, action, reward, next_state, done) in sequence order\n",
    "    \"\"\"\n",
    "    device = critic.device\n",
    "    opt    = optim.Adam(critic.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # 1) compute returns G_t\n",
    "    returns = []\n",
    "    G = 0.0\n",
    "    for (s,a,r,d,s2) in reversed(demo):\n",
    "        if d:\n",
    "            G = 0.0\n",
    "        G = r + gamma * G\n",
    "        returns.append(G)\n",
    "    returns = returns[::-1]\n",
    "\n",
    "    # 2) pre‑stack tensors\n",
    "    N = len(demo)\n",
    "    states  = torch.tensor([t[0] for t in demo],\n",
    "                           dtype=torch.float32, device=device)\n",
    "    actions = torch.tensor([t[1] for t in demo],\n",
    "                           dtype=torch.float32, device=device)\n",
    "    targets = torch.tensor(returns,\n",
    "                           dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "    critic.train()\n",
    "    for ep in range(1, epochs+1):\n",
    "        perm = torch.randperm(N, device=device)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i in range(0, N, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            s_batch = states[idx]\n",
    "            a_batch = actions[idx]\n",
    "            y_batch = targets[idx]\n",
    "\n",
    "            q_pred = critic(s_batch, a_batch)\n",
    "            loss   = loss_fn(q_pred, y_batch)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            epoch_loss += loss.item() * idx.size(0)\n",
    "\n",
    "        avg_loss = epoch_loss / N\n",
    "        print(f\"[Critic‑MC] Epoch {ep}/{epochs}, avg loss={avg_loss:.6f}\")\n",
    "\n",
    "    critic.eval()\n"
   ],
   "id": "424694aa70a5a8ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Pre‑train critic\n",
    "pre_train_critic(agent.critic,\n",
    "                 demo=expert_transitions,\n",
    "                 gamma=0.99,\n",
    "                 epochs=200,\n",
    "                 batch_size=256,\n",
    "                 lr=1e-4)"
   ],
   "id": "30d2abd8fb842fc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2) Pre‑train actor\n",
    "behaviour_clone(agent.actor,\n",
    "                demo=expert_transitions,\n",
    "                epochs=2000,\n",
    "                lr=1e-3)"
   ],
   "id": "1e8c565f89254284",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testy Pre-train results",
   "id": "eae77ce01e28978f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "env =  PushTEnv(obs_type=\"state\", render_mode=\"human\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "\n",
    "for i in range(200):\n",
    "\n",
    "    action = agent.choose_action(obs, eval=True)\n",
    "\n",
    "    nxt, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    obs = nxt\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ],
   "id": "15746f3a98e2be71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main Train",
   "id": "eb62b4fb64d1df23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "score_history = []",
   "id": "70fab3688c61be78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# env = PushTEnv(obs_type=\"state\", render_mode=\"rgb_array\")\n",
    "env = PushTEnv(obs_type=\"state\", render_mode=\"human\")"
   ],
   "id": "162a813bb9b12c60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for ep in range(1, 10001):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0.0\n",
    "    prev_reward = 0.0\n",
    "    for i in range(200):\n",
    "        env.render()\n",
    "        action = agent.choose_action(obs)\n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        prev_t_pos = obs[2:3]\n",
    "        new_t_pos = new_state[2:3]\n",
    "\n",
    "        d_move = np.sqrt(np.sum((prev_t_pos - new_t_pos)**2))\n",
    "\n",
    "        d_move_reward = d_move / 1000\n",
    "\n",
    "        buffer_reward = reward\n",
    "\n",
    "        reward -= prev_reward\n",
    "        reward += d_move_reward\n",
    "\n",
    "        prev_reward = buffer_reward\n",
    "\n",
    "        agent.memory.store_transition(obs, action, reward, new_state, done)\n",
    "        agent.learn()\n",
    "\n",
    "        obs = new_state\n",
    "        score += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    score_history.append(score)\n",
    "    if ep % 1 == 0:\n",
    "        avg = np.mean(score_history[-100:])\n",
    "        print(f\"Episode {ep:5d}  Score: {score:.2f}  100‑ep avg: {avg:.2f}\")"
   ],
   "id": "1183bc91ed2a58f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f0142a4b4cc61f2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "env.close()",
   "id": "a92f963d2b7f755",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a789cd30fc04c16a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
