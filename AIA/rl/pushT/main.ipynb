{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:29:40.953559Z",
     "start_time": "2025-04-12T12:29:40.920677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from time import sleep\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')"
   ],
   "id": "8858c545aef378d4",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:29:42.148896Z",
     "start_time": "2025-04-12T12:29:42.133986Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.is_available()",
   "id": "a0613f77d6970844",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Env",
   "id": "86356765458d747a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:30:01.058266Z",
     "start_time": "2025-04-12T12:29:43.380481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import gym_pusht\n",
    "from time import sleep\n",
    "\n",
    "env = gym.make(\"gym_pusht/PushT-v0\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    sleep(0.2)\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    image = env.render()\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ],
   "id": "ec79454c1f427403",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[44]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      6\u001B[39m observation, info = env.reset()\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m1000\u001B[39m):\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m     \u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     10\u001B[39m     action = env.action_space.sample()\n\u001B[32m     11\u001B[39m     observation, reward, terminated, truncated, info = env.step(action)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Implement the deepq Learning to compete the task\n",
    "\n",
    "# Part 1: Numerical PushT state"
   ],
   "id": "e2c5fdb9d360c10d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Observation Space\n",
    "\n",
    "If obs_type is set to state, the observation space is a 5-dimensional vector representing the state of the environment: [agent_x, agent_y, block_x, block_y, block_angle]. The values are in the range [0, 512] for the agent and block positions and [0, 2*pi] for the block angle.\n",
    "\n",
    "If obs_type is set to environment_state_agent_pos the observation space is a dictionary with: - environment_state: 16-dimensional vector representing the keypoint locations of the T (in [x0, y0, x1, y1, ...] format). The values are in the range [0, 512]. - agent_pos: A 2-dimensional vector representing the position of the robot end-effector.\n",
    "\n",
    "If obs_type is set to pixels, the observation space is a 96x96 RGB image of the environment."
   ],
   "id": "bc5e6ca06d7cae1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T17:19:32.831444Z",
     "start_time": "2025-04-11T17:19:32.828379Z"
    }
   },
   "cell_type": "code",
   "source": "env.observation_space",
   "id": "3532b284239d8319",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0.0, [512.         512.         512.         512.           6.28318531], (5,), float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The Policy model is designed to give the action $a$ given the state $s$.\n",
    "\n",
    "input:\n",
    "\n",
    "$$\n",
    "[agent_x, agent_y, t_x, t_y, t_{angle}]\n",
    "$$\n",
    "\n",
    "output\n",
    "\n",
    "$$\n",
    "[move_x, move_y]\n",
    "$$\n",
    "\n",
    "\n",
    "so the model tells\n",
    "\n",
    "$$\n",
    "a = P(s)\n",
    "$$"
   ],
   "id": "cc54863a51844fba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:30:11.806716Z",
     "start_time": "2025-04-12T12:30:11.800504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ],
   "id": "a922690350c34ee9",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:30:12.463088Z",
     "start_time": "2025-04-12T12:30:12.454752Z"
    }
   },
   "cell_type": "code",
   "source": "model = Policy()",
   "id": "507e7f6d6153081e",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:30:13.795621Z",
     "start_time": "2025-04-12T12:30:13.788925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input = torch.rand((4, 5))\n",
    "\n",
    "input"
   ],
   "id": "bd6bcbad5e249537",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8921, 0.7441, 0.9367, 0.9744, 0.6661],\n",
       "        [0.0690, 0.1201, 0.0467, 0.2564, 0.5554],\n",
       "        [0.7746, 0.5310, 0.0090, 0.2866, 0.5752],\n",
       "        [0.3824, 0.0491, 0.6623, 0.8789, 0.7061]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:30:15.022614Z",
     "start_time": "2025-04-12T12:30:15.006671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = model(input)\n",
    "\n",
    "\n",
    "print(output.shape)\n",
    "output.detach().numpy()"
   ],
   "id": "b0d3bf08af70bb2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.17463213, -0.11809652],\n",
       "       [-0.0337225 , -0.07954362],\n",
       "       [-0.10974984, -0.05514188],\n",
       "       [-0.1263717 , -0.13616867]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:31:52.281284Z",
     "start_time": "2025-04-12T12:31:52.275664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0] + env.action_space.shape[0], 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        return x"
   ],
   "id": "db60648c4b319e90",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:31:52.989910Z",
     "start_time": "2025-04-12T12:31:52.983476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Critic()\n",
    "input = torch.rand((4, 7))\n",
    "\n",
    "input"
   ],
   "id": "1c6c3f6aa5c4a3c7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0183, 0.4395, 0.6245, 0.7507, 0.4421, 0.0535, 0.4591],\n",
       "        [0.6714, 0.7539, 0.2639, 0.4286, 0.1981, 0.0199, 0.7007],\n",
       "        [0.3981, 0.7796, 0.4714, 0.0915, 0.3333, 0.9992, 0.0658],\n",
       "        [0.6996, 0.4717, 0.0114, 0.6784, 0.1693, 0.4736, 0.8691]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:31:53.538651Z",
     "start_time": "2025-04-12T12:31:53.532980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = model(input)\n",
    "\n",
    "\n",
    "print(output.shape)\n",
    "output.detach().numpy()"
   ],
   "id": "a1fcc2a9dcab25af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Critic model is aimed to estimate\n",
    "\n",
    "$$\n",
    "Q(s,a)\n",
    "$$\n",
    "\n",
    "it takes concat input $[s,a]$, more detailed\n",
    "\n",
    "input\n",
    "\n",
    "$$\n",
    "[agent_x, agent_y, t_x, t_y, t_{angle}, move_x, move_y]\n",
    "$$\n",
    "\n",
    "output\n",
    "\n",
    "$$\n",
    "score\n",
    "$$\n",
    "\n",
    "\n",
    "so it give single estimate of $Q$ value\n",
    "\n",
    "$$\n",
    "score = Q(s,a)\n",
    "$$\n",
    "\n",
    "The Policy model is trained using Critic model\n",
    "\n",
    "Since we cant directly access the action value $Q(s,a)$ we use the critic model that gives estimate $Q'(s,a)$ to understand the value of this state action pair. Then we could use this estimate to compute the loss\n",
    "\n",
    "\n",
    "$$\n",
    "loss = -Q'(s,a)\n",
    "$$\n",
    "\n",
    "\\- is used because optimization tasks aim to minimize function, hence minimizing -f is equivalent to maximizing f"
   ],
   "id": "4cf17505f9781f41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_policy(policy_model, critic_model, input, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    input = input.to(device)\n",
    "\n",
    "    output = policy_model(input)\n",
    "\n",
    "    critic_input = torch.cat((input, output), dim=1)\n",
    "\n",
    "    score = critic_model(critic_input)\n",
    "\n",
    "    loss = -score\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ],
   "id": "4b398896813e24d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For training critic, we use the actual reward we received from the env, bootstrap with critic model and train it with basic MSE loss",
   "id": "685a9eec2dbccad2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:32:23.836405Z",
     "start_time": "2025-04-12T12:32:23.829251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_critic(model, input, target, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    input = input.to(device)\n",
    "\n",
    "    output = model(input)\n",
    "\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ],
   "id": "451971957e45a10c",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "so in my approach the policy actually changes, but I could ignore the fact that actions were recorded under another distribution because I am using Q, which gives some level of abstraction which is stable for the environment and any optimal policy will converge to identical function. Then the agent here is just the another function that is trained on this level of the representation of the environment. If I had the training process without Q where the policy is responsible to somehow incorporate the knowledge of the environment inside itself then I need to also think of sampling, because on the interpretation level the actions were done on that perception of env, which changed with policy.\n",
    "\n",
    "this is idea of the model-based and model-free and particaul;larty off-policy and on-policy in the fact that env is not encoded in the model"
   ],
   "id": "d8e48b00ca270c35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here comes the problem since the update rule is based on MSE of the value\n",
    "$$\n",
    "(Q(s,a), R(s,a))\n",
    "$$\n",
    "\n",
    "where target $R(s,a)$ is calculated\n",
    "\n",
    "$$\n",
    "R(s,a) = r + \\gamma \\cdot Q(s', \\pi(s'))\n",
    "$$\n",
    "\n",
    "then the update rule:\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow \\alpha \\cdot (Q(s,a) - (r + \\gamma \\cdot Q(s', \\pi(s'))))^2\n",
    "$$"
   ],
   "id": "fdd5fb603e896f82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:32:24.446874Z",
     "start_time": "2025-04-12T12:32:24.441926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, batch_size = 128):\n",
    "        self.items = []\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, item):\n",
    "        self.items.append(item)\n",
    "\n",
    "    def sample(self):\n",
    "        return random.sample(self.items, self.batch_size)"
   ],
   "id": "8f5bbe5e16f29e23",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T17:57:35.315882Z",
     "start_time": "2025-04-11T17:57:05.344108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(policy_model, value_model, memory, episodes = 10000, max_steps = 1000):\n",
    "    env = gym.make(\"gym_pusht/PushT-v0\", obs_type=\"state\", render_mode=\"rgb_array\")\n",
    "\n",
    "    for ep in range(episodes):\n",
    "\n",
    "        state, info = env.reset()\n",
    "\n",
    "        for t in range(max_steps):\n",
    "\n",
    "            state = torch.stack([torch.from_numpy(state).float()])\n",
    "\n",
    "            action = model.forward(state)[0].detach().numpy()\n",
    "\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            sarsa = (state, action, reward, new_state)\n",
    "\n",
    "            memory.add(sarsa)\n",
    "\n",
    "            if len(memory) > memory.batch_size:\n",
    "                train_set = memory.sample()\n",
    "                policy_loss = train_policy()\n",
    "\n",
    "train(model)\n"
   ],
   "id": "4ccea2a00387e61e",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[24]\u001B[39m\u001B[32m, line 21\u001B[39m\n\u001B[32m     17\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated:\n\u001B[32m     18\u001B[39m                 observation, info = env.reset()\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[24]\u001B[39m\u001B[32m, line 15\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(model, episodes, max_steps)\u001B[39m\n\u001B[32m     12\u001B[39m action = model.forward(state)[\u001B[32m0\u001B[39m].detach().numpy()\n\u001B[32m     14\u001B[39m observation, reward, terminated, truncated, info = env.step(action)\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m image = \u001B[43menv\u001B[49m.render()\n\u001B[32m     17\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated:\n\u001B[32m     18\u001B[39m     observation, info = env.reset()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Applications/PyCharm Professional Edition.app/Contents/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_frame.py:888\u001B[39m, in \u001B[36mPyDBFrame.trace_dispatch\u001B[39m\u001B[34m(self, frame, event, arg)\u001B[39m\n\u001B[32m    885\u001B[39m             stop = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    887\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m plugin_stop:\n\u001B[32m--> \u001B[39m\u001B[32m888\u001B[39m     stopped_on_plugin = \u001B[43mplugin_manager\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmain_debugger\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop_info\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep_cmd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    889\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m stop:\n\u001B[32m    890\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m is_line:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Applications/PyCharm Professional Edition.app/Contents/plugins/python/helpers-pro/jupyter_debug/pydev_jupyter_plugin.py:169\u001B[39m, in \u001B[36mstop\u001B[39m\u001B[34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001B[39m\n\u001B[32m    167\u001B[39m     frame = suspend_jupyter(main_debugger, thread, frame, step_cmd)\n\u001B[32m    168\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m frame:\n\u001B[32m--> \u001B[39m\u001B[32m169\u001B[39m         \u001B[43mmain_debugger\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    170\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    171\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Applications/PyCharm Professional Edition.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1220\u001B[39m, in \u001B[36mPyDB.do_wait_suspend\u001B[39m\u001B[34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[39m\n\u001B[32m   1217\u001B[39m         from_this_thread.append(frame_id)\n\u001B[32m   1219\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, stop_reason):\n\u001B[32m-> \u001B[39m\u001B[32m1220\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Applications/PyCharm Professional Edition.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1235\u001B[39m, in \u001B[36mPyDB._do_wait_suspend\u001B[39m\u001B[34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[39m\n\u001B[32m   1232\u001B[39m             \u001B[38;5;28mself\u001B[39m._call_mpl_hook()\n\u001B[32m   1234\u001B[39m         \u001B[38;5;28mself\u001B[39m.process_internal_commands()\n\u001B[32m-> \u001B[39m\u001B[32m1235\u001B[39m         \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1237\u001B[39m \u001B[38;5;28mself\u001B[39m.cancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[32m   1239\u001B[39m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c12ed16288f2ae38"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
