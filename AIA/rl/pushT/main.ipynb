{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T22:15:05.202137Z",
     "start_time": "2025-04-15T22:15:05.191988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from math import gamma\n",
    "from time import sleep\n",
    "\n",
    "import torch\n",
    "\n",
    "compute_device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "cpu_device = torch.device('cpu')"
   ],
   "id": "d4bd6aaaebb2d9cf",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T22:15:05.482161Z",
     "start_time": "2025-04-15T22:15:05.472203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "81814bd45a321a2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T22:15:05.609268Z",
     "start_time": "2025-04-15T22:15:05.600641Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.is_available()",
   "id": "ce9d46d06fc032a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Env",
   "id": "a340e21036db8935"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T22:15:06.401026Z",
     "start_time": "2025-04-15T22:15:06.392685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_env(render_mode=\"rgb_array\"):\n",
    "    env = gym.make(\"gym_pusht/PushT-v0\", render_mode=render_mode)\n",
    "    return env"
   ],
   "id": "846e4b69a31c1589",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T22:15:07.013951Z",
     "start_time": "2025-04-15T22:15:07.005357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import gym_pusht\n",
    "from time import sleep\n",
    "\n",
    "env = make_env()"
   ],
   "id": "da2d78f75a51537c",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T22:12:30.341807Z",
     "start_time": "2025-04-15T22:12:27.389656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_env = make_env(render_mode=\"human\")\n",
    "\n",
    "observation, info = test_env.reset()\n",
    "\n",
    "for _ in range(50):\n",
    "    sleep(0.2)\n",
    "    action = test_env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = test_env.step(action)\n",
    "    image = test_env.render()\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = test_env.reset()\n",
    "\n",
    "test_env.close()"
   ],
   "id": "6c1c6d87acea3b25",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[26]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      3\u001B[39m observation, info = test_env.reset()\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m50\u001B[39m):\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m     \u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m     action = test_env.action_space.sample()\n\u001B[32m      8\u001B[39m     observation, reward, terminated, truncated, info = test_env.step(action)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T22:12:31.726945Z",
     "start_time": "2025-04-15T22:12:31.694438Z"
    }
   },
   "cell_type": "code",
   "source": "test_env.close()",
   "id": "7a8b77d57915fb1e",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Implement the deepq Learning to compete the task\n",
    "\n",
    "# Part 1: Numerical PushT state"
   ],
   "id": "b4bbb7452875153a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Observation Space\n",
    "\n",
    "If obs_type is set to state, the observation space is a 5-dimensional vector representing the state of the environment: [agent_x, agent_y, block_x, block_y, block_angle]. The values are in the range [0, 512] for the agent and block positions and [0, 2*pi] for the block angle.\n",
    "\n",
    "If obs_type is set to environment_state_agent_pos the observation space is a dictionary with: - environment_state: 16-dimensional vector representing the keypoint locations of the T (in [x0, y0, x1, y1, ...] format). The values are in the range [0, 512]. - agent_pos: A 2-dimensional vector representing the position of the robot end-effector.\n",
    "\n",
    "If obs_type is set to pixels, the observation space is a 96x96 RGB image of the environment."
   ],
   "id": "facbb94942ef1469"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T22:15:10.599743Z",
     "start_time": "2025-04-15T22:15:10.589695Z"
    }
   },
   "cell_type": "code",
   "source": "env.observation_space",
   "id": "c90e3fcd1c3a5c9a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
       "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
       "  1.         1.       ], (8,), float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The Policy model is designed to give the action $a$ given the state $s$.\n",
    "\n",
    "input:\n",
    "\n",
    "$$\n",
    "[agent_x, agent_y, t_x, t_y, t_{angle}]\n",
    "$$\n",
    "\n",
    "output\n",
    "\n",
    "$$\n",
    "[move_x, move_y]\n",
    "$$\n",
    "\n",
    "\n",
    "so the model tells\n",
    "\n",
    "$$\n",
    "a = P(s)\n",
    "$$"
   ],
   "id": "e6c91e3c3127892"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T22:15:11.419611Z",
     "start_time": "2025-04-15T22:15:11.411352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from torch import nn\n",
    "# import torch.nn.functional as F\n",
    "#\n",
    "#\n",
    "# class Policy(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Policy, self).__init__()\n",
    "#         self.fc1 = nn.Linear(env.observation_space.shape[0], 64)\n",
    "#         self.fc2 = nn.Linear(64, 64)\n",
    "#         self.fc3 = nn.Linear(64, 2)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#\n",
    "#         return x\n",
    "# model = Policy()\n",
    "# input = torch.rand((4, 5))\n",
    "#\n",
    "# input\n",
    "# output = model(input)\n",
    "#\n",
    "# print(output.shape)\n",
    "# output.detach().numpy()"
   ],
   "id": "8b8fc1008de91182",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T22:15:11.726347Z",
     "start_time": "2025-04-15T22:15:11.718213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class Critic(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Critic, self).__init__()\n",
    "#         self.fc1 = nn.Linear(env.observation_space.shape[0] + env.action_space.shape[0], 64)\n",
    "#         self.fc2 = nn.Linear(64, 64)\n",
    "#         self.fc3 = nn.Linear(64, 1)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = F.relu(self.fc3(x))\n",
    "#\n",
    "#         return x\n",
    "# model = Critic()\n",
    "# input = torch.rand((4, 7))\n",
    "#\n",
    "# input\n",
    "# output = model(input)\n",
    "#\n",
    "# print(output.shape)\n",
    "# output.detach().numpy()"
   ],
   "id": "10d579061cb91f02",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Critic model is aimed to estimate\n",
    "\n",
    "$$\n",
    "Q(s,a)\n",
    "$$\n",
    "\n",
    "it takes concat input $[s,a]$, more detailed\n",
    "\n",
    "input\n",
    "\n",
    "$$\n",
    "[agent_x, agent_y, t_x, t_y, t_{angle}, move_x, move_y]\n",
    "$$\n",
    "\n",
    "output\n",
    "\n",
    "$$\n",
    "score\n",
    "$$\n",
    "\n",
    "\n",
    "so it give single estimate of $Q$ value\n",
    "\n",
    "$$\n",
    "score = Q(s,a)\n",
    "$$\n",
    "\n",
    "The Policy model is trained using Critic model\n",
    "\n",
    "Since we cant directly access the action value $Q(s,a)$ we use the critic model that gives estimate $Q'(s,a)$ to understand the value of this state action pair. Then we could use this estimate to compute the loss\n",
    "\n",
    "\n",
    "$$\n",
    "loss = -Q'(s,a)\n",
    "$$\n",
    "\n",
    "\\- is used because optimization tasks aim to minimize function, hence minimizing -f is equivalent to maximizing f"
   ],
   "id": "5ca3ca89c592e1ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T22:15:12.259806Z",
     "start_time": "2025-04-15T22:15:12.251590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def train_policy(policy_model, critic_model, input, optimizer):\n",
    "#     \"\"\"\n",
    "#\n",
    "#     Args:\n",
    "#         policy_model: Model to update\n",
    "#         critic_model: Critic model to compute the value of the proposed action\n",
    "#         input: the input of shape [batch_size, 5] = (batch_size, observation_space.shape)\n",
    "#         optimizer: optimizer for the policy model\n",
    "#\n",
    "#     Returns:\n",
    "#\n",
    "#     \"\"\"\n",
    "#     optimizer.zero_grad()\n",
    "#\n",
    "#     input = input.to(compute_device)\n",
    "#\n",
    "#     output = policy_model(input)\n",
    "#\n",
    "#     critic_input = torch.cat((input, output), dim=1)\n",
    "#\n",
    "#     score = critic_model(critic_input)\n",
    "#\n",
    "#     loss = -score\n",
    "#\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#\n",
    "#     return loss.item()"
   ],
   "id": "9143c5e212e3f795",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For training critic, we use the actual reward we received from the env, bootstrap with critic model and train it with basic MSE loss",
   "id": "4f56be6e07fe0c4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T22:15:12.953Z",
     "start_time": "2025-04-15T22:15:12.944768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def train_critic(critic_model, input, target, optimizer):\n",
    "#     \"\"\"\n",
    "#     Train function that run one update on the critic network using batch of inputs.\n",
    "#\n",
    "#     Args:\n",
    "#         critic_model: Model to update\n",
    "#         input: the input of shape [batch_size, 7] = (batch_size, observation_space.shape + action_space.shape)\n",
    "#         target: Is the target reward we received from env and critic_estimate model\n",
    "#         optimizer: optimizer for the critic model\n",
    "#\n",
    "#     Returns:\n",
    "#\n",
    "#     \"\"\"\n",
    "#     optimizer.zero_grad()\n",
    "#     criterion = nn.MSELoss()\n",
    "#\n",
    "#     critic_model.train()\n",
    "#\n",
    "#     input = input.to(compute_device)\n",
    "#\n",
    "#     output = critic_model(input)\n",
    "#\n",
    "#     loss = criterion(output, target)\n",
    "#\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#\n",
    "#     return loss.item()"
   ],
   "id": "931a1849e75ffd4b",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "so in my approach the policy actually changes, but I could ignore the fact that actions were recorded under another distribution because I am using Q, which gives some level of abstraction which is stable for the environment and any optimal policy will converge to identical function. Then the agent here is just the another function that is trained on this level of the representation of the environment. If I had the training process without Q where the policy is responsible to somehow incorporate the knowledge of the environment inside itself then I need to also think of sampling, because on the interpretation level the actions were done on that perception of env, which changed with policy.\n",
    "\n",
    "this is idea of the model-based and model-free and particaul;larty off-policy and on-policy in the fact that env is not encoded in the model"
   ],
   "id": "c3324e470ba656eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here comes the problem since the update rule is based on MSE of the value\n",
    "$$\n",
    "(Q(s,a), R(s,a))\n",
    "$$\n",
    "\n",
    "where target $R(s,a)$ is calculated\n",
    "\n",
    "$$\n",
    "R(s,a) = r + \\gamma \\cdot Q(s', \\pi(s'))\n",
    "$$\n",
    "\n",
    "then the update rule:\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow \\alpha \\cdot (Q(s,a) - (r + \\gamma \\cdot Q(s', \\pi(s'))))^2\n",
    "$$\n",
    "\n",
    "To solve this problem, I use the target models, this is the same duplicate of the Critic Model that is training with delay from the actual model, this will ensure more stable training process and prevent explode, when update depends on itself."
   ],
   "id": "fb52477e1df13e9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T22:15:14.037066Z",
     "start_time": "2025-04-15T22:15:14.027675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, batch_size=128, max_size=1024):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_size = max_size\n",
    "\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.dones = []\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.next_states.append(next_state)\n",
    "        self.dones.append(done)\n",
    "\n",
    "        if len(self.states) > self.max_size:\n",
    "            self._pop_oldest()\n",
    "\n",
    "    def _pop_oldest(self):\n",
    "        self.states.pop(0)\n",
    "        self.actions.pop(0)\n",
    "        self.rewards.pop(0)\n",
    "        self.next_states.pop(0)\n",
    "\n",
    "    def enough_sample(self):\n",
    "        return len(self.states) >= self.batch_size\n",
    "\n",
    "    def sample(self):\n",
    "        idxs = random.sample(range(len(self.states)), self.batch_size)\n",
    "        batch = dict(\n",
    "            states=torch.stack([self.states[i] for i in idxs]),\n",
    "            actions=torch.stack([self.actions[i] for i in idxs]),\n",
    "            rewards=torch.stack([self.rewards[i] for i in idxs]),\n",
    "            next_states=torch.stack([self.next_states[i] for i in idxs]),\n",
    "            dones=torch.stack([self.dones[i] for i in idxs])\n",
    "        )\n",
    "        return batch\n"
   ],
   "id": "7c145db16d011d47",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T22:15:14.623559Z",
     "start_time": "2025-04-15T22:15:14.615269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ],
   "id": "240bf1f2415d91e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/pycharm_project_216/AIA/rl/pushT\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T22:15:15.138426Z",
     "start_time": "2025-04-15T22:15:15.129690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for root, dirs, files in os.walk('.'):\n",
    "    for file in files:\n",
    "        print(os.path.join(root, file))"
   ],
   "id": "876e01a73ddd7d94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./trainer.py\n",
      "./Nets.py\n",
      "./main.ipynb\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T22:15:41.337042Z",
     "start_time": "2025-04-15T22:15:41.325778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from AIA.rl.pushT.Nets import Policy, Critic, OUNoise\n",
    "from AIA.rl.pushT.trainer import train_critic, train_policy, soft_update\n",
    "\n",
    "\n",
    "def train(policy, critic, policy_target, critic_target, optimizer_policy, optimizer_critic,\n",
    "          memory, episodes=4000, max_steps=1000, gamma=0.99, tau=0.005, living_cost=-0.001):\n",
    "    env = make_env()\n",
    "\n",
    "    # Initialize noise with linear decay over all episodes\n",
    "    ou_noise = OUNoise(env.action_space.shape[0],\n",
    "                      initial_sigma=25,\n",
    "                      final_sigma=1,\n",
    "                      decay_steps=episodes * 500)  # Decay over total episodes\n",
    "\n",
    "    # Statistics tracking\n",
    "    reward_buffer = []\n",
    "    critic_loss_buffer = []\n",
    "    policy_loss_buffer = []\n",
    "    print_interval = 25\n",
    "    stats_window = 100\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        prev_reward = 0\n",
    "        episode_critic_loss = []\n",
    "        episode_policy_loss = []\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            # Action selection and environment step\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float).to(compute_device) / 512.0\n",
    "                action = policy(state_tensor).cpu().numpy()\n",
    "\n",
    "            # Apply noise with current sigma\n",
    "            noise = ou_noise.sample()\n",
    "\n",
    "            action = action + noise\n",
    "\n",
    "            action_env = np.clip(action, 0, 512)\n",
    "\n",
    "            # Environment interaction\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action_env)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Reward calculation\n",
    "            shaped_reward = 100 * (reward - prev_reward) + living_cost\n",
    "            prev_reward = reward\n",
    "\n",
    "            # Store transition\n",
    "            memory.add(torch.tensor(state, dtype=torch.float)/512.0,\n",
    "                      torch.tensor(action, dtype=torch.float),\n",
    "                      torch.tensor(shaped_reward, dtype=torch.float),\n",
    "                      torch.tensor(next_state, dtype=torch.float)/512.0,\n",
    "                      torch.tensor(int(done)))\n",
    "\n",
    "            episode_reward += shaped_reward\n",
    "            state = next_state\n",
    "\n",
    "            # Training step\n",
    "            if memory.enough_sample():\n",
    "                batch = memory.sample()\n",
    "                states = batch['states'].to(compute_device)\n",
    "                actions = batch['actions'].to(compute_device)\n",
    "                rewards = batch['rewards'].to(compute_device)\n",
    "                next_states = batch['next_states'].to(compute_device)\n",
    "                dones = batch['dones'].to(compute_device)\n",
    "\n",
    "                # Critic update\n",
    "                with torch.no_grad():\n",
    "                    target_actions = policy_target(next_states)\n",
    "                    target_actions = torch.clamp(target_actions, 0, 512)\n",
    "                    target_q = critic_target(torch.cat((next_states, target_actions), 1))\n",
    "                    targets = rewards + gamma * (1 - dones) * target_q.squeeze(-1)  # Match shapes\n",
    "\n",
    "                critic_loss = train_critic(critic, torch.cat((states, actions), 1), targets,\n",
    "                                         optimizer_critic, compute_device)\n",
    "                episode_critic_loss.append(critic_loss)\n",
    "\n",
    "                # Policy update\n",
    "                policy_loss = train_policy(policy, critic, states, optimizer_policy, compute_device)\n",
    "                episode_policy_loss.append(policy_loss)\n",
    "\n",
    "                # Target updates\n",
    "                soft_update(policy_target, policy, tau)\n",
    "                soft_update(critic_target, critic, tau)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Store episode statistics\n",
    "        avg_critic_loss = np.mean(episode_critic_loss) if episode_critic_loss else 0\n",
    "        avg_policy_loss = np.mean(episode_policy_loss) if episode_policy_loss else 0\n",
    "\n",
    "        reward_buffer.append(episode_reward)\n",
    "        critic_loss_buffer.append(avg_critic_loss)\n",
    "        policy_loss_buffer.append(avg_policy_loss)\n",
    "\n",
    "        # Print statistics every 100 episodes\n",
    "        if (episode + 1) % print_interval == 0 or episode == episodes - 1:\n",
    "            start_idx = max(0, len(reward_buffer) - stats_window)\n",
    "\n",
    "            mean_reward = np.mean(reward_buffer[start_idx:])\n",
    "            mean_critic = np.mean(critic_loss_buffer[start_idx:])\n",
    "            mean_policy = np.mean(policy_loss_buffer[start_idx:])\n",
    "\n",
    "            print(f\"Episodes {episode+1}: \"\n",
    "                  f\"Mean Reward: {mean_reward:.2f}, \"\n",
    "                  f\"Critic Loss: {mean_critic:.4f}, \"\n",
    "                  f\"Policy Loss: {mean_policy:.4f}, \"\n",
    "                  f\"Noise Sigma: {ou_noise.current_sigma():.2f}\")\n",
    "\n",
    "    return reward_buffer"
   ],
   "id": "b3495bc45ecb2067",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T22:15:44.070915Z",
     "start_time": "2025-04-15T22:15:44.051084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "policy_model = Policy(env).to(compute_device)\n",
    "critic_model = Critic(env).to(compute_device)\n",
    "policy_target = Policy(env).to(compute_device)\n",
    "critic_target = Critic(env).to(compute_device)\n",
    "policy_target.load_state_dict(policy_model.state_dict())\n",
    "critic_target.load_state_dict(critic_model.state_dict())\n",
    "optimizer_policy = torch.optim.Adam(policy_model.parameters(), lr=1e-4)\n",
    "optimizer_critic = torch.optim.Adam(critic_model.parameters(), lr=1e-3)\n",
    "memory = Memory(batch_size=512, max_size=40000, min_sample=1024)"
   ],
   "id": "d42b30518e149a8e",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T22:20:23.428007Z",
     "start_time": "2025-04-15T22:15:44.923596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train(\n",
    "    policy_model,\n",
    "    critic_model,\n",
    "    policy_target,\n",
    "    critic_target,\n",
    "    optimizer_policy,\n",
    "    optimizer_critic,\n",
    "    memory,\n",
    "    episodes= 10000,\n",
    "    max_steps = 1000,\n",
    "    gamma = 0.99,\n",
    "    tau = 0.005)"
   ],
   "id": "b3feaa16c4ab4271",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes 1: Mean Reward: -138.13, Critic Loss: 0.0000, Policy Loss: 0.0000, Noise Sigma: 2.50\n",
      "Episodes 2: Mean Reward: -300.55, Critic Loss: 0.0000, Policy Loss: 0.0000, Noise Sigma: 2.50\n",
      "Episodes 3: Mean Reward: -231.73, Critic Loss: 0.0000, Policy Loss: 0.0000, Noise Sigma: 2.50\n",
      "Episodes 4: Mean Reward: -256.48, Critic Loss: 0.0000, Policy Loss: 0.0000, Noise Sigma: 2.50\n",
      "Episodes 5: Mean Reward: -306.30, Critic Loss: 0.0000, Policy Loss: 0.0000, Noise Sigma: 2.50\n",
      "Episodes 6: Mean Reward: -296.53, Critic Loss: 0.0000, Policy Loss: 0.0000, Noise Sigma: 2.50\n",
      "Episodes 7: Mean Reward: -273.74, Critic Loss: 0.0000, Policy Loss: 0.0000, Noise Sigma: 2.50\n",
      "Episodes 8: Mean Reward: -296.41, Critic Loss: 0.0000, Policy Loss: 0.0000, Noise Sigma: 2.50\n",
      "Episodes 9: Mean Reward: -288.01, Critic Loss: 0.0000, Policy Loss: 0.0000, Noise Sigma: 2.50\n",
      "Episodes 10: Mean Reward: -303.01, Critic Loss: 0.0000, Policy Loss: 0.0000, Noise Sigma: 2.50\n",
      "Episodes 11: Mean Reward: -325.15, Critic Loss: 0.0000, Policy Loss: 0.0000, Noise Sigma: 2.50\n",
      "Episodes 12: Mean Reward: -341.91, Critic Loss: 0.0000, Policy Loss: 0.0000, Noise Sigma: 2.50\n",
      "Episodes 13: Mean Reward: -360.44, Critic Loss: 5.3256, Policy Loss: 0.2317, Noise Sigma: 2.50\n",
      "Episodes 14: Mean Reward: -334.43, Critic Loss: 7.8865, Policy Loss: 0.4769, Noise Sigma: 2.50\n",
      "Episodes 15: Mean Reward: -342.34, Critic Loss: 9.4550, Policy Loss: 0.7106, Noise Sigma: 2.50\n",
      "Episodes 16: Mean Reward: -332.55, Critic Loss: 10.5038, Policy Loss: 0.9441, Noise Sigma: 2.50\n",
      "Episodes 17: Mean Reward: -344.35, Critic Loss: 11.0947, Policy Loss: 1.1641, Noise Sigma: 2.50\n",
      "Episodes 18: Mean Reward: -332.17, Critic Loss: 11.4551, Policy Loss: 1.4044, Noise Sigma: 2.50\n",
      "Episodes 19: Mean Reward: -313.99, Critic Loss: 11.5205, Policy Loss: 1.6229, Noise Sigma: 2.50\n",
      "Episodes 20: Mean Reward: -312.30, Critic Loss: 11.4390, Policy Loss: 1.8250, Noise Sigma: 2.50\n",
      "Episodes 21: Mean Reward: -310.50, Critic Loss: 11.2767, Policy Loss: 2.0204, Noise Sigma: 2.50\n",
      "Episodes 22: Mean Reward: -303.26, Critic Loss: 11.0927, Policy Loss: 2.2215, Noise Sigma: 2.50\n",
      "Episodes 23: Mean Reward: -309.93, Critic Loss: 10.8438, Policy Loss: 2.4139, Noise Sigma: 2.50\n",
      "Episodes 24: Mean Reward: -306.35, Critic Loss: 10.5949, Policy Loss: 2.6289, Noise Sigma: 2.50\n",
      "Episodes 25: Mean Reward: -303.98, Critic Loss: 10.3493, Policy Loss: 2.8238, Noise Sigma: 2.50\n",
      "Episodes 26: Mean Reward: -315.83, Critic Loss: 10.1850, Policy Loss: 3.0650, Noise Sigma: 2.50\n",
      "Episodes 27: Mean Reward: -324.63, Critic Loss: 10.2233, Policy Loss: 3.3393, Noise Sigma: 2.50\n",
      "Episodes 28: Mean Reward: -334.67, Critic Loss: 10.1075, Policy Loss: 3.6373, Noise Sigma: 2.50\n",
      "Episodes 29: Mean Reward: -333.24, Critic Loss: 9.9943, Policy Loss: 3.9535, Noise Sigma: 2.50\n",
      "Episodes 30: Mean Reward: -325.60, Critic Loss: 9.8562, Policy Loss: 4.2302, Noise Sigma: 2.50\n",
      "Episodes 31: Mean Reward: -317.24, Critic Loss: 9.8449, Policy Loss: 4.4722, Noise Sigma: 2.50\n",
      "Episodes 32: Mean Reward: -318.02, Critic Loss: 9.7997, Policy Loss: 4.6920, Noise Sigma: 2.50\n",
      "Episodes 33: Mean Reward: -312.50, Critic Loss: 9.7661, Policy Loss: 4.9267, Noise Sigma: 2.50\n",
      "Episodes 34: Mean Reward: -317.01, Critic Loss: 9.7112, Policy Loss: 5.1608, Noise Sigma: 2.50\n",
      "Episodes 35: Mean Reward: -319.29, Critic Loss: 9.6440, Policy Loss: 5.3894, Noise Sigma: 2.50\n",
      "Episodes 36: Mean Reward: -318.81, Critic Loss: 9.5874, Policy Loss: 5.6415, Noise Sigma: 2.50\n",
      "Episodes 37: Mean Reward: -325.20, Critic Loss: 9.5191, Policy Loss: 5.9110, Noise Sigma: 2.50\n",
      "Episodes 38: Mean Reward: -320.76, Critic Loss: 9.4436, Policy Loss: 6.1805, Noise Sigma: 2.50\n",
      "Episodes 39: Mean Reward: -318.33, Critic Loss: 9.3971, Policy Loss: 6.4478, Noise Sigma: 2.50\n",
      "Episodes 40: Mean Reward: -313.50, Critic Loss: 9.3300, Policy Loss: 6.7031, Noise Sigma: 2.50\n",
      "Episodes 41: Mean Reward: -317.08, Critic Loss: 9.2772, Policy Loss: 6.9492, Noise Sigma: 2.50\n",
      "Episodes 42: Mean Reward: -325.09, Critic Loss: 9.2175, Policy Loss: 7.2183, Noise Sigma: 2.50\n",
      "Episodes 43: Mean Reward: -331.89, Critic Loss: 9.2381, Policy Loss: 7.5089, Noise Sigma: 2.50\n",
      "Episodes 44: Mean Reward: -327.91, Critic Loss: 9.2087, Policy Loss: 7.7995, Noise Sigma: 2.50\n",
      "Episodes 45: Mean Reward: -323.65, Critic Loss: 9.1742, Policy Loss: 8.0618, Noise Sigma: 2.50\n",
      "Episodes 46: Mean Reward: -331.50, Critic Loss: 9.1310, Policy Loss: 8.3346, Noise Sigma: 2.50\n",
      "Episodes 47: Mean Reward: -335.00, Critic Loss: 9.1197, Policy Loss: 8.6012, Noise Sigma: 2.50\n",
      "Episodes 48: Mean Reward: -337.11, Critic Loss: 9.0807, Policy Loss: 8.8777, Noise Sigma: 2.50\n",
      "Episodes 49: Mean Reward: -337.73, Critic Loss: 9.0569, Policy Loss: 9.1438, Noise Sigma: 2.50\n",
      "Episodes 50: Mean Reward: -338.42, Critic Loss: 9.0902, Policy Loss: 9.3973, Noise Sigma: 2.50\n",
      "Episodes 51: Mean Reward: -333.70, Critic Loss: 9.1067, Policy Loss: 9.6571, Noise Sigma: 2.50\n",
      "Episodes 52: Mean Reward: -334.08, Critic Loss: 9.1298, Policy Loss: 9.9159, Noise Sigma: 2.50\n",
      "Episodes 53: Mean Reward: -334.70, Critic Loss: 9.1475, Policy Loss: 10.1748, Noise Sigma: 2.50\n",
      "Episodes 54: Mean Reward: -335.56, Critic Loss: 9.1577, Policy Loss: 10.4454, Noise Sigma: 2.50\n",
      "Episodes 55: Mean Reward: -337.65, Critic Loss: 9.2115, Policy Loss: 10.7067, Noise Sigma: 2.50\n",
      "Episodes 56: Mean Reward: -339.22, Critic Loss: 9.3548, Policy Loss: 10.9719, Noise Sigma: 2.50\n",
      "Episodes 57: Mean Reward: -343.18, Critic Loss: 9.4739, Policy Loss: 11.2484, Noise Sigma: 2.50\n",
      "Episodes 58: Mean Reward: -342.91, Critic Loss: 9.5582, Policy Loss: 11.5351, Noise Sigma: 2.50\n",
      "Episodes 59: Mean Reward: -337.74, Critic Loss: 9.6442, Policy Loss: 11.8128, Noise Sigma: 2.50\n",
      "Episodes 60: Mean Reward: -334.38, Critic Loss: 9.7367, Policy Loss: 12.0911, Noise Sigma: 2.50\n",
      "Episodes 61: Mean Reward: -336.09, Critic Loss: 9.8337, Policy Loss: 12.3558, Noise Sigma: 2.50\n",
      "Episodes 62: Mean Reward: -331.24, Critic Loss: 9.9064, Policy Loss: 12.6069, Noise Sigma: 2.50\n",
      "Episodes 63: Mean Reward: -335.04, Critic Loss: 10.0175, Policy Loss: 12.8458, Noise Sigma: 2.50\n",
      "Episodes 64: Mean Reward: -331.11, Critic Loss: 10.1446, Policy Loss: 13.0841, Noise Sigma: 2.50\n",
      "Episodes 65: Mean Reward: -335.54, Critic Loss: 10.2590, Policy Loss: 13.3197, Noise Sigma: 2.50\n",
      "Episodes 66: Mean Reward: -330.78, Critic Loss: 10.3974, Policy Loss: 13.5617, Noise Sigma: 2.50\n",
      "Episodes 67: Mean Reward: -327.14, Critic Loss: 10.4961, Policy Loss: 13.7991, Noise Sigma: 2.50\n",
      "Episodes 68: Mean Reward: -322.85, Critic Loss: 10.5940, Policy Loss: 14.0217, Noise Sigma: 2.50\n",
      "Episodes 69: Mean Reward: -328.33, Critic Loss: 10.7058, Policy Loss: 14.2510, Noise Sigma: 2.50\n",
      "Episodes 70: Mean Reward: -329.71, Critic Loss: 10.8285, Policy Loss: 14.4872, Noise Sigma: 2.50\n",
      "Episodes 71: Mean Reward: -329.98, Critic Loss: 10.8787, Policy Loss: 14.7133, Noise Sigma: 2.50\n",
      "Episodes 72: Mean Reward: -330.69, Critic Loss: 10.9758, Policy Loss: 14.9316, Noise Sigma: 2.50\n",
      "Episodes 73: Mean Reward: -329.08, Critic Loss: 11.0341, Policy Loss: 15.1400, Noise Sigma: 2.50\n",
      "Episodes 74: Mean Reward: -325.27, Critic Loss: 11.0605, Policy Loss: 15.3429, Noise Sigma: 2.50\n",
      "Episodes 75: Mean Reward: -323.11, Critic Loss: 11.1288, Policy Loss: 15.5395, Noise Sigma: 2.49\n",
      "Episodes 76: Mean Reward: -320.81, Critic Loss: 11.1643, Policy Loss: 15.7229, Noise Sigma: 2.49\n",
      "Episodes 77: Mean Reward: -317.51, Critic Loss: 11.1838, Policy Loss: 15.9069, Noise Sigma: 2.49\n",
      "Episodes 78: Mean Reward: -318.24, Critic Loss: 11.1994, Policy Loss: 16.0745, Noise Sigma: 2.49\n",
      "Episodes 79: Mean Reward: -321.47, Critic Loss: 11.2142, Policy Loss: 16.2499, Noise Sigma: 2.49\n",
      "Episodes 80: Mean Reward: -324.32, Critic Loss: 11.2268, Policy Loss: 16.4183, Noise Sigma: 2.49\n",
      "Episodes 81: Mean Reward: -325.38, Critic Loss: 11.2450, Policy Loss: 16.5976, Noise Sigma: 2.49\n",
      "Episodes 82: Mean Reward: -326.67, Critic Loss: 11.2480, Policy Loss: 16.7789, Noise Sigma: 2.49\n",
      "Episodes 83: Mean Reward: -323.56, Critic Loss: 11.2441, Policy Loss: 16.9521, Noise Sigma: 2.49\n",
      "Episodes 84: Mean Reward: -321.50, Critic Loss: 11.2690, Policy Loss: 17.1272, Noise Sigma: 2.49\n",
      "Episodes 85: Mean Reward: -325.71, Critic Loss: 11.2921, Policy Loss: 17.3186, Noise Sigma: 2.49\n",
      "Episodes 86: Mean Reward: -328.13, Critic Loss: 11.3084, Policy Loss: 17.4991, Noise Sigma: 2.49\n",
      "Episodes 87: Mean Reward: -326.35, Critic Loss: 11.3295, Policy Loss: 17.6948, Noise Sigma: 2.49\n",
      "Episodes 88: Mean Reward: -326.69, Critic Loss: 11.3552, Policy Loss: 17.8708, Noise Sigma: 2.49\n",
      "Episodes 89: Mean Reward: -328.71, Critic Loss: 11.3754, Policy Loss: 18.0457, Noise Sigma: 2.49\n",
      "Episodes 90: Mean Reward: -326.24, Critic Loss: 11.4020, Policy Loss: 18.2282, Noise Sigma: 2.49\n",
      "Episodes 91: Mean Reward: -327.90, Critic Loss: 11.4216, Policy Loss: 18.4114, Noise Sigma: 2.49\n",
      "Episodes 92: Mean Reward: -327.73, Critic Loss: 11.4465, Policy Loss: 18.5963, Noise Sigma: 2.49\n",
      "Episodes 93: Mean Reward: -331.30, Critic Loss: 11.4663, Policy Loss: 18.7824, Noise Sigma: 2.49\n",
      "Episodes 94: Mean Reward: -330.96, Critic Loss: 11.4873, Policy Loss: 18.9733, Noise Sigma: 2.49\n",
      "Episodes 95: Mean Reward: -336.75, Critic Loss: 11.5028, Policy Loss: 19.1571, Noise Sigma: 2.49\n",
      "Episodes 96: Mean Reward: -336.31, Critic Loss: 11.5332, Policy Loss: 19.3549, Noise Sigma: 2.49\n",
      "Episodes 97: Mean Reward: -335.15, Critic Loss: 11.5605, Policy Loss: 19.5453, Noise Sigma: 2.49\n",
      "Episodes 98: Mean Reward: -337.65, Critic Loss: 11.5702, Policy Loss: 19.7345, Noise Sigma: 2.49\n",
      "Episodes 99: Mean Reward: -335.92, Critic Loss: 11.6258, Policy Loss: 19.9325, Noise Sigma: 2.49\n",
      "Episodes 100: Mean Reward: -335.35, Critic Loss: 11.6736, Policy Loss: 20.1175, Noise Sigma: 2.49\n",
      "Episodes 101: Mean Reward: -336.78, Critic Loss: 11.8453, Policy Loss: 20.4901, Noise Sigma: 2.49\n",
      "Episodes 102: Mean Reward: -336.21, Critic Loss: 12.0088, Policy Loss: 20.8709, Noise Sigma: 2.49\n",
      "Episodes 103: Mean Reward: -336.46, Critic Loss: 12.1758, Policy Loss: 21.2494, Noise Sigma: 2.49\n",
      "Episodes 104: Mean Reward: -333.59, Critic Loss: 12.3431, Policy Loss: 21.6262, Noise Sigma: 2.49\n",
      "Episodes 105: Mean Reward: -330.03, Critic Loss: 12.5246, Policy Loss: 21.9970, Noise Sigma: 2.49\n",
      "Episodes 106: Mean Reward: -332.22, Critic Loss: 12.6768, Policy Loss: 22.3822, Noise Sigma: 2.49\n",
      "Episodes 107: Mean Reward: -331.77, Critic Loss: 12.8240, Policy Loss: 22.7517, Noise Sigma: 2.49\n",
      "Episodes 108: Mean Reward: -333.00, Critic Loss: 12.9743, Policy Loss: 23.1197, Noise Sigma: 2.49\n",
      "Episodes 109: Mean Reward: -337.63, Critic Loss: 13.1100, Policy Loss: 23.5145, Noise Sigma: 2.49\n",
      "Episodes 110: Mean Reward: -333.84, Critic Loss: 13.2726, Policy Loss: 23.9112, Noise Sigma: 2.49\n",
      "Episodes 111: Mean Reward: -329.39, Critic Loss: 13.4541, Policy Loss: 24.3059, Noise Sigma: 2.49\n",
      "Episodes 112: Mean Reward: -325.18, Critic Loss: 13.6114, Policy Loss: 24.7018, Noise Sigma: 2.49\n",
      "Episodes 113: Mean Reward: -325.37, Critic Loss: 13.0817, Policy Loss: 25.0669, Noise Sigma: 2.49\n",
      "Episodes 114: Mean Reward: -326.14, Critic Loss: 12.8169, Policy Loss: 25.4276, Noise Sigma: 2.49\n",
      "Episodes 115: Mean Reward: -323.57, Critic Loss: 12.6587, Policy Loss: 25.7853, Noise Sigma: 2.49\n",
      "Episodes 116: Mean Reward: -323.14, Critic Loss: 12.5460, Policy Loss: 26.1405, Noise Sigma: 2.49\n",
      "Episodes 117: Mean Reward: -319.68, Critic Loss: 12.5007, Policy Loss: 26.4911, Noise Sigma: 2.49\n",
      "Episodes 118: Mean Reward: -319.40, Critic Loss: 12.4747, Policy Loss: 26.8330, Noise Sigma: 2.49\n",
      "Episodes 119: Mean Reward: -322.06, Critic Loss: 12.4868, Policy Loss: 27.1768, Noise Sigma: 2.49\n",
      "Episodes 120: Mean Reward: -324.65, Critic Loss: 12.5343, Policy Loss: 27.5225, Noise Sigma: 2.49\n",
      "Episodes 121: Mean Reward: -328.44, Critic Loss: 12.5852, Policy Loss: 27.8582, Noise Sigma: 2.49\n",
      "Episodes 122: Mean Reward: -333.23, Critic Loss: 12.6453, Policy Loss: 28.1946, Noise Sigma: 2.49\n",
      "Episodes 123: Mean Reward: -332.16, Critic Loss: 12.7337, Policy Loss: 28.5285, Noise Sigma: 2.49\n",
      "Episodes 124: Mean Reward: -331.51, Critic Loss: 12.8119, Policy Loss: 28.8513, Noise Sigma: 2.49\n",
      "Episodes 125: Mean Reward: -335.64, Critic Loss: 12.9058, Policy Loss: 29.1826, Noise Sigma: 2.49\n",
      "Episodes 126: Mean Reward: -333.77, Critic Loss: 12.9797, Policy Loss: 29.5006, Noise Sigma: 2.49\n",
      "Episodes 127: Mean Reward: -330.79, Critic Loss: 13.0027, Policy Loss: 29.8083, Noise Sigma: 2.49\n",
      "Episodes 128: Mean Reward: -326.64, Critic Loss: 13.0830, Policy Loss: 30.0994, Noise Sigma: 2.49\n",
      "Episodes 129: Mean Reward: -326.19, Critic Loss: 13.1444, Policy Loss: 30.3883, Noise Sigma: 2.49\n",
      "Episodes 130: Mean Reward: -329.09, Critic Loss: 13.2373, Policy Loss: 30.6821, Noise Sigma: 2.49\n",
      "Episodes 131: Mean Reward: -329.41, Critic Loss: 13.2872, Policy Loss: 30.9752, Noise Sigma: 2.49\n",
      "Episodes 132: Mean Reward: -330.26, Critic Loss: 13.3254, Policy Loss: 31.2742, Noise Sigma: 2.49\n",
      "Episodes 133: Mean Reward: -331.61, Critic Loss: 13.3647, Policy Loss: 31.5638, Noise Sigma: 2.49\n",
      "Episodes 134: Mean Reward: -327.84, Critic Loss: 13.4176, Policy Loss: 31.8435, Noise Sigma: 2.49\n",
      "Episodes 135: Mean Reward: -326.84, Critic Loss: 13.4920, Policy Loss: 32.1309, Noise Sigma: 2.49\n",
      "Episodes 136: Mean Reward: -330.13, Critic Loss: 13.5465, Policy Loss: 32.4048, Noise Sigma: 2.49\n",
      "Episodes 137: Mean Reward: -329.68, Critic Loss: 13.5930, Policy Loss: 32.6641, Noise Sigma: 2.49\n",
      "Episodes 138: Mean Reward: -334.14, Critic Loss: 13.6692, Policy Loss: 32.9204, Noise Sigma: 2.49\n",
      "Episodes 139: Mean Reward: -332.86, Critic Loss: 13.7254, Policy Loss: 33.1686, Noise Sigma: 2.49\n",
      "Episodes 140: Mean Reward: -335.92, Critic Loss: 13.7874, Policy Loss: 33.4242, Noise Sigma: 2.49\n",
      "Episodes 141: Mean Reward: -337.02, Critic Loss: 13.8487, Policy Loss: 33.6793, Noise Sigma: 2.49\n",
      "Episodes 142: Mean Reward: -331.44, Critic Loss: 13.9196, Policy Loss: 33.9203, Noise Sigma: 2.49\n",
      "Episodes 143: Mean Reward: -327.30, Critic Loss: 13.9475, Policy Loss: 34.1483, Noise Sigma: 2.49\n",
      "Episodes 144: Mean Reward: -332.14, Critic Loss: 13.9948, Policy Loss: 34.3715, Noise Sigma: 2.49\n",
      "Episodes 145: Mean Reward: -335.00, Critic Loss: 14.0532, Policy Loss: 34.6165, Noise Sigma: 2.49\n",
      "Episodes 146: Mean Reward: -329.26, Critic Loss: 14.1179, Policy Loss: 34.8460, Noise Sigma: 2.49\n",
      "Episodes 147: Mean Reward: -325.40, Critic Loss: 14.1646, Policy Loss: 35.0691, Noise Sigma: 2.49\n",
      "Episodes 148: Mean Reward: -328.31, Critic Loss: 14.2658, Policy Loss: 35.2935, Noise Sigma: 2.49\n",
      "Episodes 149: Mean Reward: -330.42, Critic Loss: 14.3381, Policy Loss: 35.5135, Noise Sigma: 2.49\n",
      "Episodes 150: Mean Reward: -331.59, Critic Loss: 14.3726, Policy Loss: 35.7595, Noise Sigma: 2.49\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[57]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpolicy_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcritic_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpolicy_target\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcritic_target\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptimizer_policy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptimizer_critic\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepisodes\u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m10000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_steps\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0.99\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtau\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0.005\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[55]\u001B[39m\u001B[32m, line 43\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(policy, critic, policy_target, critic_target, optimizer_policy, optimizer_critic, memory, episodes, max_steps, gamma, tau, living_cost)\u001B[39m\n\u001B[32m     40\u001B[39m action_env = action  \u001B[38;5;66;03m# Directly use clipped action\u001B[39;00m\n\u001B[32m     42\u001B[39m \u001B[38;5;66;03m# Environment interaction\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m43\u001B[39m next_state, reward, terminated, truncated, _ = \u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction_env\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     44\u001B[39m done = terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n\u001B[32m     46\u001B[39m \u001B[38;5;66;03m# Reward calculation\u001B[39;00m\n\u001B[32m     47\u001B[39m \u001B[38;5;66;03m# shaped_reward = 100 * (reward - prev_reward) + living_cost\u001B[39;00m\n\u001B[32m     48\u001B[39m \u001B[38;5;66;03m# prev_reward = reward\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/gymnasium/wrappers/common.py:125\u001B[39m, in \u001B[36mTimeLimit.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    112\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstep\u001B[39m(\n\u001B[32m    113\u001B[39m     \u001B[38;5;28mself\u001B[39m, action: ActType\n\u001B[32m    114\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[ObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[32m    115\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[32m    116\u001B[39m \n\u001B[32m    117\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    123\u001B[39m \n\u001B[32m    124\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     observation, reward, terminated, truncated, info = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    126\u001B[39m     \u001B[38;5;28mself\u001B[39m._elapsed_steps += \u001B[32m1\u001B[39m\n\u001B[32m    128\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._elapsed_steps >= \u001B[38;5;28mself\u001B[39m._max_episode_steps:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/gymnasium/wrappers/common.py:393\u001B[39m, in \u001B[36mOrderEnforcing.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    391\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._has_reset:\n\u001B[32m    392\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[33m\"\u001B[39m\u001B[33mCannot call env.step() before calling env.reset()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m393\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/gymnasium/core.py:327\u001B[39m, in \u001B[36mWrapper.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    323\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstep\u001B[39m(\n\u001B[32m    324\u001B[39m     \u001B[38;5;28mself\u001B[39m, action: WrapperActType\n\u001B[32m    325\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[32m    326\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m327\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/gymnasium/wrappers/common.py:285\u001B[39m, in \u001B[36mPassiveEnvChecker.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    283\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m.env, action)\n\u001B[32m    284\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m285\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/gymnasium/envs/box2d/lunar_lander.py:665\u001B[39m, in \u001B[36mLunarLander.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    662\u001B[39m     reward = +\u001B[32m100\u001B[39m\n\u001B[32m    664\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.render_mode == \u001B[33m\"\u001B[39m\u001B[33mhuman\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m665\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    666\u001B[39m \u001B[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001B[39;00m\n\u001B[32m    667\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m np.array(state, dtype=np.float32), reward, terminated, \u001B[38;5;28;01mFalse\u001B[39;00m, {}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/gymnasium/envs/box2d/lunar_lander.py:778\u001B[39m, in \u001B[36mLunarLander.render\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    776\u001B[39m     \u001B[38;5;28mself\u001B[39m.screen.blit(\u001B[38;5;28mself\u001B[39m.surf, (\u001B[32m0\u001B[39m, \u001B[32m0\u001B[39m))\n\u001B[32m    777\u001B[39m     pygame.event.pump()\n\u001B[32m--> \u001B[39m\u001B[32m778\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrender_fps\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    779\u001B[39m     pygame.display.flip()\n\u001B[32m    780\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.render_mode == \u001B[33m\"\u001B[39m\u001B[33mrgb_array\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_env = make_env(render_mode=\"rgb_array\")\n",
    "\n",
    "state, _ = test_env.reset()\n",
    "\n",
    "for _ in range(50):\n",
    "    sleep(0.2)\n",
    "\n",
    "    input = torch.tensor(state, dtype=torch.float).to(compute_device) / 512.0\n",
    "\n",
    "    action = policy_model(input)\n",
    "\n",
    "    new_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "\n",
    "    state = new_state\n",
    "\n",
    "    # image = test_env.render()\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = test_env.reset()\n",
    "\n",
    "test_env.close()"
   ],
   "id": "80048ddd1c6b3bc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_env.close()",
   "id": "5e779b2cc96a5b37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f0e301aa608bf378"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
