{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T11:20:11.911111Z",
     "start_time": "2025-04-15T11:20:11.258683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from math import gamma\n",
    "from time import sleep\n",
    "\n",
    "import torch\n",
    "\n",
    "compute_device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "cpu_device = torch.device('cpu')"
   ],
   "id": "d4bd6aaaebb2d9cf",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T11:20:13.364269Z",
     "start_time": "2025-04-15T11:20:13.344074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "81814bd45a321a2e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T11:20:13.657344Z",
     "start_time": "2025-04-15T11:20:13.650925Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.is_available()",
   "id": "ce9d46d06fc032a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Env",
   "id": "a340e21036db8935"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T11:20:14.373431Z",
     "start_time": "2025-04-15T11:20:14.367577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_env(render_mode=\"rgb_array\"):\n",
    "    env = gym.make(\"gym_pusht/PushT-v0\", render_mode=render_mode)\n",
    "    return env"
   ],
   "id": "846e4b69a31c1589",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T11:20:14.722167Z",
     "start_time": "2025-04-15T11:20:14.674858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import gym_pusht\n",
    "from time import sleep\n",
    "\n",
    "env = make_env()"
   ],
   "id": "da2d78f75a51537c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T11:20:27.743336Z",
     "start_time": "2025-04-15T11:20:17.133564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_env = make_env(render_mode=\"human\")\n",
    "\n",
    "observation, info = test_env.reset()\n",
    "\n",
    "for _ in range(50):\n",
    "    sleep(0.2)\n",
    "    action = test_env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = test_env.step(action)\n",
    "    image = test_env.render()\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = test_env.reset()\n",
    "\n",
    "test_env.close()"
   ],
   "id": "6c1c6d87acea3b25",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR is invalid or not set in the environment.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T11:20:27.891087Z",
     "start_time": "2025-04-15T11:20:27.884938Z"
    }
   },
   "cell_type": "code",
   "source": "test_env.close()",
   "id": "7a8b77d57915fb1e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Implement the deepq Learning to compete the task\n",
    "\n",
    "# Part 1: Numerical PushT state"
   ],
   "id": "b4bbb7452875153a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Observation Space\n",
    "\n",
    "If obs_type is set to state, the observation space is a 5-dimensional vector representing the state of the environment: [agent_x, agent_y, block_x, block_y, block_angle]. The values are in the range [0, 512] for the agent and block positions and [0, 2*pi] for the block angle.\n",
    "\n",
    "If obs_type is set to environment_state_agent_pos the observation space is a dictionary with: - environment_state: 16-dimensional vector representing the keypoint locations of the T (in [x0, y0, x1, y1, ...] format). The values are in the range [0, 512]. - agent_pos: A 2-dimensional vector representing the position of the robot end-effector.\n",
    "\n",
    "If obs_type is set to pixels, the observation space is a 96x96 RGB image of the environment."
   ],
   "id": "facbb94942ef1469"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T11:20:34.335884Z",
     "start_time": "2025-04-15T11:20:34.329010Z"
    }
   },
   "cell_type": "code",
   "source": "env.observation_space",
   "id": "c90e3fcd1c3a5c9a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0.0, [512.         512.         512.         512.           6.28318531], (5,), float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The Policy model is designed to give the action $a$ given the state $s$.\n",
    "\n",
    "input:\n",
    "\n",
    "$$\n",
    "[agent_x, agent_y, t_x, t_y, t_{angle}]\n",
    "$$\n",
    "\n",
    "output\n",
    "\n",
    "$$\n",
    "[move_x, move_y]\n",
    "$$\n",
    "\n",
    "\n",
    "so the model tells\n",
    "\n",
    "$$\n",
    "a = P(s)\n",
    "$$"
   ],
   "id": "e6c91e3c3127892"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T11:20:35.162007Z",
     "start_time": "2025-04-15T11:20:35.155602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from torch import nn\n",
    "# import torch.nn.functional as F\n",
    "#\n",
    "#\n",
    "# class Policy(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Policy, self).__init__()\n",
    "#         self.fc1 = nn.Linear(env.observation_space.shape[0], 64)\n",
    "#         self.fc2 = nn.Linear(64, 64)\n",
    "#         self.fc3 = nn.Linear(64, 2)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#\n",
    "#         return x\n",
    "# model = Policy()\n",
    "# input = torch.rand((4, 5))\n",
    "#\n",
    "# input\n",
    "# output = model(input)\n",
    "#\n",
    "# print(output.shape)\n",
    "# output.detach().numpy()"
   ],
   "id": "8b8fc1008de91182",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T11:20:35.673225Z",
     "start_time": "2025-04-15T11:20:35.667112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class Critic(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Critic, self).__init__()\n",
    "#         self.fc1 = nn.Linear(env.observation_space.shape[0] + env.action_space.shape[0], 64)\n",
    "#         self.fc2 = nn.Linear(64, 64)\n",
    "#         self.fc3 = nn.Linear(64, 1)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = F.relu(self.fc3(x))\n",
    "#\n",
    "#         return x\n",
    "# model = Critic()\n",
    "# input = torch.rand((4, 7))\n",
    "#\n",
    "# input\n",
    "# output = model(input)\n",
    "#\n",
    "# print(output.shape)\n",
    "# output.detach().numpy()"
   ],
   "id": "10d579061cb91f02",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Critic model is aimed to estimate\n",
    "\n",
    "$$\n",
    "Q(s,a)\n",
    "$$\n",
    "\n",
    "it takes concat input $[s,a]$, more detailed\n",
    "\n",
    "input\n",
    "\n",
    "$$\n",
    "[agent_x, agent_y, t_x, t_y, t_{angle}, move_x, move_y]\n",
    "$$\n",
    "\n",
    "output\n",
    "\n",
    "$$\n",
    "score\n",
    "$$\n",
    "\n",
    "\n",
    "so it give single estimate of $Q$ value\n",
    "\n",
    "$$\n",
    "score = Q(s,a)\n",
    "$$\n",
    "\n",
    "The Policy model is trained using Critic model\n",
    "\n",
    "Since we cant directly access the action value $Q(s,a)$ we use the critic model that gives estimate $Q'(s,a)$ to understand the value of this state action pair. Then we could use this estimate to compute the loss\n",
    "\n",
    "\n",
    "$$\n",
    "loss = -Q'(s,a)\n",
    "$$\n",
    "\n",
    "\\- is used because optimization tasks aim to minimize function, hence minimizing -f is equivalent to maximizing f"
   ],
   "id": "5ca3ca89c592e1ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T11:20:36.697674Z",
     "start_time": "2025-04-15T11:20:36.691379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def train_policy(policy_model, critic_model, input, optimizer):\n",
    "#     \"\"\"\n",
    "#\n",
    "#     Args:\n",
    "#         policy_model: Model to update\n",
    "#         critic_model: Critic model to compute the value of the proposed action\n",
    "#         input: the input of shape [batch_size, 5] = (batch_size, observation_space.shape)\n",
    "#         optimizer: optimizer for the policy model\n",
    "#\n",
    "#     Returns:\n",
    "#\n",
    "#     \"\"\"\n",
    "#     optimizer.zero_grad()\n",
    "#\n",
    "#     input = input.to(compute_device)\n",
    "#\n",
    "#     output = policy_model(input)\n",
    "#\n",
    "#     critic_input = torch.cat((input, output), dim=1)\n",
    "#\n",
    "#     score = critic_model(critic_input)\n",
    "#\n",
    "#     loss = -score\n",
    "#\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#\n",
    "#     return loss.item()"
   ],
   "id": "9143c5e212e3f795",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For training critic, we use the actual reward we received from the env, bootstrap with critic model and train it with basic MSE loss",
   "id": "4f56be6e07fe0c4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T11:20:37.926246Z",
     "start_time": "2025-04-15T11:20:37.920185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def train_critic(critic_model, input, target, optimizer):\n",
    "#     \"\"\"\n",
    "#     Train function that run one update on the critic network using batch of inputs.\n",
    "#\n",
    "#     Args:\n",
    "#         critic_model: Model to update\n",
    "#         input: the input of shape [batch_size, 7] = (batch_size, observation_space.shape + action_space.shape)\n",
    "#         target: Is the target reward we received from env and critic_estimate model\n",
    "#         optimizer: optimizer for the critic model\n",
    "#\n",
    "#     Returns:\n",
    "#\n",
    "#     \"\"\"\n",
    "#     optimizer.zero_grad()\n",
    "#     criterion = nn.MSELoss()\n",
    "#\n",
    "#     critic_model.train()\n",
    "#\n",
    "#     input = input.to(compute_device)\n",
    "#\n",
    "#     output = critic_model(input)\n",
    "#\n",
    "#     loss = criterion(output, target)\n",
    "#\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#\n",
    "#     return loss.item()"
   ],
   "id": "931a1849e75ffd4b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "so in my approach the policy actually changes, but I could ignore the fact that actions were recorded under another distribution because I am using Q, which gives some level of abstraction which is stable for the environment and any optimal policy will converge to identical function. Then the agent here is just the another function that is trained on this level of the representation of the environment. If I had the training process without Q where the policy is responsible to somehow incorporate the knowledge of the environment inside itself then I need to also think of sampling, because on the interpretation level the actions were done on that perception of env, which changed with policy.\n",
    "\n",
    "this is idea of the model-based and model-free and particaul;larty off-policy and on-policy in the fact that env is not encoded in the model"
   ],
   "id": "c3324e470ba656eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here comes the problem since the update rule is based on MSE of the value\n",
    "$$\n",
    "(Q(s,a), R(s,a))\n",
    "$$\n",
    "\n",
    "where target $R(s,a)$ is calculated\n",
    "\n",
    "$$\n",
    "R(s,a) = r + \\gamma \\cdot Q(s', \\pi(s'))\n",
    "$$\n",
    "\n",
    "then the update rule:\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow \\alpha \\cdot (Q(s,a) - (r + \\gamma \\cdot Q(s', \\pi(s'))))^2\n",
    "$$\n",
    "\n",
    "To solve this problem, I use the target models, this is the same duplicate of the Critic Model that is training with delay from the actual model, this will ensure more stable training process and prevent explode, when update depends on itself."
   ],
   "id": "fb52477e1df13e9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T11:20:39.566415Z",
     "start_time": "2025-04-15T11:20:39.558731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, batch_size=128, max_size=1024, min_sample=4096):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_size = max_size\n",
    "        self.min_sample = min_sample\n",
    "\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.dones = []\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.next_states.append(next_state)\n",
    "        self.dones.append(done)\n",
    "\n",
    "        if len(self.states) > self.max_size:\n",
    "            self._pop_oldest()\n",
    "\n",
    "    def _pop_oldest(self):\n",
    "        self.states.pop(0)\n",
    "        self.actions.pop(0)\n",
    "        self.rewards.pop(0)\n",
    "        self.next_states.pop(0)\n",
    "\n",
    "    def enough_sample(self):\n",
    "        return len(self.states) >= self.min_sample\n",
    "\n",
    "    def sample(self):\n",
    "        idxs = random.sample(range(len(self.states)), self.batch_size)\n",
    "        batch = dict(\n",
    "            states=torch.stack([self.states[i] for i in idxs]),\n",
    "            actions=torch.stack([self.actions[i] for i in idxs]),\n",
    "            rewards=torch.stack([self.rewards[i] for i in idxs]),\n",
    "            next_states=torch.stack([self.next_states[i] for i in idxs]),\n",
    "            dones=torch.stack([self.dones[i] for i in idxs])\n",
    "        )\n",
    "        return batch\n"
   ],
   "id": "7c145db16d011d47",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T11:29:41.030806Z",
     "start_time": "2025-04-15T11:29:41.024249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ],
   "id": "240bf1f2415d91e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/pycharm_project_216/AIA/rl/pushT\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T11:29:41.571758Z",
     "start_time": "2025-04-15T11:29:41.565479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for root, dirs, files in os.walk('.'):\n",
    "    for file in files:\n",
    "        print(os.path.join(root, file))"
   ],
   "id": "876e01a73ddd7d94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./trainer.py\n",
      "./Nets.py\n",
      "./main.ipynb\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T11:29:56.423938Z",
     "start_time": "2025-04-15T11:29:56.414039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from AIA.rl.pushT.Nets import Policy, Critic, OUNoise\n",
    "from AIA.rl.pushT.trainer import train_critic, train_policy, soft_update\n",
    "\n",
    "\n",
    "def train(policy, critic, policy_target, critic_target, optimizer_policy, optimizer_critic,\n",
    "          memory, episodes=4000, max_steps=1000, gamma=0.99, tau=0.005, living_cost=-0.001):\n",
    "    env = make_env()\n",
    "\n",
    "    # Initialize noise with linear decay over all episodes\n",
    "    ou_noise = OUNoise(env.action_space.shape[0],\n",
    "                      initial_sigma=25,\n",
    "                      final_sigma=1,\n",
    "                      decay_steps=episodes * 500)  # Decay over total episodes\n",
    "\n",
    "    # Statistics tracking\n",
    "    reward_buffer = []\n",
    "    critic_loss_buffer = []\n",
    "    policy_loss_buffer = []\n",
    "    print_interval = 25\n",
    "    stats_window = 100\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        prev_reward = 0\n",
    "        episode_critic_loss = []\n",
    "        episode_policy_loss = []\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            # Action selection and environment step\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float).to(compute_device) / 512.0\n",
    "                action = policy(state_tensor).cpu().numpy()\n",
    "\n",
    "            # Apply noise with current sigma\n",
    "            noise = ou_noise.sample()\n",
    "\n",
    "            action = action + noise\n",
    "\n",
    "            action_env = np.clip(action, 0, 512)\n",
    "\n",
    "            # Environment interaction\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action_env)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Reward calculation\n",
    "            shaped_reward = 100 * (reward - prev_reward) + living_cost\n",
    "            prev_reward = reward\n",
    "\n",
    "            # Store transition\n",
    "            memory.add(torch.tensor(state, dtype=torch.float)/512.0,\n",
    "                      torch.tensor(action, dtype=torch.float),\n",
    "                      torch.tensor(shaped_reward, dtype=torch.float),\n",
    "                      torch.tensor(next_state, dtype=torch.float)/512.0,\n",
    "                      torch.tensor(int(done)))\n",
    "\n",
    "            episode_reward += shaped_reward\n",
    "            state = next_state\n",
    "\n",
    "            # Training step\n",
    "            if memory.enough_sample():\n",
    "                batch = memory.sample()\n",
    "                states = batch['states'].to(compute_device)\n",
    "                actions = batch['actions'].to(compute_device)\n",
    "                rewards = batch['rewards'].to(compute_device)\n",
    "                next_states = batch['next_states'].to(compute_device)\n",
    "                dones = batch['dones'].to(compute_device)\n",
    "\n",
    "                # Critic update\n",
    "                with torch.no_grad():\n",
    "                    target_actions = policy_target(next_states)\n",
    "                    target_actions = torch.clamp(target_actions, 0, 512)\n",
    "                    target_q = critic_target(torch.cat((next_states, target_actions), 1))\n",
    "                    targets = rewards + gamma * (1 - dones) * target_q.squeeze(-1)  # Match shapes\n",
    "\n",
    "                critic_loss = train_critic(critic, torch.cat((states, actions), 1), targets,\n",
    "                                         optimizer_critic, compute_device)\n",
    "                episode_critic_loss.append(critic_loss)\n",
    "\n",
    "                # Policy update\n",
    "                policy_loss = train_policy(policy, critic, states, optimizer_policy, compute_device)\n",
    "                episode_policy_loss.append(policy_loss)\n",
    "\n",
    "                # Target updates\n",
    "                soft_update(policy_target, policy, tau)\n",
    "                soft_update(critic_target, critic, tau)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Store episode statistics\n",
    "        avg_critic_loss = np.mean(episode_critic_loss) if episode_critic_loss else 0\n",
    "        avg_policy_loss = np.mean(episode_policy_loss) if episode_policy_loss else 0\n",
    "\n",
    "        reward_buffer.append(episode_reward)\n",
    "        critic_loss_buffer.append(avg_critic_loss)\n",
    "        policy_loss_buffer.append(avg_policy_loss)\n",
    "\n",
    "        # Print statistics every 100 episodes\n",
    "        if (episode + 1) % print_interval == 0 or episode == episodes - 1:\n",
    "            start_idx = max(0, len(reward_buffer) - stats_window)\n",
    "\n",
    "            mean_reward = np.mean(reward_buffer[start_idx:])\n",
    "            mean_critic = np.mean(critic_loss_buffer[start_idx:])\n",
    "            mean_policy = np.mean(policy_loss_buffer[start_idx:])\n",
    "\n",
    "            print(f\"Episodes {episode+1}: \"\n",
    "                  f\"Mean Reward: {mean_reward:.2f}, \"\n",
    "                  f\"Critic Loss: {mean_critic:.4f}, \"\n",
    "                  f\"Policy Loss: {mean_policy:.4f}, \"\n",
    "                  f\"Noise Sigma: {ou_noise.current_sigma():.2f}\")\n",
    "\n",
    "    return reward_buffer"
   ],
   "id": "b3495bc45ecb2067",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T11:29:58.867431Z",
     "start_time": "2025-04-15T11:29:58.340691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "policy_model = Policy(env).to(compute_device)\n",
    "critic_model = Critic(env).to(compute_device)\n",
    "policy_target = Policy(env).to(compute_device)\n",
    "critic_target = Critic(env).to(compute_device)\n",
    "policy_target.load_state_dict(policy_model.state_dict())\n",
    "critic_target.load_state_dict(critic_model.state_dict())\n",
    "optimizer_policy = torch.optim.Adam(policy_model.parameters(), lr=1e-4)\n",
    "optimizer_critic = torch.optim.Adam(critic_model.parameters(), lr=1e-3)\n",
    "memory = Memory(batch_size=512, max_size=40000, min_sample=1024)"
   ],
   "id": "d42b30518e149a8e",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:16:38.003478Z",
     "start_time": "2025-04-15T11:30:03.274929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train(\n",
    "    policy_model,\n",
    "    critic_model,\n",
    "    policy_target,\n",
    "    critic_target,\n",
    "    optimizer_policy,\n",
    "    optimizer_critic,\n",
    "    memory,\n",
    "    episodes= 10000,\n",
    "    max_steps = 1000,\n",
    "    gamma = 0.99,\n",
    "    tau = 0.005)"
   ],
   "id": "b3feaa16c4ab4271",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes 25: Mean Reward: 5.40, Critic Loss: 0.3073, Policy Loss: -0.2939, Noise Sigma: 24.96\n",
      "Episodes 50: Mean Reward: 4.00, Critic Loss: 0.5710, Policy Loss: -3.9498, Noise Sigma: 24.93\n",
      "Episodes 75: Mean Reward: 4.87, Critic Loss: 1.8122, Policy Loss: -12.9958, Noise Sigma: 24.89\n",
      "Episodes 100: Mean Reward: 4.90, Critic Loss: 3.0226, Policy Loss: -20.5460, Noise Sigma: 24.86\n",
      "Episodes 125: Mean Reward: 5.33, Critic Loss: 4.4704, Policy Loss: -30.8719, Noise Sigma: 24.82\n",
      "Episodes 150: Mean Reward: 6.97, Critic Loss: 5.7989, Policy Loss: -39.2958, Noise Sigma: 24.78\n",
      "Episodes 175: Mean Reward: 6.66, Critic Loss: 6.0953, Policy Loss: -41.3088, Noise Sigma: 24.75\n",
      "Episodes 200: Mean Reward: 6.55, Critic Loss: 9.6870, Policy Loss: -47.9146, Noise Sigma: 24.71\n",
      "Episodes 225: Mean Reward: 5.34, Critic Loss: 28.6211, Policy Loss: -73.1152, Noise Sigma: 24.68\n",
      "Episodes 250: Mean Reward: 3.94, Critic Loss: 42.7442, Policy Loss: -95.3980, Noise Sigma: 24.64\n",
      "Episodes 275: Mean Reward: 3.00, Critic Loss: 48.9606, Policy Loss: -108.3112, Noise Sigma: 24.60\n",
      "Episodes 300: Mean Reward: 3.51, Critic Loss: 47.4854, Policy Loss: -106.4900, Noise Sigma: 24.57\n",
      "Episodes 325: Mean Reward: 4.46, Critic Loss: 29.0372, Policy Loss: -81.9844, Noise Sigma: 24.53\n",
      "Episodes 350: Mean Reward: 3.88, Critic Loss: 14.6495, Policy Loss: -58.0565, Noise Sigma: 24.50\n",
      "Episodes 375: Mean Reward: 3.77, Critic Loss: 8.4099, Policy Loss: -43.2462, Noise Sigma: 24.46\n",
      "Episodes 400: Mean Reward: 2.36, Critic Loss: 6.6875, Policy Loss: -35.7835, Noise Sigma: 24.42\n",
      "Episodes 425: Mean Reward: 0.99, Critic Loss: 6.7611, Policy Loss: -32.2839, Noise Sigma: 24.39\n",
      "Episodes 450: Mean Reward: 1.38, Critic Loss: 7.7754, Policy Loss: -30.8297, Noise Sigma: 24.35\n",
      "Episodes 475: Mean Reward: 2.19, Critic Loss: 12.1577, Policy Loss: -33.7328, Noise Sigma: 24.32\n",
      "Episodes 500: Mean Reward: 2.98, Critic Loss: 15.7554, Policy Loss: -39.9735, Noise Sigma: 24.28\n",
      "Episodes 525: Mean Reward: 4.40, Critic Loss: 18.3956, Policy Loss: -47.4383, Noise Sigma: 24.24\n",
      "Episodes 550: Mean Reward: 5.03, Critic Loss: 19.5160, Policy Loss: -53.7920, Noise Sigma: 24.21\n",
      "Episodes 575: Mean Reward: 5.51, Critic Loss: 16.0160, Policy Loss: -53.7212, Noise Sigma: 24.17\n",
      "Episodes 600: Mean Reward: 5.17, Critic Loss: 12.0983, Policy Loss: -48.6570, Noise Sigma: 24.14\n",
      "Episodes 625: Mean Reward: 4.44, Critic Loss: 11.0957, Policy Loss: -46.6975, Noise Sigma: 24.10\n",
      "Episodes 650: Mean Reward: 5.05, Critic Loss: 11.4842, Policy Loss: -46.0564, Noise Sigma: 24.06\n",
      "Episodes 675: Mean Reward: 4.50, Critic Loss: 12.0022, Policy Loss: -46.2942, Noise Sigma: 24.03\n",
      "Episodes 700: Mean Reward: 7.76, Critic Loss: 12.5447, Policy Loss: -46.9886, Noise Sigma: 23.99\n",
      "Episodes 725: Mean Reward: 9.59, Critic Loss: 10.6369, Policy Loss: -42.6358, Noise Sigma: 23.96\n",
      "Episodes 750: Mean Reward: 8.09, Critic Loss: 8.7422, Policy Loss: -38.9236, Noise Sigma: 23.92\n",
      "Episodes 775: Mean Reward: 7.98, Critic Loss: 7.7548, Policy Loss: -36.6091, Noise Sigma: 23.88\n",
      "Episodes 800: Mean Reward: 7.60, Critic Loss: 7.5436, Policy Loss: -35.6714, Noise Sigma: 23.85\n",
      "Episodes 825: Mean Reward: 5.96, Critic Loss: 7.8009, Policy Loss: -36.0782, Noise Sigma: 23.81\n",
      "Episodes 850: Mean Reward: 7.19, Critic Loss: 8.0264, Policy Loss: -36.1058, Noise Sigma: 23.78\n",
      "Episodes 875: Mean Reward: 7.56, Critic Loss: 8.4811, Policy Loss: -36.9589, Noise Sigma: 23.74\n",
      "Episodes 900: Mean Reward: 7.03, Critic Loss: 8.5576, Policy Loss: -37.1312, Noise Sigma: 23.70\n",
      "Episodes 925: Mean Reward: 7.74, Critic Loss: 8.7815, Policy Loss: -37.1910, Noise Sigma: 23.67\n",
      "Episodes 950: Mean Reward: 7.60, Critic Loss: 8.9876, Policy Loss: -37.7175, Noise Sigma: 23.63\n",
      "Episodes 975: Mean Reward: 7.14, Critic Loss: 9.0647, Policy Loss: -37.7226, Noise Sigma: 23.60\n",
      "Episodes 1000: Mean Reward: 5.43, Critic Loss: 8.8389, Policy Loss: -37.3674, Noise Sigma: 23.56\n",
      "Episodes 1025: Mean Reward: 5.26, Critic Loss: 7.8616, Policy Loss: -35.5671, Noise Sigma: 23.52\n",
      "Episodes 1050: Mean Reward: 5.29, Critic Loss: 6.4654, Policy Loss: -31.9141, Noise Sigma: 23.49\n",
      "Episodes 1075: Mean Reward: 5.37, Critic Loss: 7.2943, Policy Loss: -31.0110, Noise Sigma: 23.45\n",
      "Episodes 1100: Mean Reward: 4.73, Critic Loss: 11.5268, Policy Loss: -34.7127, Noise Sigma: 23.42\n",
      "Episodes 1125: Mean Reward: 4.36, Critic Loss: 15.9471, Policy Loss: -41.6124, Noise Sigma: 23.38\n",
      "Episodes 1150: Mean Reward: 3.63, Critic Loss: 19.7774, Policy Loss: -48.4706, Noise Sigma: 23.34\n",
      "Episodes 1175: Mean Reward: 4.10, Critic Loss: 21.6614, Policy Loss: -53.6129, Noise Sigma: 23.31\n",
      "Episodes 1200: Mean Reward: 5.63, Critic Loss: 19.6111, Policy Loss: -54.5606, Noise Sigma: 23.27\n",
      "Episodes 1225: Mean Reward: 5.88, Critic Loss: 16.9825, Policy Loss: -52.6813, Noise Sigma: 23.24\n",
      "Episodes 1250: Mean Reward: 6.49, Critic Loss: 15.7063, Policy Loss: -53.1799, Noise Sigma: 23.20\n",
      "Episodes 1275: Mean Reward: 7.20, Critic Loss: 15.1688, Policy Loss: -52.7890, Noise Sigma: 23.16\n",
      "Episodes 1300: Mean Reward: 7.72, Critic Loss: 16.1477, Policy Loss: -53.4862, Noise Sigma: 23.13\n",
      "Episodes 1325: Mean Reward: 7.40, Critic Loss: 17.2873, Policy Loss: -54.6700, Noise Sigma: 23.09\n",
      "Episodes 1350: Mean Reward: 6.63, Critic Loss: 17.4414, Policy Loss: -54.8386, Noise Sigma: 23.06\n",
      "Episodes 1375: Mean Reward: 6.17, Critic Loss: 17.1995, Policy Loss: -54.1610, Noise Sigma: 23.02\n",
      "Episodes 1400: Mean Reward: 4.65, Critic Loss: 21.0258, Policy Loss: -54.4491, Noise Sigma: 22.98\n",
      "Episodes 1425: Mean Reward: 5.05, Critic Loss: 25.4000, Policy Loss: -56.0850, Noise Sigma: 22.95\n",
      "Episodes 1450: Mean Reward: 5.00, Critic Loss: 29.0915, Policy Loss: -57.4288, Noise Sigma: 22.91\n",
      "Episodes 1475: Mean Reward: 3.99, Critic Loss: 33.4169, Policy Loss: -61.6402, Noise Sigma: 22.88\n",
      "Episodes 1500: Mean Reward: 3.62, Critic Loss: 37.9171, Policy Loss: -69.2515, Noise Sigma: 22.84\n",
      "Episodes 1525: Mean Reward: 2.15, Critic Loss: 40.7062, Policy Loss: -75.3206, Noise Sigma: 22.80\n",
      "Episodes 1550: Mean Reward: 2.65, Critic Loss: 45.6076, Policy Loss: -80.3350, Noise Sigma: 22.77\n",
      "Episodes 1575: Mean Reward: 4.21, Critic Loss: 52.0382, Policy Loss: -84.3031, Noise Sigma: 22.73\n",
      "Episodes 1600: Mean Reward: 4.57, Critic Loss: 51.8224, Policy Loss: -82.5682, Noise Sigma: 22.70\n",
      "Episodes 1625: Mean Reward: 6.00, Critic Loss: 50.0142, Policy Loss: -79.6369, Noise Sigma: 22.66\n",
      "Episodes 1650: Mean Reward: 6.79, Critic Loss: 45.2597, Policy Loss: -76.8746, Noise Sigma: 22.62\n",
      "Episodes 1675: Mean Reward: 5.96, Critic Loss: 36.3358, Policy Loss: -73.9831, Noise Sigma: 22.59\n",
      "Episodes 1700: Mean Reward: 6.96, Critic Loss: 27.7867, Policy Loss: -69.9511, Noise Sigma: 22.55\n",
      "Episodes 1725: Mean Reward: 7.00, Critic Loss: 21.7582, Policy Loss: -65.0499, Noise Sigma: 22.52\n",
      "Episodes 1750: Mean Reward: 5.73, Critic Loss: 18.0632, Policy Loss: -60.6755, Noise Sigma: 22.48\n",
      "Episodes 1775: Mean Reward: 5.28, Critic Loss: 15.1466, Policy Loss: -53.6798, Noise Sigma: 22.44\n",
      "Episodes 1800: Mean Reward: 4.23, Critic Loss: 12.8543, Policy Loss: -46.6890, Noise Sigma: 22.41\n",
      "Episodes 1825: Mean Reward: 4.42, Critic Loss: 11.3156, Policy Loss: -41.6663, Noise Sigma: 22.37\n",
      "Episodes 1850: Mean Reward: 4.09, Critic Loss: 9.3052, Policy Loss: -36.6606, Noise Sigma: 22.34\n",
      "Episodes 1875: Mean Reward: 3.76, Critic Loss: 7.7917, Policy Loss: -33.2559, Noise Sigma: 22.30\n",
      "Episodes 1900: Mean Reward: 3.22, Critic Loss: 6.6671, Policy Loss: -30.7327, Noise Sigma: 22.26\n",
      "Episodes 1925: Mean Reward: 2.79, Critic Loss: 5.6502, Policy Loss: -28.2629, Noise Sigma: 22.23\n",
      "Episodes 1950: Mean Reward: 3.15, Critic Loss: 4.8553, Policy Loss: -26.1182, Noise Sigma: 22.19\n",
      "Episodes 1975: Mean Reward: 4.54, Critic Loss: 4.3442, Policy Loss: -25.2425, Noise Sigma: 22.16\n",
      "Episodes 2000: Mean Reward: 6.51, Critic Loss: 4.4492, Policy Loss: -26.1342, Noise Sigma: 22.12\n",
      "Episodes 2025: Mean Reward: 7.22, Critic Loss: 5.2871, Policy Loss: -28.6570, Noise Sigma: 22.08\n",
      "Episodes 2050: Mean Reward: 7.31, Critic Loss: 6.5775, Policy Loss: -32.2260, Noise Sigma: 22.05\n",
      "Episodes 2075: Mean Reward: 6.99, Critic Loss: 7.7615, Policy Loss: -34.8771, Noise Sigma: 22.01\n",
      "Episodes 2100: Mean Reward: 6.95, Critic Loss: 8.8399, Policy Loss: -36.3408, Noise Sigma: 21.98\n",
      "Episodes 2125: Mean Reward: 5.47, Critic Loss: 8.9768, Policy Loss: -35.8385, Noise Sigma: 21.94\n",
      "Episodes 2150: Mean Reward: 5.46, Critic Loss: 8.2297, Policy Loss: -33.1983, Noise Sigma: 21.90\n",
      "Episodes 2175: Mean Reward: 5.52, Critic Loss: 7.4265, Policy Loss: -30.3254, Noise Sigma: 21.87\n",
      "Episodes 2200: Mean Reward: 4.79, Critic Loss: 6.8040, Policy Loss: -28.0654, Noise Sigma: 21.83\n",
      "Episodes 2225: Mean Reward: 5.19, Critic Loss: 6.5107, Policy Loss: -27.1128, Noise Sigma: 21.80\n",
      "Episodes 2250: Mean Reward: 5.60, Critic Loss: 6.5879, Policy Loss: -28.1351, Noise Sigma: 21.76\n",
      "Episodes 2275: Mean Reward: 5.66, Critic Loss: 6.7001, Policy Loss: -29.9723, Noise Sigma: 21.72\n",
      "Episodes 2300: Mean Reward: 4.77, Critic Loss: 6.7948, Policy Loss: -32.3895, Noise Sigma: 21.69\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[29]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpolicy_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcritic_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpolicy_target\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcritic_target\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptimizer_policy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptimizer_critic\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepisodes\u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m10000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_steps\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0.99\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtau\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0.005\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[27]\u001B[39m\u001B[32m, line 82\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(policy, critic, policy_target, critic_target, optimizer_policy, optimizer_critic, memory, episodes, max_steps, gamma, tau, living_cost)\u001B[39m\n\u001B[32m     79\u001B[39m episode_critic_loss.append(critic_loss)\n\u001B[32m     81\u001B[39m \u001B[38;5;66;03m# Policy update\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m82\u001B[39m policy_loss = \u001B[43mtrain_policy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpolicy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcritic\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstates\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_policy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompute_device\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     83\u001B[39m episode_policy_loss.append(policy_loss)\n\u001B[32m     85\u001B[39m \u001B[38;5;66;03m# Target updates\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/tmp/pycharm_project_216/AIA/rl/pushT/trainer.py:30\u001B[39m, in \u001B[36mtrain_policy\u001B[39m\u001B[34m(policy_model, critic_model, input, optimizer, compute_device)\u001B[39m\n\u001B[32m     27\u001B[39m loss = -score.mean()\n\u001B[32m     29\u001B[39m loss.backward()\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mutils\u001B[49m\u001B[43m.\u001B[49m\u001B[43mclip_grad_norm_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpolicy_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_norm\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1.0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     31\u001B[39m optimizer.step()\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss.item()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py:33\u001B[39m, in \u001B[36m_no_grad.<locals>._no_grad_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     32\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_no_grad_wrapper\u001B[39m(*args, **kwargs):\n\u001B[32m---> \u001B[39m\u001B[32m33\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mno_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m     34\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m func(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/torch/autograd/grad_mode.py:76\u001B[39m, in \u001B[36mno_grad.__init__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     75\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m76\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_jit_internal\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_scripting\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m     77\u001B[39m         \u001B[38;5;28msuper\u001B[39m().\u001B[34m__init__\u001B[39m()\n\u001B[32m     78\u001B[39m     \u001B[38;5;28mself\u001B[39m.prev = \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/remote_env/lib/python3.12/site-packages/torch/_jit_internal.py:103\u001B[39m, in \u001B[36mis_scripting\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     99\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m2\u001B[39m, \u001B[32m7\u001B[39m):\n\u001B[32m    100\u001B[39m     \u001B[38;5;28mglobals\u001B[39m()[\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mBroadcastingList\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m] = BroadcastingList1\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mis_scripting\u001B[39m() -> \u001B[38;5;28mbool\u001B[39m:\n\u001B[32m    104\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    105\u001B[39m \u001B[33;03m    Function that returns True when in compilation and False otherwise. This\u001B[39;00m\n\u001B[32m    106\u001B[39m \u001B[33;03m    is useful especially with the @unused decorator to leave code in your\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    120\u001B[39m \u001B[33;03m                return unsupported_linear_op(x)\u001B[39;00m\n\u001B[32m    121\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m    122\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_env = make_env(render_mode=\"rgb_array\")\n",
    "\n",
    "state, _ = test_env.reset()\n",
    "\n",
    "for _ in range(50):\n",
    "    sleep(0.2)\n",
    "\n",
    "    input = torch.tensor(state, dtype=torch.float).to(compute_device) / 512.0\n",
    "\n",
    "    action = policy_model(input)\n",
    "\n",
    "    new_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "\n",
    "    state = new_state\n",
    "\n",
    "    # image = test_env.render()\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = test_env.reset()\n",
    "\n",
    "test_env.close()"
   ],
   "id": "80048ddd1c6b3bc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_env.close()",
   "id": "5e779b2cc96a5b37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f0e301aa608bf378"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
